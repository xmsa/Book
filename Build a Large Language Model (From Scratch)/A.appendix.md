<!-- language: rtl -->

# پیوست A: مقدمه‌ای بر PyTorch

این پیوست به منظور تجهیز شما به مهارت‌ها و دانش لازم برای به‌کارگیری یادگیری عمیق و پیاده‌سازی مدل‌های زبانی بزرگ (LLMs) از پایه طراحی شده است. PyTorch، کتابخانه محبوب یادگیری عمیق مبتنی بر پایتون، ابزار اصلی ما در این کتاب خواهد بود. من شما را در راه‌اندازی محیط کاری یادگیری عمیق با پشتیبانی از PyTorch و GPU راهنمایی خواهم کرد.

سپس با مفهوم بنیادی تنسورها و نحوه استفاده از آن‌ها در PyTorch آشنا خواهید شد. همچنین به موتور مشتق‌گیری خودکار PyTorch خواهیم پرداخت، ویژگی‌ای که امکان استفاده آسان و بهینه از پس‌انتشار (backpropagation) را فراهم می‌کند، که بخش حیاتی آموزش شبکه‌های عصبی است.

این پیوست به عنوان مقدمه‌ای برای افرادی است که تازه با یادگیری عمیق در PyTorch آشنا می‌شوند. هرچند PyTorch را از ابتدا توضیح می‌دهد، هدف ارائه پوشش کامل کتابخانه نیست بلکه تمرکز بر اصول پایه‌ای PyTorch است که برای پیاده‌سازی LLMها به آن نیاز داریم. اگر قبلاً با یادگیری عمیق آشنا هستید، می‌توانید این پیوست را رد کرده و مستقیم به فصل دوم بروید.

---

## بخش A.1 PyTorch چیست؟

PyTorch (https://pytorch.org/) یک کتابخانه متن‌باز یادگیری عمیق مبتنی بر پایتون است. بر اساس آمار سایت Papers With Code (https://paperswithcode.com/trends)، که روندها و تحلیل‌های مقالات پژوهشی را پیگیری می‌کند، از سال ۲۰۱۹ به طور قابل توجهی پرکاربردترین کتابخانه یادگیری عمیق در تحقیقات بوده است. همچنین بر اساس نظرسنجی Kaggle در سال ۲۰۲۲ (https://www.kaggle.com/c/kaggle-survey-2022)، حدود ۴۰٪ پاسخ‌دهندگان از PyTorch استفاده می‌کنند که این میزان هر سال افزایش می‌یابد.

یکی از دلایل محبوبیت PyTorch، رابط کاربری ساده و در عین حال کارآمد آن است. با وجود دسترسی آسان، از انعطاف‌پذیری بالایی برخوردار است که به کاربران حرفه‌ای اجازه می‌دهد جزئیات سطح پایین مدل‌های خود را برای سفارشی‌سازی و بهینه‌سازی تغییر دهند. به طور خلاصه، برای بسیاری از پژوهشگران و توسعه‌دهندگان، PyTorch تعادلی مناسب بین سهولت استفاده و امکانات فراهم می‌کند.

---

### زیربخش A.1.1 سه مؤلفه اصلی PyTorch

PyTorch کتابخانه‌ای نسبتاً جامع است و می‌توان آن را از دید سه مؤلفه اصلی بررسی کرد که در شکل A.1 خلاصه شده‌اند.

- **کتابخانه تنسورها**: بلوک ساختمانی پایه برای محاسبات، توسعه یافته از مفهوم آرایه‌ها در NumPy و با قابلیت تسریع محاسبات روی GPU که امکان جابجایی بی‌وقفه بین CPU و GPU را فراهم می‌کند.
- **موتور مشتق‌گیری خودکار (autograd)**: این بخش به طور خودکار مشتقات عملیات تنسورها را محاسبه می‌کند که فرایند پس‌انتشار و بهینه‌سازی مدل را ساده می‌کند.
- **ابزارهای یادگیری عمیق**: مجموعه‌ای از ابزارهای ماژولار، انعطاف‌پذیر و کارآمد شامل مدل‌های پیش‌آموزش‌دیده، توابع خطا و بهینه‌سازها برای طراحی و آموزش انواع مدل‌های یادگیری عمیق.

---

### زیربخش A.1.2 تعریف یادگیری عمیق

در اخبار، مدل‌های زبانی بزرگ (LLMs) اغلب به عنوان مدل‌های هوش مصنوعی (AI) شناخته می‌شوند، اما در واقع LLMها نوعی شبکه عصبی عمیق هستند و PyTorch کتابخانه‌ای برای یادگیری عمیق است. ممکن است کمی گیج‌کننده باشد؛ پس خلاصه‌ای از ارتباط این اصطلاحات را مرور می‌کنیم.

- **هوش مصنوعی (AI)**: ساخت سیستم‌های کامپیوتری که قادر به انجام کارهایی هستند که معمولاً به هوش انسانی نیاز دارند؛ مانند درک زبان طبیعی، تشخیص الگوها و تصمیم‌گیری. (اگرچه پیشرفت‌های زیادی شده، AI هنوز به هوش عمومی انسانی نرسیده است.)
- **یادگیری ماشین**: زیرمجموعه‌ای از AI که بر توسعه الگوریتم‌هایی تمرکز دارد که کامپیوترها بتوانند بدون برنامه‌ریزی صریح، از داده‌ها بیاموزند و پیش‌بینی یا تصمیم‌گیری کنند.
- **یادگیری عمیق**: زیرمجموعه‌ای از یادگیری ماشین که بر آموزش و کاربرد شبکه‌های عصبی عمیق تمرکز دارد. «عمیق» به لایه‌های پنهان متعدد در این شبکه‌ها اشاره دارد که توانایی مدل‌سازی روابط پیچیده و غیرخطی در داده‌ها را فراهم می‌کند.

روند معمول مدل‌سازی پیش‌بینی در یادگیری ماشین و یادگیری عمیق (یادگیری نظارت‌شده) شامل مراحل آموزش مدل روی داده‌های برچسب‌خورده و سپس استفاده از مدل برای پیش‌بینی برچسب داده‌های جدید است (شکل A.3).

برای مثال، در یک مدل تشخیص ایمیل‌های هرزنامه، داده‌های آموزشی شامل ایمیل‌ها و برچسب «هرزنامه» یا «غیر هرزنامه» است. سپس مدل آموزش‌دیده می‌تواند ایمیل‌های جدید را طبقه‌بندی کند. ارزیابی مدل بین مرحله آموزش و استفاده نیز اهمیت دارد تا مطمئن شویم عملکرد مدل کافی است.

اگر LLMها را برای تولید متن آموزش دهیم، روند مشابه است اما برچسب‌ها (labels) در پیش‌آموزش از خود متن استخراج می‌شوند (مثلاً پیش‌بینی کلمه بعدی).

---

### زیربخش A.1.3 نصب PyTorch

PyTorch مانند هر کتابخانه پایتون دیگری نصب می‌شود، اما به دلیل پشتیبانی از CPU و GPU، نکات اضافی دارد.

- **نسخه پایتون**: بهتر است از نسخه‌ای استفاده کنید که یک یا دو نسخه عقب‌تر از آخرین نسخه رسمی است، چون بسیاری از کتابخانه‌های علمی هنوز جدیدترین نسخه را پشتیبانی نمی‌کنند.
- **نسخه‌های PyTorch**: دو نسخه وجود دارد: نسخه ساده فقط CPU و نسخه کامل که از CPU و GPU پشتیبانی می‌کند. اگر کارت گرافیک CUDA-سازگار دارید (مانند NVIDIA T4، RTX 2080 Ti یا جدیدتر)، نسخه GPU را نصب کنید.
- دستور پیش‌فرض نصب در ترمینال کد:

```bash
pip install torch
```

اگر GPU سازگار باشد، نسخه مناسب GPU به صورت خودکار نصب می‌شود.

- برای نصب نسخه خاص (مثلاً 2.4.0) توصیه می‌شود دستور زیر را استفاده کنید:

```bash
pip install torch==2.4.0
```

- برای بررسی نسخه نصب شده در PyTorch:

```python
import torch
torch.__version__
```

خروجی:

```
'2.4.0'
```

- PyTorch ریشه در کتابخانه Torch دارد که با زبان Lua توسعه یافته بود.

- برای بررسی شناسایی GPU توسط PyTorch:

```python
import torch
torch.cuda.is_available()
```

اگر خروجی `True` بود، نصب و شناسایی GPU درست انجام شده است.

- اگر GPU ندارید، می‌توانید از سرویس‌های ابری مانند Google Colab استفاده کنید که دسترسی محدود زمانی به GPU ارائه می‌دهد.

- برای کاربران مک با تراشه Apple Silicon (مثل M1، M2، M3) قابلیت شتاب‌دهی PyTorch با این تراشه‌ها وجود دارد. برای بررسی پشتیبانی:

```python
print(torch.backends.mps.is_available())
```

خروجی `True` نشان‌دهنده پشتیبانی است.

---

**تمرینات:**

- **تمرین A.1:** نصب و راه‌اندازی PyTorch روی سیستم خود.
- **تمرین A.2:** اجرای کد آزمایشی از لینک [https://mng.bz/o05v](https://mng.bz/o05v) برای بررسی صحیح بودن محیط کاری.

---

## بخش A.2 آشنایی با تنسورها

تنسورها مفهوم ریاضی‌ای هستند که بردارها و ماتریس‌ها را به ابعاد بالاتر تعمیم می‌دهند. به عبارت دیگر، تنسورها اشیاء ریاضی هستند که بر اساس رتبه یا درجه‌شان (تعداد ابعاد) تعریف می‌شوند. مثال‌ها:

- اسکالر (یک عدد ساده): رتبه 0
- بردار: رتبه 1
- ماتریس: رتبه 2

(شکل A.6)

از دید محاسباتی، تنسورها داده‌های چندبعدی را در خود نگهداری می‌کنند، که هر بعد نمایانگر ویژگی متفاوتی است. کتابخانه‌های تنسور مانند PyTorch امکان ایجاد، دستکاری و محاسبه روی این آرایه‌ها را به صورت بهینه فراهم می‌کنند.

تنسورهای PyTorch مشابه آرایه‌های NumPy هستند اما ویژگی‌های اضافه مانند موتور مشتق‌گیری خودکار و پشتیبانی از محاسبات GPU را دارند که در یادگیری عمیق اهمیت دارد.

---

### زیربخش A.2.1 اسکالر، بردار، ماتریس و تنسور

همانطور که گفته شد، تنسورهای PyTorch نگهدارنده داده‌های آرایه‌ای هستند:

```python
import torch
tensor0d = torch.tensor(1)     # اسکالر (رتبه 0)
tensor1d = torch.tensor([1, 2, 3])    # بردار (رتبه 1)
tensor2d = torch.tensor([[1, 2],
                         [3, 4]])     # ماتریس (رتبه 2)
tensor3d = torch.tensor([[[1, 2], [3, 4]],
                         [[5, 6], [7, 8]]])    # تنسور سه‌بعدی (رتبه 3)
```

---

### زیربخش A.2.2 نوع داده تنسورها

PyTorch به طور پیش‌فرض از نوع داده صحیح ۶۴ بیتی (int64) استفاده می‌کند:

```python
tensor1d = torch.tensor([1, 2, 3])
print(tensor1d.dtype)
```

خروجی:

```
torch.int64
```

اگر تنسورها از اعداد اعشاری ساخته شوند، پیش‌فرض ۳۲ بیت شناور است:

```python
floatvec = torch.tensor([1.0, 2.0, 3.0])
print(floatvec.dtype)
```

خروجی:

```
torch.float32
```

این انتخاب به دلیل تعادل بین دقت و کارایی محاسباتی است. پردازشگرهای گرافیکی برای محاسبات ۳۲ بیت بهینه شده‌اند و سرعت آموزش مدل را افزایش می‌دهند.

برای تغییر نوع داده تنسور می‌توان از متد `.to` استفاده کرد:

```python
floatvec = tensor1d.to(torch.float32)
print(floatvec.dtype)
```

خروجی:

```
torch.float32
```

---

### زیربخش A.2.3 عملیات رایج روی تنسورها

اینجا فقط برخی عملیات مهم معرفی می‌شود.

- ایجاد تنسور دوبعدی:

```python
tensor2d = torch.tensor([[1, 2, 3],
                         [4, 5, 6]])
print(tensor2d)
```

خروجی:

```
tensor([[1, 2, 3],
        [4, 5, 6]])
```

- دسترسی به شکل تنسور:

```python
print(tensor2d.shape)
```

خروجی:

```
torch.Size([2, 3])
```

- تغییر شکل به ۳×۲:

```python
print(tensor2d.reshape(3, 2))
```

خروجی:

```
tensor([[1, 2],
        [3, 4],
        [5, 6]])
```

- روش رایج‌تر برای تغییر شکل `.view()` است:

```python
print(tensor2d.view(3, 2))
```

خروجی مشابه بالا.

- ترانهاده (تبدیل ستون به ردیف و بالعکس):

```python
print(tensor2d.T)
```

خروجی:

```
tensor([[1, 4],
        [2, 5],
        [3, 6]])
```

- ضرب ماتریسی:

```python
print(tensor2d.matmul(tensor2d.T))
print(tensor2d @ tensor2d.T)
```

خروجی هر دو:

```
tensor([[14, 32],
        [32, 77]])
```

برای مطالعه بیشتر و مشاهده تمامی عملیات، می‌توانید به مستندات رسمی PyTorch به آدرس [https://pytorch.org/docs/stable/tensors.xhtml](https://pytorch.org/docs/stable/tensors.xhtml) مراجعه کنید.

---

## بخش A.3 مشاهده مدل‌ها به‌عنوان گراف‌های محاسباتی

حال به موتور مشتق‌گیری خودکار PyTorch، که به نام autograd نیز شناخته می‌شود، می‌پردازیم. سیستم autograd در PyTorch توابعی را فراهم می‌کند که به‌صورت خودکار مشتق‌ها را در گراف‌های محاسباتی پویا محاسبه می‌کنند.  
گراف محاسباتی، گرافی جهت‌دار است که امکان نمایش و تحلیل عبارات ریاضی را فراهم می‌کند. در زمینه یادگیری عمیق، این گراف ترتیب محاسبات لازم برای به‌دست آوردن خروجی شبکه عصبی را مشخص می‌کند؛ این گراف برای محاسبه گرادیان‌های موردنیاز الگوریتم پس‌انتشار خطا (backpropagation) که اصلی‌ترین روش آموزش شبکه‌های عصبی است، ضروری است.

برای درک بهتر، مثال مشخصی از گراف محاسباتی ارائه می‌کنیم. کد زیر، بخش پیشروی یک مدل رگرسیون لجستیک ساده (یک شبکه عصبی تک‌لایه) را پیاده‌سازی می‌کند که خروجی آن عددی بین ۰ و ۱ است و با برچسب واقعی (۰ یا ۱) برای محاسبه خطا مقایسه می‌شود.

```python
import torch.nn.functional as F     #1
y = torch.tensor([1.0])            #2
x1 = torch.tensor([1.1])           #3
w1 = torch.tensor([2.2])           #4
b = torch.tensor([0.0])            #5
z = x1 * w1 + b                    #6
a = torch.sigmoid(z)               #7
loss = F.binary_cross_entropy(a, y)
```

> #1: وارد کردن کتابخانه معمول در PyTorch برای جلوگیری از خطوط طولانی کد  
> #2: برچسب واقعی  
> #3: ویژگی ورودی  
> #4: پارامتر وزن  
> #5: بایاس
> #6: ورودی خالص  
> #7: فعال‌سازی و خروجی

اگر همه اجزای کد برایتان واضح نیست، نگران نباشید؛ هدف این مثال توضیح رگرسیون لجستیک نیست، بلکه نشان دادن مفهوم گراف محاسباتی است (شکل A.7). PyTorch به‌صورت خودکار چنین گرافی را پشت صحنه می‌سازد که می‌توان از آن برای محاسبه گرادیان‌ها نسبت به پارامترهای مدل (w1 و b در اینجا) استفاده کرد تا مدل آموزش داده شود.

---

## بخش A.4 مشتق‌گیری خودکار به زبان ساده

وقتی در PyTorch محاسبات انجام می‌دهیم، اگر یکی از نودهای انتهایی دارای ویژگی `requires_grad=True` باشد، به‌صورت خودکار یک گراف محاسباتی ساخته می‌شود. این موضوع زمانی کاربرد دارد که بخواهیم گرادیان‌ها را محاسبه کنیم. گرادیان‌ها برای آموزش شبکه‌های عصبی با استفاده از الگوریتم پس‌انتشار خطا که در واقع پیاده‌سازی قانون زنجیره‌ای مشتق‌گیری از حساب دیفرانسیل است، لازم هستند (شکل A.8).
قانون زنجیره‌ای به ما اجازه می‌دهد گرادیان‌ها را از خروجی شبکه به ورودی محاسبه کنیم و این گرادیان‌ها نحوه به‌روزرسانی وزن‌ها و بایاس‌ها را تعیین می‌کنند.

گرادیان‌ها، برداری از مشتقات جزئی تابع چندمتغیره هستند که نرخ تغییر تابع نسبت به هر متغیر را نشان می‌دهند.

اگر با مشتقات جزئی، گرادیان‌ها یا قانون زنجیره‌ای آشنا نیستید، جای نگرانی نیست؛ کافی است بدانید که PyTorch با کمک autograd این محاسبات را به‌صورت خودکار انجام می‌دهد.

به‌عنوان مثال، با استفاده از تابع `grad` در PyTorch می‌توان گرادیان تابع خطا نسبت به پارامترهای مدل را محاسبه کرد:

```python
import torch.nn.functional as F
from torch.autograd import grad
y = torch.tensor([1.0])
x1 = torch.tensor([1.1])
w1 = torch.tensor([2.2], requires_grad=True)
b = torch.tensor([0.0], requires_grad=True)
z = x1 * w1 + b
a = torch.sigmoid(z)
loss = F.binary_cross_entropy(a, y)
grad_L_w1 = grad(loss, w1, retain_graph=True)   #1
grad_L_b = grad(loss, b, retain_graph=True)
```

> #1: به‌صورت پیش‌فرض، PyTorch پس از محاسبه گرادیان‌ها گراف را حذف می‌کند تا حافظه آزاد شود. با `retain_graph=True` گراف حفظ می‌شود چون ممکن است دوباره استفاده شود.

خروجی گرادیان‌ها به شکل زیر است:

```python
print(grad_L_w1)
print(grad_L_b)
```

```
(tensor([-0.0898]),)
(tensor([-0.0817]),)
```

به‌صورت معمول، برای ساده‌تر کردن فرآیند، کافی است متد `.backward()` را روی `loss` فراخوانی کنیم تا همه گرادیان‌های مربوط به پارامترهای مدل به صورت خودکار محاسبه شده و در `w1.grad` و `b.grad` ذخیره شوند:

```python
loss.backward()
print(w1.grad)
print(b.grad)
```

که همان خروجی قبلی را خواهد داشت.

خلاصه اینکه PyTorch با متد `.backward()` محاسبات مشتق‌گیری را برای ما انجام می‌دهد و نیازی به محاسبه دستی مشتقات نیست.

---

## بخش A.5 پیاده‌سازی شبکه‌های عصبی چندلایه

حالا به استفاده از PyTorch برای پیاده‌سازی شبکه‌های عصبی عمیق می‌پردازیم. مثال ما یک پرسپترون چندلایه (MLP) با دو لایه پنهان است (شکل A.9).
برای ساخت مدل‌های دلخواه در PyTorch، کلاس پایه `torch.nn.Module` را ارث‌بری می‌کنیم و در آن معماری شبکه را تعریف می‌کنیم.

در تابع سازنده `__init__`، لایه‌ها ساخته می‌شوند و در تابع `forward` نحوه جریان داده‌ها از ورودی به خروجی مشخص می‌شود. معمولاً نیازی به پیاده‌سازی `backward` نیست چون PyTorch به صورت خودکار آن را انجام می‌دهد.

مثال پیاده‌سازی MLP با دو لایه پنهان:

```python
class NeuralNetwork(torch.nn.Module):
    def __init__(self, num_inputs, num_outputs):    #1
        super().__init__()
        self.layers = torch.nn.Sequential(
            torch.nn.Linear(num_inputs, 30),    #2
            torch.nn.ReLU(),                     #3
            torch.nn.Linear(30, 20),             #4
            torch.nn.ReLU(),
            torch.nn.Linear(20, num_outputs),
        )
    def forward(self, x):
        logits = self.layers(x)
        return logits           #5
```

> #1: تعریف تعداد ورودی‌ها و خروجی‌ها به صورت متغیر برای انعطاف‌پذیری  
> #2: لایه خطی (Linear) که تعداد ورودی و خروجی را می‌گیرد  
> #3: تابع فعال‌سازی غیرخطی ReLU بین لایه‌ها  
> #4: تعداد نودهای خروجی لایه قبلی باید برابر تعداد ورودی لایه بعدی باشد  
> #5: خروجی آخرین لایه به نام logits شناخته می‌شود

ساخت نمونه‌ای از مدل:

```python
model = NeuralNetwork(50, 3)
print(model)
```

خروجی:

```
NeuralNetwork(
  (layers): Sequential(
    (0): Linear(in_features=50, out_features=30, bias=True)
    (1): ReLU()
    (2): Linear(in_features=30, out_features=20, bias=True)
    (3): ReLU()
    (4): Linear(in_features=20, out_features=3, bias=True)
  )
)
```

می‌توان تعداد کل پارامترهای قابل آموزش مدل را به شکل زیر محاسبه کرد:

```python
num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print("Total number of trainable model parameters:", num_params)
```

خروجی:

```
Total number of trainable model parameters: 2213
```

هر پارامتری که `requires_grad=True` داشته باشد، در فرآیند آموزش به‌روزرسانی می‌شود. پارامترهای وزن و بایاس در لایه‌های `Linear` قرار دارند.

مثلاً وزن‌های اولین لایه خطی را می‌توان با کد زیر مشاهده کرد:

```python
print(model.layers[0].weight)
print(model.layers[0].weight.shape)
```

خروجی ابعاد وزن:

```
torch.Size([30, 50])
```

وزن‌ها به صورت پیش‌فرض با اعداد کوچک تصادفی مقداردهی اولیه می‌شوند تا از هم‌تقارنی در فرآیند آموزش جلوگیری شود. با تنظیم دانه تصادفی (`manual_seed`) می‌توان این مقداردهی را قابل بازتولید کرد:

```python
torch.manual_seed(123)
model = NeuralNetwork(50, 3)
print(model.layers[0].weight)
```

برای اجرای گذر رو به جلو (forward pass):

```python
torch.manual_seed(123)
X = torch.rand((1, 50))
out = model(X)
print(out)
```

خروجی شامل سه مقدار است که هرکدام نمره مربوط به یکی از سه خروجی شبکه را نشان می‌دهد:

```
tensor([[-0.1262,  0.1080, -0.1792]], grad_fn=<AddmmBackward0>)
```

عبارت `grad_fn=<AddmmBackward0>` نشان‌دهنده تابع آخرین عملیات روی گراف محاسباتی است، که در اینجا عملیات ضرب ماتریسی و جمع است.

---

#### استفاده از مدل در حالت پیش‌بینی (بدون محاسبه گرادیان)

برای کاهش مصرف حافظه و زمان در زمان پیش‌بینی (inference)، می‌توان از `torch.no_grad()` استفاده کرد که اجازه نمی‌دهد PyTorch گراف محاسباتی را ذخیره کند:

```python
with torch.no_grad():
    out = model(X)
print(out)
```

خروجی مشابه قبل ولی بدون گراف:

```
tensor([[-0.1262,  0.1080, -0.1792]])
```

---

#### تبدیل خروجی به احتمال کلاس‌ها

در PyTorch معمولاً خروجی لایه آخر (logits) بدون تابع فعال‌سازی نهایی بازگردانده می‌شود، چون توابع خطا مثل `CrossEntropyLoss` خودشان عملیات softmax را انجام می‌دهند.

اگر بخواهیم احتمال عضویت کلاس‌ها را به صورت صریح ببینیم، باید تابع softmax را به صورت دستی اعمال کنیم:

```python
with torch.no_grad():
    out = torch.softmax(model(X), dim=1)
print(out)
```

خروجی:

```
tensor([[0.3113, 0.3934, 0.2952]])
```

این مقادیر احتمال‌های عضویت در کلاس‌ها هستند که مجموعشان برابر ۱ است. برای مدل‌های تازه‌سازمان‌یافته و آموزش‌ندیده، معمولاً این احتمالات نزدیک به هم هستند.

---

**خلاصه:**

- PyTorch از گراف‌های محاسباتی برای محاسبه مشتقات استفاده می‌کند.
- autograd مسئول ساخت گراف و محاسبه گرادیان‌ها است.
- پیاده‌سازی شبکه‌های عصبی چندلایه با `torch.nn.Module` آسان است.
- برای استفاده در حالت پیش‌بینی بهتر است `torch.no_grad()` را به کار ببریم.
- خروجی‌های لایه آخر معمولاً logits هستند و برای تبدیل به احتمالات باید softmax بگیریم.

## بخش A.6: راه‌اندازی داده‌خوان‌های کارآمد

قبل از آموزش مدل، لازم است به صورت خلاصه نحوه ایجاد داده‌خوان‌های (DataLoader) کارآمد در PyTorch را بررسی کنیم. داده‌خوان‌ها نقش مهمی در تکرار روی داده‌ها در طول آموزش دارند. ایده کلی بارگذاری داده در PyTorch در شکل A.10 نشان داده شده است.

**شکل A.10**  
PyTorch دو کلاس اصلی `Dataset` و `DataLoader` را پیاده‌سازی می‌کند. کلاس `Dataset` مشخص می‌کند که هر نمونه داده چگونه بارگذاری شود، و کلاس `DataLoader` مسئول برهم زدن (shuffle) داده‌ها و دسته‌بندی (batch) آنهاست.

طبق شکل A.10، ما یک کلاس Dataset سفارشی می‌سازیم که از آن برای ایجاد مجموعه داده‌های آموزش و آزمون استفاده می‌کنیم و سپس با آن داده‌خوان‌ها را می‌سازیم. ابتدا یک مجموعه داده ساده با پنج نمونه آموزش و دو ویژگی برای هر نمونه ایجاد می‌کنیم. همچنین یک تنسور شامل برچسب کلاس هر نمونه (سه نمونه کلاس 0 و دو نمونه کلاس 1) ساخته می‌شود. مجموعه آزمون شامل دو نمونه است. کد ایجاد این مجموعه داده به شرح زیر است:

```python
X_train = torch.tensor([
    [-1.2, 3.1],
    [-0.9, 2.9],
    [-0.5, 2.6],
    [2.3, -1.1],
    [2.7, -1.5]
])
y_train = torch.tensor([0, 0, 0, 1, 1])
X_test = torch.tensor([
    [-0.8, 2.8],
    [2.6, -1.6],
])
y_test = torch.tensor([0, 1])
```

> **نکته:**
> برچسب‌های کلاس در PyTorch باید از صفر شروع شوند و بزرگترین مقدار برچسب نباید از تعداد خروجی‌های شبکه (تعداد نورون‌های لایه خروجی) بیشتر باشد. مثلاً اگر برچسب‌ها 0 تا 4 هستند، لایه خروجی باید پنج نورون داشته باشد.

سپس، کلاس Dataset سفارشی به نام `ToyDataset` را با ارث‌بری از کلاس پایه `Dataset` در PyTorch تعریف می‌کنیم:

```python
from torch.utils.data import Dataset

class ToyDataset(Dataset):
    def __init__(self, X, y):
        self.features = X
        self.labels = y

    def __getitem__(self, index):        # ۱
        one_x = self.features[index]     # ۱
        one_y = self.labels[index]       # ۱
        return one_x, one_y              # ۱

    def __len__(self):
        return self.labels.shape[0]      # ۲

train_ds = ToyDataset(X_train, y_train)
test_ds = ToyDataset(X_test, y_test)
```

> #1: دستورات مربوط به بازیابی یک نمونه داده به همراه برچسب آن از طریق اندیس  
> #2: دستورات مربوط به بازگرداندن تعداد کل نمونه‌های مجموعه داده

هدف از تعریف این کلاس `ToyDataset`، ایجاد یک داده‌خوان `DataLoader` در PyTorch است.
در PyTorch، سه جزء اصلی در تعریف یک کلاس Dataset عبارتند از: سازنده `__init__`، متد `__getitem__` و متد `__len__`. در متد `__init__` ویژگی‌هایی که در متدهای دیگر استفاده می‌شوند تنظیم می‌گردند (مثلاً مسیر فایل، اتصال به دیتابیس یا در اینجا تنسورهای داده).
متد `__getitem__` نحوه دسترسی به یک نمونه داده مشخص از طریق اندیس را تعریف می‌کند. این نمونه شامل ویژگی‌ها و برچسب کلاس است.
متد `__len__` تعداد کل نمونه‌های موجود را بازمی‌گرداند که در این مثال برابر با تعداد ردیف‌های آرایه ویژگی‌هاست.

برای نمونه، بررسی طول مجموعه داده آموزش:

```python
print(len(train_ds))
```

خروجی:

```
5
```

حالا که کلاس Dataset را تعریف کردیم، می‌توانیم داده‌خوان‌ها را با استفاده از کلاس `DataLoader` بسازیم:

```python
from torch.utils.data import DataLoader

torch.manual_seed(123)

train_loader = DataLoader(
    dataset=train_ds,     # ۱
    batch_size=2,
    shuffle=True,          # ۲
    num_workers=0          # ۳
)

test_loader = DataLoader(
    dataset=test_ds,
    batch_size=2,
    shuffle=False,         # ۴
    num_workers=0
)
```

> #1: ورودی داده‌خوان که همان نمونه Dataset است  
> #2: مشخص می‌کند که داده‌ها قبل از دسته‌بندی شدن برهم زده شوند یا خیر  
> #3: تعداد پردازش‌های پس‌زمینه برای بارگذاری داده  
> #4: معمولاً در داده‌های آزمون نیازی به برهم زدن داده‌ها نیست

حالا می‌توانیم روی `train_loader` تکرار کنیم:

```python
for idx, (x, y) in enumerate(train_loader):
    print(f"Batch {idx+1}:", x, y)
```

خروجی نمونه:

```
Batch 1: tensor([[-1.2000,  3.1000],
                 [-0.5000,  2.6000]]) tensor([0, 0])
Batch 2: tensor([[ 2.3000, -1.1000],
                 [-0.9000,  2.9000]]) tensor([1, 0])
Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])
```

همانطور که دیده می‌شود، `train_loader` کل داده‌های آموزش را به صورت دسته‌ای و به ترتیب تصادفی (با توجه به بذر عدد تصادفی) بازدید می‌کند. این یک epoch (دور کامل آموزش) محسوب می‌شود. توجه داشته باشید که بار دوم تکرار روی داده‌ها ترتیب برهم خوردن تغییر خواهد کرد که این به جلوگیری از گیر افتادن شبکه در چرخه‌های تکراری کمک می‌کند.

توجه کنید که اندازه دسته (batch size) ۲ است ولی دسته آخر فقط شامل یک نمونه است؛ زیرا تعداد کل نمونه‌ها ۵ است و ۵ بر ۲ بخش‌پذیر نیست. این موضوع می‌تواند روند همگرایی آموزش را مختل کند. برای جلوگیری از این مشکل، می‌توان پارامتر `drop_last=True` را به `DataLoader` اضافه کرد تا دسته آخر که کمتر از اندازه کامل است حذف شود:

```python
train_loader = DataLoader(
    dataset=train_ds,
    batch_size=2,
    shuffle=True,
    num_workers=0,
    drop_last=True
)
```

اکنون خروجی تکرار مشابه این خواهد بود:

```
Batch 1: tensor([[-0.9000,  2.9000],
        [ 2.3000, -1.1000]]) tensor([0, 1])
Batch 2: tensor([[ 2.7000, -1.5000],
        [-0.5000,  2.6000]]) tensor([1, 0])
```

---

### اهمیت پارامتر `num_workers` در DataLoader

پارامتر `num_workers` مشخص می‌کند چند فرایند موازی برای بارگذاری و پیش‌پردازش داده‌ها استفاده شود. اگر `num_workers=0` باشد، بارگذاری داده‌ها در فرایند اصلی انجام می‌شود و این ممکن است باعث ایجاد گلوگاه (bottleneck) شود، به خصوص در شبکه‌های بزرگ که پردازشگر گرافیکی (GPU) منتظر بارگذاری داده توسط CPU می‌ماند.

وقتی `num_workers` بزرگ‌تر از صفر باشد، چند فرایند جداگانه داده‌ها را به صورت موازی بارگذاری می‌کنند و فرایند اصلی آزاد می‌شود تا روی آموزش مدل تمرکز کند. این موضوع به استفاده بهتر از منابع سخت‌افزاری کمک می‌کند.

**شکل A.11**
نمایش تفاوت بارگذاری داده بدون استفاده از چند worker (سمت چپ) که باعث توقف مدل تا بارگذاری دسته بعدی می‌شود، و بارگذاری داده با چند worker که داده‌ها را به صورت صف در پس‌زمینه آماده می‌کند (سمت راست).

---

### نکات مهم:

- اگر با مجموعه داده‌های بسیار کوچک یا محیط‌های تعاملی مانند Jupyter Notebook کار می‌کنید، افزایش `num_workers` ممکن است تاثیری نداشته باشد و حتی باعث مشکلاتی مانند بالا رفتن هزینه راه‌اندازی پردازش‌ها یا تداخل در منابع شود.
- در محیط Jupyter، `num_workers > 0` گاهی به خطا یا کرش نوت‌بوک منجر می‌شود.
- مقدار بهینه `num_workers` بستگی به سخت‌افزار و اندازه مجموعه داده دارد. در تجربه نویسنده، مقدار ۴ معمولاً عملکرد بهینه را در مجموعه داده‌های واقعی دارد.

---

## بخش A.7 حلقه‌ی معمول آموزش مدل

اکنون قصد داریم یک شبکه عصبی را روی داده‌های ساده (toy dataset) آموزش دهیم. کد آموزش در ادامه آمده است.

```python
import torch.nn.functional as F
torch.manual_seed(123)
model = NeuralNetwork(num_inputs=2, num_outputs=2)    #1
optimizer = torch.optim.SGD(
    model.parameters(), lr=0.5
)            #2
num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    for batch_idx, (features, labels) in enumerate(train_loader):
        logits = model(features)
        loss = F.cross_entropy(logits, labels)
        optimizer.zero_grad()            #3
        loss.backward()         #4
        optimizer.step()        #5
        ### LOGGING
        print(f"Epoch: {epoch+1:03d}/{num_epochs:03d}"
              f" | Batch {batch_idx:03d}/{len(train_loader):03d}"
              f" | Train Loss: {loss:.2f}")
    model.eval()
    # Insert optional model evaluation code
```

توضیحات کد:

> #1 تعداد ورودی‌ها ۲ و تعداد خروجی‌ها ۲ است (دو ویژگی و دو کلاس در داده‌ها).  
> #2 به بهینه‌ساز (optimizer) مشخص می‌کنیم کدام پارامترها باید به‌روزرسانی شوند.  
> #3 با فراخوانی `optimizer.zero_grad()` گرادیان‌های قبلی صفر می‌شوند تا انباشت ناخواسته رخ ندهد.  
> #4 با `loss.backward()` گرادیان‌ها نسبت به پارامترهای مدل محاسبه می‌شوند.  
> #5 با `optimizer.step()` پارامترها بر اساس گرادیان‌ها به‌روزرسانی می‌شوند.

خروجی اجرای این کد به صورت زیر است:

```
Epoch: 001/003 | Batch 000/002 | Train Loss: 0.75
Epoch: 001/003 | Batch 001/002 | Train Loss: 0.65
Epoch: 002/003 | Batch 000/002 | Train Loss: 0.44
Epoch: 002/003 | Batch 001/002 | Trainl Loss: 0.13
Epoch: 003/003 | Batch 000/002 | Train Loss: 0.03
Epoch: 003/003 | Batch 001/002 | Train Loss: 0.00
```

مشاهده می‌شود که پس از سه دوره (epoch) مقدار خطا (loss) به صفر می‌رسد که نشان‌دهنده همگرایی مدل روی داده‌های آموزش است.

در این مثال، مدل با دو ورودی و دو خروجی ساخته شده است چون داده ما دو ویژگی و دو کلاس دارد. از بهینه‌ساز گرادیان کاهشی تصادفی (SGD) با نرخ یادگیری 0.5 استفاده شده است. نرخ یادگیری یک ابرپارامتر است که باید با آزمون و خطا تنظیم شود تا بهترین همگرایی حاصل شود. همچنین تعداد epoch ها نیز یک ابرپارامتر است.

---

**تمرین A.3**
شبکه عصبی معرفی شده در لیست A.9 چند پارامتر دارد؟

---

در عمل معمولاً از یک مجموعه داده سوم به نام مجموعه اعتبارسنجی (validation dataset) برای انتخاب بهترین تنظیمات ابرپارامترها استفاده می‌کنیم. مجموعه اعتبارسنجی مشابه مجموعه تست است ولی می‌توانیم چندین بار از آن استفاده کنیم، در حالی که مجموعه تست فقط یک بار برای ارزیابی نهایی به کار می‌رود.

---

**حالت‌های `model.train()` و `model.eval()`**
این دستورات مدل را به حالت آموزش یا ارزیابی می‌برند. این تفاوت برای بخش‌هایی از مدل که در حالت آموزش و ارزیابی رفتار متفاوتی دارند (مانند dropout یا batch normalization) اهمیت دارد.
در شبکه ما که چنین لایه‌هایی ندارد، استفاده از این دستورات ضروری نیست ولی به عنوان بهترین روش توصیه می‌شود تا در تغییر معماری یا استفاده مجدد از کد، مشکلی پیش نیاید.

---

تابع `cross_entropy` درون خود عملیات softmax را انجام می‌دهد تا کارایی و پایداری عددی حفظ شود.

`loss.backward()` گرادیان‌ها را محاسبه می‌کند و `optimizer.step()` پارامترها را بر اساس این گرادیان‌ها به‌روزرسانی می‌کند. برای SGD، پارامترها بر اساس گرادیان ضربدر نرخ یادگیری اصلاح می‌شوند.

> **نکته:** برای جلوگیری از انباشت ناخواسته گرادیان‌ها، حتماً در هر مرحله آموزش باید `optimizer.zero_grad()` فراخوانی شود.

---

پس از آموزش مدل، می‌توانیم پیش‌بینی‌ها را به شکل زیر انجام دهیم:

```python
model.eval()
with torch.no_grad():
    outputs = model(X_train)
print(outputs)
```

خروجی:

```
tensor([[ 2.8569, -4.1618],
        [ 2.5382, -3.7548],
        [ 2.0944, -3.1820],
        [-1.4814,  1.4816],
        [-1.7176,  1.7342]])
```

برای به دست آوردن احتمال عضویت در کلاس‌ها، از تابع softmax استفاده می‌کنیم:

```python
torch.set_printoptions(sci_mode=False)
probas = torch.softmax(outputs, dim=1)
print(probas)
```

خروجی:

```
tensor([[    0.9991,     0.0009],
        [    0.9982,     0.0018],
        [    0.9949,     0.0051],
        [    0.0491,     0.9509],
        [    0.0307,     0.9693]])
```

مثلاً در ردیف اول، احتمال تعلق به کلاس ۰ برابر ۹۹.۹۱٪ و کلاس ۱ برابر ۰.۰۹٪ است.

---

برای گرفتن برچسب کلاس نهایی، از تابع `argmax` استفاده می‌کنیم که بزرگ‌ترین مقدار را در هر ردیف برمی‌گرداند:

```python
predictions = torch.argmax(probas, dim=1)
print(predictions)
```

خروجی:

```
tensor([0, 0, 0, 1, 1])
```

در واقع نیازی به محاسبه softmax برای گرفتن برچسب نیست و می‌توان مستقیم روی logits تابع argmax را اعمال کرد:

```python
predictions = torch.argmax(outputs, dim=1)
print(predictions)
```

خروجی مشابه بالا است.

---

از آنجا که داده آموزشی کوچک است، می‌توان پیش‌بینی‌ها را با برچسب‌های واقعی مقایسه کرد:

```python
predictions == y_train
```

خروجی:

```
tensor([True, True, True, True, True])
```

و تعداد پیش‌بینی‌های صحیح با `torch.sum` محاسبه می‌شود:

```python
torch.sum(predictions == y_train)
```

خروجی:

```
5
```

که نشان می‌دهد دقت پیش‌بینی ۱۰۰٪ است.

---

برای محاسبه دقت پیش‌بینی به صورت کلی‌تر، تابع زیر تعریف می‌شود:

```python
def compute_accuracy(model, dataloader):
    model = model.eval()
    correct = 0.0
    total_examples = 0
    for idx, (features, labels) in enumerate(dataloader):
        with torch.no_grad():
            logits = model(features)
        predictions = torch.argmax(logits, dim=1)
        compare = labels == predictions       #1
        correct += torch.sum(compare)      #2
        total_examples += len(compare)
    return (correct / total_examples).item()    #3
```

توضیح کد:

> #1 خروجی مقایسه‌ای از درستی پیش‌بینی‌ها (True/False) است.  
> #2 تعداد موارد درست را جمع می‌کند.  
> #3 دقت کل را به صورت عدد بین ۰ تا ۱ برمی‌گرداند.

این تابع با استفاده از data loader داده‌ها را به دسته‌های کوچک تقسیم می‌کند و به همین دلیل می‌تواند برای داده‌های بزرگ هم کارآمد باشد.

---

اجرای تابع برای داده آموزش:

```python
print(compute_accuracy(model, train_loader))
```

خروجی:

```
1.0
```

و برای داده تست:

```python
print(compute_accuracy(model, test_loader))
```

خروجی:

```
1.0
```

---

## بخش A.8 ذخیره و بارگذاری مدل‌ها

پس از آموزش مدل، معمولاً نیاز داریم آن را ذخیره کنیم تا بعدها دوباره استفاده کنیم. روش پیشنهادی در PyTorch به صورت زیر است:

```python
torch.save(model.state_dict(), "model.pth")
```

`state_dict` یک دیکشنری پایتونی است که هر لایه مدل را به پارامترهای قابل آموزش (وزن‌ها و بایاس‌ها) نگاشت می‌کند. نام فایل می‌تواند دلخواه باشد، ولی `.pth` یا `.pt` رایج‌ترین پسوندها هستند.

---

برای بارگذاری مدل ذخیره شده:

```python
model = NeuralNetwork(2, 2)
model.load_state_dict(torch.load("model.pth"))
```

تابع `torch.load` فایل را می‌خواند و دیکشنری پارامترها را بازسازی می‌کند و `load_state_dict` آنها را روی مدل اعمال می‌کند.

خط `model = NeuralNetwork(2, 2)` لازم است مگر اینکه در همان جلسه کدی اجرا شود که مدل ذخیره شده در آن ساخته شده باشد. معماری مدل جدید باید دقیقاً با مدل ذخیره شده یکسان باشد.

---

## بخش A.9 بهینه‌سازی عملکرد آموزش با استفاده از GPUها

در ادامه، نحوه استفاده از GPUها را بررسی می‌کنیم. GPUها در مقایسه با CPUهای معمولی، سرعت آموزش شبکه‌های عصبی عمیق را به طور قابل توجهی افزایش می‌دهند. ابتدا مفاهیم اصلی مربوط به محاسبات GPU در PyTorch را مرور می‌کنیم، سپس آموزش مدل روی یک GPU را نشان می‌دهیم و در نهایت به آموزش توزیع‌شده روی چند GPU می‌پردازیم.

---

### A.9.1 محاسبات PyTorch روی دستگاه‌های GPU

تغییر حلقه آموزش برای اجرا به صورت اختیاری روی GPU نسبتا ساده است و فقط نیازمند تغییر سه خط کد است (به بخش A.7 مراجعه شود). پیش از انجام این تغییرات، لازم است مفهوم اصلی محاسبات GPU در PyTorch را درک کنیم. در PyTorch، دستگاه (device) محلی است که محاسبات در آن انجام شده و داده‌ها روی آن قرار دارند. CPU و GPU نمونه‌هایی از دستگاه‌ها هستند. یک Tensor در PyTorch روی یک دستگاه قرار دارد و عملیات روی آن Tensor نیز روی همان دستگاه اجرا می‌شود.

برای درک بهتر، فرض کنید نسخه‌ای از PyTorch که قابلیت استفاده از GPU را دارد نصب کرده‌اید (به بخش A.1.3 مراجعه شود). می‌توانیم با اجرای کد زیر بررسی کنیم که محیط ما پشتیبانی از GPU را دارد یا خیر:

```python
print(torch.cuda.is_available())
```

خروجی:

```
True
```

حال، فرض کنیم دو Tensor داریم که می‌خواهیم آن‌ها را جمع کنیم. به طور پیش‌فرض این محاسبه روی CPU انجام می‌شود:

```python
tensor_1 = torch.tensor([1., 2., 3.])
tensor_2 = torch.tensor([4., 5., 6.])
print(tensor_1 + tensor_2)
```

خروجی:

```
tensor([5., 7., 9.])
```

اکنون می‌توانیم از متد `.to()` استفاده کنیم، که مشابه تغییر نوع داده Tensor است (به بخش 2.2.2 مراجعه شود)، تا این Tensorها را روی GPU منتقل کنیم و جمع آن‌ها را روی GPU انجام دهیم:

```python
tensor_1 = tensor_1.to("cuda")
tensor_2 = tensor_2.to("cuda")
print(tensor_1 + tensor_2)
```

خروجی:

```
tensor([5., 7., 9.], device='cuda:0')
```

در خروجی، اطلاعات دستگاه به صورت `device='cuda:0'` آمده که نشان می‌دهد Tensorها روی اولین GPU قرار دارند. اگر چند GPU داشته باشید، می‌توانید مشخص کنید کدام GPU مورد نظر است، مثلاً `.to("cuda:0")` یا `.to("cuda:1")` و غیره.

نکته مهم: همه Tensorها باید روی یک دستگاه باشند، در غیر این صورت محاسبه با خطا مواجه می‌شود. برای مثال اگر یک Tensor روی CPU و دیگری روی GPU باشد:

```python
tensor_1 = tensor_1.to("cpu")
print(tensor_1 + tensor_2)
```

خروجی خطا:

```
RuntimeError: Expected all tensors to be on the same device, but found at
least two devices, cuda:0 and cpu!
```

پس در نهایت فقط کافی است Tensorها را روی همان دستگاه GPU منتقل کنیم تا PyTorch بقیه کارها را انجام دهد.

---

### زیربخش A.9.2 آموزش روی یک GPU

حالا که انتقال Tensorها به GPU را آموختیم، می‌توانیم حلقه آموزش را طوری تغییر دهیم که روی GPU اجرا شود. این کار فقط سه خط کد را تغییر می‌دهد، همانطور که در کد زیر مشاهده می‌کنید:

```python
torch.manual_seed(123)
model = NeuralNetwork(num_inputs=2, num_outputs=2)
device = torch.device("cuda")      #1
model = model.to(device)            #2
optimizer = torch.optim.SGD(model.parameters(), lr=0.5)
num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    for batch_idx, (features, labels) in enumerate(train_loader):
        features, labels = features.to(device), labels.to(device)   #3
        logits = model(features)
        loss = F.cross_entropy(logits, labels) # Loss function
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        ### LOGGING
        print(f"Epoch: {epoch+1:03d}/{num_epochs:03d}"
              f" | Batch {batch_idx:03d}/{len(train_loader):03d}"
              f" | Train/Val Loss: {loss:.2f}")
    model.eval()
    # Insert optional model evaluation code
```

توضیحات:

> #1 تعریف متغیر دستگاه به عنوان GPU  
> #2 انتقال مدل به GPU  
> #3 انتقال داده‌ها (ویژگی‌ها و برچسب‌ها) به GPU

اجرای این کد خروجی مشابه اجرای روی CPU (بخش A.7) خواهد داشت:

```
Epoch: 001/003 | Batch 000/002 | Train/Val Loss: 0.75
Epoch: 001/003 | Batch 001/002 | Train/Val Loss: 0.65
Epoch: 002/003 | Batch 000/002 | Train/Val Loss: 0.44
Epoch: 002/003 | Batch 001/002 | Train/Val Loss: 0.13
Epoch: 003/003 | Batch 000/002 | Train/Val Loss: 0.03
Epoch: 003/003 | Batch 001/002 | Train/Val Loss: 0.00
```

می‌توان به جای تعریف `device = torch.device("cuda")` مستقیما از `.to("cuda")` استفاده کرد که کوتاه‌تر است (بخش A.9.1). همچنین می‌توان کد را به گونه‌ای تغییر داد که اگر GPU موجود نبود، روی CPU اجرا شود؛ این یک روش استاندارد برای به اشتراک‌گذاری کد PyTorch است:

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

در حالت آموزش مدل‌های کوچک مانند این، ممکن است به دلیل هزینه انتقال داده‌ها بین CPU و GPU افزایش سرعت محسوسی نداشته باشیم، اما در آموزش شبکه‌های عصبی عمیق و مدل‌های بزرگ (مانند LLMها) سرعت قابل توجهی مشاهده می‌شود.

---

#### نکته مخصوص کاربران macOS

اگر از مک‌های اپل با چیپ Apple Silicon (مثل M1, M2, M3 و مدل‌های جدیدتر) استفاده می‌کنید که به جای GPU انویدیا، چیپ اختصاصی دارند، می‌توانید دستور زیر را به جای آنچه در بالا ذکر شد به کار ببرید تا از این چیپ بهره ببرید:

```python
device = torch.device(
    "mps" if torch.backends.mps.is_available() else "cpu"
)
```

---

**تمرین A.4**

زمان اجرای ضرب ماتریس روی CPU و GPU را مقایسه کنید. در چه ابعادی از ماتریس‌ها سرعت ضرب روی GPU از CPU بیشتر می‌شود؟
راهنما: در محیط Jupyter از دستور `%timeit` برای مقایسه زمان اجرا استفاده کنید. برای مثال، اگر ماتریس‌های `a` و `b` تعریف شده باشند، دستور `%timeit a @ b` را در یک سلول جدید اجرا کنید.


> [ 
    7.تنظیم دقیق برای پیروی از دستورالعمل‌ها
     (قبلی) ](
        <07.Fine-tuning to follow instructions.md>
        ) <پیوست A>
    [
    پیوست B
(بعدی)
](<B.appendix.md>)