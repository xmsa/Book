<!-- language: rtl -->


# 1.درک مدل‌های زبانی بزرگ

مدل‌های زبانی بزرگ (LLM) مانند مدل‌های ارائه شده توسط ChatGPT شرکت OpenAI، مدل‌هایی مبتنی بر شبکه‌های عصبی عمیق هستند که در سال‌های اخیر توسعه یافته‌اند. این مدل‌ها نقطه عطفی در پردازش زبان طبیعی (NLP) به شمار می‌روند. پیش از ظهور این مدل‌ها، روش‌های سنتی در انجام وظایفی مانند دسته‌بندی ایمیل‌های اسپم و شناسایی الگوهای ساده که با قوانین دستی یا مدل‌های ساده قابل انجام بود، عملکرد مناسبی داشتند. اما این روش‌ها در وظایف پیچیده‌تر زبانی که نیازمند درک عمیق و تولید متن بود، مانند تجزیه و تحلیل دستورات دقیق، تحلیل متنی بر اساس زمینه و ایجاد متنی منسجم و متناسب با موقعیت، معمولاً ضعیف عمل می‌کردند. برای مثال، نسل‌های پیشین مدل‌های زبانی قادر به نگارش ایمیل از روی لیستی از کلمات کلیدی نبودند، کاری که برای مدل‌های امروزی بسیار ساده است.

مدل‌های زبانی بزرگ توانایی‌های چشمگیری در درک، تولید و تفسیر زبان انسانی دارند. البته باید توجه داشت که وقتی می‌گوییم این مدل‌ها «درک» دارند، منظور این نیست که آنها دارای هوشیاری یا فهم انسانی هستند، بلکه فقط قادرند متنی تولید کنند که از نظر ظاهری هماهنگ و مرتبط با زمینه باشد.

این مدل‌ها که با پیشرفت‌های یادگیری عمیق—شاخه‌ای از یادگیری ماشین و هوش مصنوعی که روی شبکه‌های عصبی متمرکز است—امکان‌پذیر شده‌اند، با استفاده از حجم بسیار بزرگی از داده‌های متنی آموزش داده شده‌اند. این آموزش وسیع باعث شده است تا مدل‌ها بتوانند اطلاعات عمیق‌تر و ظرایف زبان انسانی را نسبت به روش‌های پیشین بهتر درک کنند. در نتیجه، عملکرد این مدل‌ها در بسیاری از وظایف NLP از جمله ترجمه متن، تحلیل احساسات، پاسخ به پرسش‌ها و موارد دیگر به طور قابل توجهی بهبود یافته است.

تفاوت مهم دیگر میان مدل‌های زبانی بزرگ و مدل‌های NLP قدیمی‌تر این است که مدل‌های پیشین معمولاً برای وظایف خاصی مانند دسته‌بندی متن یا ترجمه زبان طراحی شده بودند. در حالی که آن مدل‌ها در حوزه‌های محدود خود موفق بودند، مدل‌های بزرگ توانایی انجام گسترده‌تری از وظایف NLP را دارند.

موفقیت این مدل‌ها به معماری ترنسفورمر بازمی‌گردد که پایه بسیاری از مدل‌های بزرگ است، و همچنین حجم عظیم داده‌هایی که برای آموزش آن‌ها استفاده می‌شود، که به مدل‌ها امکان می‌دهد ظرایف و الگوهای مختلف زبانی را که به سختی قابل برنامه‌نویسی دستی هستند، فرا بگیرند.

این تغییر به سوی استفاده از مدل‌های مبتنی بر معماری ترنسفورمر و آموزش با داده‌های گسترده، تحول بنیادینی در NLP ایجاد کرده است و ابزارهای قدرتمندتری برای درک و تعامل با زبان انسانی فراهم آورده است.

هدف این بحث پایه‌گذاری مفاهیم لازم برای رسیدن به هدف اصلی این کتاب است: درک مدل‌های زبانی بزرگ از طریق پیاده‌سازی گام به گام یک مدل مشابه ChatGPT بر اساس معماری ترنسفورمر در قالب کد.

---

## 1.1 مدل زبانی بزرگ (LLM) چیست؟

مدل زبانی بزرگ، شبکه‌ای عصبی است که برای درک، تولید و پاسخگویی به متن‌های مشابه زبان انسان طراحی شده است. این مدل‌ها شبکه‌های عصبی عمیقی هستند که روی حجم عظیمی از داده‌های متنی آموزش داده شده‌اند، گاهی شامل بخش‌های بزرگی از متن‌های در دسترس عمومی اینترنت.

اصطلاح «بزرگ» در مدل‌های زبانی بزرگ هم به اندازه مدل (تعداد پارامترها) و هم به حجم داده‌های آموزشی آن اشاره دارد. این مدل‌ها معمولاً شامل ده‌ها یا حتی صدها میلیارد پارامتر هستند؛ پارامترهایی که وزن‌های قابل تنظیم شبکه‌اند و طی آموزش برای پیش‌بینی کلمه بعدی در یک جمله بهینه می‌شوند.

پیش‌بینی کلمه بعدی یک کار منطقی است، زیرا زبان ذاتاً متوالی است و این روش به مدل کمک می‌کند تا زمینه، ساختار و روابط در متن را یاد بگیرد. با این حال، این کار بسیار ساده به نظر می‌رسد و برای بسیاری از پژوهشگران تعجب‌آور است که چگونه از همین روش ساده چنین مدل‌های قدرتمندی به دست آمده است. در فصل‌های بعدی، روند آموزش برای پیش‌بینی کلمه بعدی را به طور گام به گام بررسی و پیاده‌سازی خواهیم کرد.

مدل‌های زبانی بزرگ از معماری ترنسفورمر بهره می‌برند که به آن‌ها امکان می‌دهد به صورت انتخابی به بخش‌های مختلف ورودی توجه کنند و پیش‌بینی‌های دقیق‌تری انجام دهند، که این ویژگی آن‌ها را در مدیریت ظرایف و پیچیدگی‌های زبان انسانی بسیار توانمند می‌کند.

از آنجا که این مدل‌ها قادر به تولید متن هستند، به آنها نوعی هوش مصنوعی مولد (generative AI یا GenAI) نیز گفته می‌شود. شکل ۱.۱ ارتباط بین حوزه‌های مختلف هوش مصنوعی را نشان می‌دهد، که شامل توسعه ماشین‌هایی است که قادر به انجام کارهایی مشابه هوش انسانی هستند، مانند درک زبان، شناسایی الگو و تصمیم‌گیری، و شامل زیرشاخه‌هایی مانند یادگیری ماشین و یادگیری عمیق است.

---

الگوریتم‌های به کار رفته در هوش مصنوعی، موضوع اصلی یادگیری ماشین هستند. یادگیری ماشین به توسعه الگوریتم‌هایی می‌پردازد که می‌توانند از داده‌ها بیاموزند و بدون برنامه‌نویسی صریح، پیش‌بینی یا تصمیم‌گیری کنند. به عنوان مثال، یک فیلتر اسپم کاربردی از یادگیری ماشین است که به جای تعریف قوانین دستی، با نمونه‌هایی از ایمیل‌های اسپم و غیر اسپم آموزش داده می‌شود و با کمینه کردن خطا در داده‌های آموزشی، الگوها و ویژگی‌های مشخص کننده اسپم را یاد می‌گیرد.

همان‌طور که در شکل ۱.۱ نشان داده شده است، یادگیری عمیق زیرمجموعه‌ای از یادگیری ماشین است که روی استفاده از شبکه‌های عصبی با سه یا چند لایه (شبکه‌های عصبی عمیق) برای مدل‌سازی الگوها و انتزاعات پیچیده داده تمرکز دارد. برخلاف یادگیری عمیق، یادگیری ماشین سنتی نیازمند استخراج دستی ویژگی‌ها است، یعنی کارشناسان باید مهم‌ترین ویژگی‌ها را شناسایی و انتخاب کنند.

هرچند یادگیری ماشین و یادگیری عمیق امروزه بخش اعظم هوش مصنوعی را تشکیل می‌دهند، اما روش‌های دیگر مانند سیستم‌های مبتنی بر قوانین، الگوریتم‌های ژنتیکی، سیستم‌های خبره، منطق فازی یا استدلال نمادین نیز وجود دارند.

در مثال فیلتر اسپم، در یادگیری ماشین سنتی، کارشناسان ویژگی‌هایی مانند تکرار کلمات خاص (مثلاً «جایزه»، «برد»، «رایگان»)، تعداد علامت تعجب، استفاده از حروف بزرگ یا وجود لینک‌های مشکوک را استخراج می‌کنند و مدل بر اساس این داده‌ها آموزش می‌بیند. اما در یادگیری عمیق استخراج ویژگی دستی لازم نیست، هرچند برای هر دو روش نیاز به داده‌های برچسب‌دار (اسپم یا غیر اسپم) وجود دارد که باید توسط کارشناسان یا کاربران فراهم شود.

در ادامه، به برخی از مشکلاتی که مدل‌های زبانی بزرگ امروزه می‌توانند حل کنند، چالش‌های موجود و معماری کلی این مدل‌ها که در ادامه پیاده‌سازی خواهیم کرد، خواهیم پرداخت.

## ۱.۲ کاربردهای مدل‌های زبانی بزرگ (LLMs)

با توجه به توانمندی‌های پیشرفته مدل‌های زبانی بزرگ در پردازش و درک داده‌های متنی غیرساخت‌یافته، این مدل‌ها در حوزه‌های متنوعی کاربرد گسترده‌ای یافته‌اند. امروزه از LLMها برای ترجمه ماشینی، تولید متون نوآورانه (شکل ۱.۲)، تحلیل احساسات، خلاصه‌سازی متن و بسیاری وظایف دیگر استفاده می‌شود. این مدل‌ها اخیراً در خلق محتوا، از جمله نگارش داستان‌های تخیلی، مقالات و حتی کدنویسی رایانه‌ای کاربرد یافته‌اند. همچنین LLMها توانمند هستند تا چت‌بات‌ها و دستیارهای مجازی پیچیده‌ای مانند ChatGPT از OpenAI یا Gemini (که پیش‌تر Bard نام داشت) را پشتیبانی کنند؛ این سیستم‌ها می‌توانند به پرسش‌های کاربران پاسخ دهند و عملکرد موتورهای جستجوی سنتی مانند Google Search و Microsoft Bing را تقویت کنند.

علاوه بر این، LLMها می‌توانند در بازیابی موثر دانش از حجم‌های عظیم متون تخصصی در حوزه‌هایی مانند پزشکی یا حقوق کاربرد داشته باشند؛ از جمله جست‌وجو در اسناد، خلاصه‌سازی بخش‌های طولانی و پاسخ به سوالات تخصصی. به طور خلاصه، LLMها ابزارهای ارزشمندی برای خودکارسازی تقریباً هر وظیفه‌ای هستند که شامل پردازش و تولید متن می‌شود. دامنه کاربردهای آن‌ها تقریباً نامحدود است و با پیشرفت‌های مستمر، این مدل‌ها می‌توانند نحوه تعامل ما با فناوری را تغییر دهند و آن را بیشتر محاوره‌ای، شهودی و در دسترس کنند.

تمرکز ما بر درک ساختاری نحوه عملکرد LLMها از پایه است؛ به گونه‌ای که بتوانیم یک مدل تولیدکننده متن را کدنویسی کنیم. همچنین تکنیک‌هایی را می‌آموزید که به مدل اجازه می‌دهد انواع پرسش‌ها را پاسخ دهد، از خلاصه‌سازی و ترجمه متن تا سایر وظایف مرتبط. به عبارت دیگر، با ساخت مرحله به مرحله یک دستیار پیشرفته مانند ChatGPT، فرآیند کارکرد این مدل‌ها را به طور عملی یاد خواهید گرفت.

---

## ۱.۳ مراحل ساخت و استفاده از مدل‌های زبانی بزرگ

چرا باید مدل‌های زبانی بزرگ خودمان را بسازیم؟ کدنویسی یک LLM از پایه تمرینی عالی برای درک مکانیزم‌ها و محدودیت‌های آن است. همچنین این دانش لازم برای پیش‌آموزش (pretraining) یا تنظیم دقیق (fine-tuning) مدل‌های متن‌باز موجود را فراهم می‌کند تا بتوانیم آن‌ها را برای داده‌ها یا وظایف تخصصی خودمان سفارشی کنیم.

**نکته:** اکثر مدل‌های LLM امروزه با استفاده از کتابخانه یادگیری عمیق PyTorch پیاده‌سازی می‌شوند که در این کتاب نیز از آن استفاده خواهیم کرد. معرفی کامل PyTorch در ضمیمه A آمده است.

تحقیقات نشان داده‌اند که مدل‌های سفارشی‌سازی‌شده (تخصصی برای حوزه‌ها یا وظایف خاص) می‌توانند در عملکرد از مدل‌های عمومی مانند ChatGPT که برای کاربردهای گسترده طراحی شده‌اند، پیشی بگیرند. نمونه‌هایی از این مدل‌ها شامل BloombergGPT (تخصصی در حوزه مالی) و مدل‌های پزشکی برای پاسخگویی به سوالات تخصصی هستند (برای جزئیات بیشتر به ضمیمه B مراجعه کنید).

استفاده از مدل‌های سفارشی مزایای متعددی دارد، به ویژه در زمینه حفظ حریم خصوصی داده‌ها. برای مثال، شرکت‌ها ممکن است به دلیل نگرانی‌های محرمانگی تمایلی به اشتراک‌گذاری داده‌های حساس خود با ارائه‌دهندگان شخص ثالث مانند OpenAI نداشته باشند. علاوه بر این، توسعه مدل‌های کوچکتر و تخصصی امکان اجرای مستقیم آن‌ها روی دستگاه‌های مشتری مانند لپ‌تاپ و گوشی‌های هوشمند را فراهم می‌کند؛ رویکردی که شرکت‌هایی مانند اپل به آن علاقه‌مند هستند. این روش اجرای محلی می‌تواند به طور چشمگیری زمان پاسخ‌دهی را کاهش داده و هزینه‌های سرور را کم کند. همچنین، مدل‌های سفارشی به توسعه‌دهندگان آزادی کامل در به‌روزرسانی و تغییرات مدل را می‌دهد.

فرآیند کلی ساخت یک LLM شامل دو مرحله اصلی است: پیش‌آموزش و تنظیم دقیق. پیش‌آموزش به معنای مرحله ابتدایی آموزش مدل روی مجموعه داده‌ای بزرگ و متنوع است تا مدل درک وسیعی از زبان کسب کند. این مدل پیش‌آموزش‌شده به عنوان پایه‌ای برای مرحله بعدی—تنظیم دقیق—عمل می‌کند. در تنظیم دقیق، مدل روی داده‌های محدودتر و تخصصی‌تر برای وظایف یا حوزه‌های مشخص آموزش داده می‌شود. این دو مرحله در شکل ۱.۳ نمایش داده شده‌اند.

---

در گام اول ساخت LLM، مدل روی یک مجموعه بزرگ از متن‌های خام (متون بدون برچسب) آموزش داده می‌شود. معمولاً در این مرحله داده‌ها فقط شامل متن عادی هستند و برچسب‌گذاری خاصی ندارند (البته ممکن است فرآیندهایی مانند حذف کاراکترهای فرمت‌بندی شده یا متون به زبان‌های نامشخص انجام شود).

**نکته:** برای خوانندگان آشنا با یادگیری ماشین، این نکته حائز اهمیت است که برخلاف مدل‌های یادگیری ماشین سنتی که معمولاً نیازمند داده‌های برچسب‌دار هستند، در پیش‌آموزش LLMها از یادگیری خودنظارتی استفاده می‌شود. یعنی مدل خودش از داده ورودی برچسب‌سازی می‌کند.

این مرحله پیش‌آموزش که به ایجاد یک مدل پایه یا foundation model منجر می‌شود، نمونه‌ای مانند GPT-3 (مدل پیشین ChatGPT) را شامل می‌شود. این مدل توانایی تکمیل جملات نیمه‌کامل را دارد و قابلیت یادگیری چندنمونه‌ای محدود (few-shot learning) را نیز داراست، به این معنا که می‌تواند با چند نمونه اندک وظایف جدید را یاد بگیرد.

پس از پیش‌آموزش، می‌توان مدل را با داده‌های برچسب‌دار به صورت تنظیم دقیق آموزش داد. دو دسته اصلی تنظیم دقیق عبارتند از: تنظیم دقیق مبتنی بر دستورالعمل (instruction fine-tuning) و تنظیم دقیق مبتنی بر دسته‌بندی (classification fine-tuning). در نوع اول، داده‌ها شامل جفت‌های دستور و پاسخ هستند، مثلاً یک پرسش برای ترجمه متن به همراه ترجمه صحیح آن. در نوع دوم، داده‌ها شامل متن‌ها به همراه برچسب‌های کلاس مرتبط هستند، مانند ایمیل‌هایی که با برچسب «اسپم» یا «غیر اسپم» مشخص شده‌اند.

در ادامه، به پیاده‌سازی کدهای پیش‌آموزش و تنظیم دقیق LLM خواهیم پرداخت و به طور مفصل به جزئیات هر دو نوع تنظیم دقیق پس از ساخت مدل پایه خواهیم پرداخت.

## ۱.۴ معرفی معماری ترنسفورمر

اکثر مدل‌های زبانی بزرگ امروزی مبتنی بر معماری ترنسفورمر هستند، که یک ساختار شبکه عصبی عمیق است و برای نخستین بار در مقاله‌ای با عنوان «توجه، تنها چیزی است که نیاز دارید» (۲۰۱۷) معرفی شد ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)). برای درک کامل LLMها، لازم است ابتدا معماری اصلی ترنسفورمر را که برای ترجمه ماشینی، از جمله ترجمه متون انگلیسی به آلمانی و فرانسوی توسعه یافته بود، بشناسیم. نسخه‌ای ساده‌شده از این معماری در شکل ۱.۴ نمایش داده شده است.

معماری ترنسفورمر از دو زیرماژول اصلی تشکیل شده است: انکودر (رمزگذار) و دیکودر (رمزگشا). بخش انکودر متن ورودی را پردازش کرده و آن را به یک سری نمایش‌های عددی یا بردار تبدیل می‌کند که اطلاعات زمینه‌ای (contextual) متن را در خود جای داده‌اند. سپس دیکودر این بردارها را دریافت کرده و متن خروجی را تولید می‌کند. به عنوان مثال، در وظیفه ترجمه، انکودر متن زبان مبدأ را به بردارهایی تبدیل می‌کند و دیکودر آن‌ها را به متن زبان مقصد رمزگشایی می‌کند. هر دو بخش انکودر و دیکودر از چندین لایه تشکیل شده‌اند که با مکانیزم «توجه به خود» (self-attention) به یکدیگر متصل شده‌اند. احتمالاً سوالاتی درباره نحوه پیش‌پردازش ورودی‌ها و رمزگذاری آن‌ها دارید که در فصل‌های بعدی و از طریق پیاده‌سازی مرحله به مرحله پاسخ داده خواهد شد.

یکی از اجزای کلیدی ترنسفورمرها و LLMها، مکانیزم توجه به خود است که به مدل اجازه می‌دهد اهمیت نسبی هر کلمه یا توکن در یک توالی را نسبت به سایر قسمت‌ها بسنجد. این مکانیزم به مدل امکان می‌دهد وابستگی‌ها و روابط زمینه‌ای در متن را حتی در فواصل طولانی تشخیص دهد و در نتیجه خروجی‌های همبسته و مرتبط با متن ورودی تولید کند. به دلیل پیچیدگی این مکانیزم، توضیحات کامل آن به فصل سوم موکول شده است، جایی که به صورت گام‌به‌گام آن را بررسی و پیاده‌سازی خواهیم کرد.

---

شکل ۱.۴ نمای ساده‌شده‌ای از معماری اصلی ترنسفورمر را نشان می‌دهد که برای ترجمه متون طراحی شده است. این معماری شامل دو بخش است: (الف) انکودر که متن ورودی را پردازش و به یک نمایش عددی (بردار) تبدیل می‌کند و (ب) دیکودر که از این نمایش برای تولید متن ترجمه شده، کلمه به کلمه، استفاده می‌کند. این شکل مرحله نهایی ترجمه را نشان می‌دهد که دیکودر تنها کلمه پایانی جمله را تولید می‌کند (کلمه «Beispiel» به معنی «مثال») با فرض اینکه ورودی اصلی «This is an example» و جمله نیمه‌تمام «Das ist ein» باشد.

---

نسخه‌های بعدی معماری ترنسفورمر، مانند BERT (مخفف Bidirectional Encoder Representations from Transformers) و مدل‌های مختلف GPT (مخفف Generative Pretrained Transformers)، بر پایه همین مفهوم ساخته شده‌اند و برای کاربردهای متفاوتی سازگار شده‌اند. برای مطالعه بیشتر، به ضمیمه B مراجعه کنید.

مدل BERT که بر اساس زیرماژول انکودر ترنسفورمر اصلی ساخته شده، در روش آموزش با GPT متفاوت است. GPT برای وظایف تولید متن طراحی شده، در حالی که BERT و نسخه‌های آن تخصصی در پیش‌بینی کلمات ماسک‌شده (مخفی شده) هستند؛ به این صورت که مدل کلمات پنهان در جمله را پیش‌بینی می‌کند (شکل ۱.۵). این روش آموزش، BERT را برای وظایف دسته‌بندی متن، مانند پیش‌بینی احساسات و طبقه‌بندی اسناد، قدرتمند می‌سازد. به عنوان نمونه‌ای از کاربرد آن، در زمان نگارش این متن، پلتفرم X (که پیش‌تر توییتر بود) از BERT برای شناسایی محتوای مخرب استفاده می‌کند.

از سوی دیگر، GPT بر بخش دیکودر معماری ترنسفورمر تمرکز دارد و برای وظایفی که نیاز به تولید متن دارند، مناسب است. این وظایف شامل ترجمه ماشینی، خلاصه‌سازی متن، نوشتن داستان، کدنویسی و موارد دیگر است.

---

شکل ۱.۵ نمایی تصویری از زیرماژول‌های انکودر و دیکودر را نشان می‌دهد: بخش چپ نمایانگر LLMهای شبیه BERT است که به پیش‌بینی کلمات ماسک شده تمرکز دارند و عمدتاً در دسته‌بندی متن کاربرد دارند، و بخش راست LLMهای شبیه GPT را نشان می‌دهد که برای وظایف تولید متن طراحی شده‌اند.

---

مدل‌های GPT که عمدتاً برای تکمیل متن آموزش دیده‌اند، توانایی قابل توجهی در انجام وظایف zero-shot و few-shot دارند. یادگیری zero-shot به معنای توانایی تعمیم به وظایف کاملاً جدید بدون نمونه‌های قبلی است. یادگیری few-shot به معنای یادگیری بر اساس تعداد محدودی نمونه ورودی است که کاربر ارائه می‌دهد (شکل ۱.۶).

---

شکل ۱.۶ نشان می‌دهد که LLMهای شبیه GPT علاوه بر تکمیل متن، می‌توانند وظایف متنوعی را بدون نیاز به آموزش مجدد، تنظیم دقیق یا تغییر ساختار مدل انجام دهند. گاهی ارائه نمونه‌های هدف در ورودی (few-shot) مفید است، اما این مدل‌ها قادرند بدون نمونه خاصی (zero-shot) نیز وظایف را اجرا کنند.

---

### تفاوت ترنسفورمر و LLM

امروزه LLMها بر اساس معماری ترنسفورمر ساخته شده‌اند، بنابراین در بسیاری از منابع این دو واژه به صورت مترادف به کار می‌روند. اما توجه داشته باشید که همه ترنسفورمرها LLM نیستند، زیرا از ترنسفورمرها در حوزه‌هایی مانند بینایی ماشین نیز استفاده می‌شود. همچنین همه LLMها بر پایه ترنسفورمر نیستند؛ برخی LLMها مبتنی بر معماری‌های بازگشتی (Recurrent) و کانولوشنی (Convolutional) هستند که هدف آن‌ها بهبود کارایی محاسباتی است. هنوز مشخص نیست که آیا این معماری‌های جایگزین می‌توانند با قدرت ترنسفورمرها رقابت کنند و در عمل پذیرفته شوند یا خیر. برای ساده‌سازی، در این متن واژه «LLM» به مدل‌های مبتنی بر ترنسفورمر شبیه GPT اشاره دارد.

مطالعه بیشتر درباره این معماری‌ها را می‌توانید در ضمیمه B بیابید.

---

## ۱.۵ استفاده از داده‌های بزرگ

داده‌های آموزشی بزرگ مدل‌های محبوبی مانند GPT و BERT شامل مجموعه‌ای گسترده و متنوع از متون با میلیاردها کلمه هستند که موضوعات مختلف و زبان‌های طبیعی و برنامه‌نویسی را دربر می‌گیرند. به عنوان مثال مشخص، جدول ۱.۱ داده‌های مورد استفاده برای پیش‌آموزش مدل GPT-3 را نشان می‌دهد که پایه نسخه اول ChatGPT بوده است.

در جدول ۱.۱ تعداد توکن‌ها گزارش شده است. توکن واحدی از متن است که مدل آن را می‌خواند و تعداد توکن‌ها تقریباً برابر با تعداد کلمات و علائم نگارشی موجود در متن است. در فصل دوم، فرایند تبدیل متن به توکن (توکن‌سازی) توضیح داده شده است. نکته اصلی این است که حجم و تنوع داده‌های آموزشی باعث می‌شود این مدل‌ها در انجام وظایف مختلف از جمله درک نحو، معنا و زمینه زبان، حتی وظایفی که به دانش عمومی نیاز دارند، عملکرد خوبی داشته باشند.

در جدول ۱.۱ جزئیات داده‌های مورد استفاده برای GPT-3 ارائه شده است. ستون «نسبت‌ها» مجموع داده‌های نمونه‌برداری شده را به ۱۰۰٪ می‌رساند (با در نظر گرفتن خطاهای گرد کردن). اگرچه مجموع توکن‌های ستون «تعداد توکن‌ها» برابر با ۴۹۹ میلیارد است، مدل فقط با ۳۰۰ میلیارد توکن آموزش داده شده است. دلایل انتخاب این مقدار توسط نویسندگان مقاله GPT-3 بیان نشده است.

برای مقایسه، حجم داده‌های مجموعه CommonCrawl به تنهایی ۴۱۰ میلیارد توکن است و ذخیره آن حدود ۵۷۰ گیگابایت نیاز دارد. مدل‌های بعدی مانند LLaMA از Meta دامنه آموزشی خود را گسترش داده‌اند و داده‌های بیشتری مانند مقالات علمی Arxiv (۹۲ گیگابایت) و پرسش و پاسخ‌های مرتبط با کدنویسی در StackExchange (۷۸ گیگابایت) را به آن اضافه کرده‌اند.

مجموعه داده آموزشی GPT-3 منتشر نشده است، اما مجموعه‌ای مشابه به نام Dolma (سه تریلیون توکن) به صورت عمومی در دسترس است (Soldaini و همکاران، ۲۰۲۴). با این حال، این مجموعه ممکن است شامل آثار دارای حق کپی‌رایت باشد و شرایط استفاده از آن بسته به هدف و کشور متفاوت است.

ویژگی پیش‌آموزش این مدل‌ها باعث شده آن‌ها برای تنظیم‌های بعدی (fine-tuning) در وظایف خاص بسیار قابل استفاده باشند، به همین دلیل آن‌ها را مدل‌های پایه یا بنیادین می‌نامند. پیش‌آموزش این مدل‌ها نیازمند منابع عظیم و هزینه‌های بالایی است؛ برای مثال، هزینه پیش‌آموزش GPT-3 حدود ۴.۶ میلیون دلار برآورد شده است.

خبر خوب این است که بسیاری از مدل‌های پیش‌آموزش یافته به صورت متن‌باز در دسترس هستند و می‌توان از آن‌ها به عنوان ابزارهای عمومی برای نوشتن، استخراج و ویرایش متن‌هایی استفاده کرد که در داده‌های آموزشی نبوده‌اند. همچنین می‌توان این مدل‌ها را با داده‌های کوچک‌تر برای وظایف خاص تنظیم کرد که باعث کاهش هزینه محاسباتی و افزایش دقت می‌شود.

در ادامه، ما کد پیش‌آموزش را پیاده‌سازی خواهیم کرد و برای اهداف آموزشی از آن استفاده خواهیم کرد. تمام محاسبات قابل اجرا روی سخت‌افزار مصرفی هستند. پس از پیاده‌سازی کد، یاد می‌گیریم چگونه وزن‌های مدل‌های موجود را بارگذاری کنیم تا از مرحله پرهزینه پیش‌آموزش صرف‌نظر کنیم و مدل را برای وظایف خاص تنظیم کنیم.

---

## ۱.۶ نگاهی دقیق‌تر به معماری GPT

مدل GPT نخستین بار در مقاله‌ای با عنوان «بهبود درک زبان با پیش‌آموزش تولیدی» توسط Radford و همکاران از OpenAI معرفی شد. GPT-3 نسخه گسترش یافته این مدل است که پارامترهای بیشتری دارد و روی داده‌های بزرگ‌تری آموزش دیده است. نسخه اولیه ارائه شده در ChatGPT با تنظیم GPT-3 روی داده‌های دستوری بزرگ ساخته شده است، روشی که در مقاله InstructGPT توضیح داده شده است.

همان‌طور که در شکل ۱.۶ نشان داده شده، این مدل‌ها توانایی تکمیل متن دارند و می‌توانند وظایف دیگری مانند تصحیح املایی، طبقه‌بندی یا ترجمه زبان را نیز انجام دهند. این امر بسیار جالب است چون GPT بر اساس وظیفه نسبتاً ساده پیش‌بینی کلمه بعدی آموزش می‌بیند (شکل ۱.۷).

وظیفه پیش‌بینی کلمه بعدی نوعی یادگیری خودنظارتی است که بر پایه استفاده از ساختار داده برای تولید برچسب است، به این معنی که نیازی به برچسب‌گذاری دستی نیست. مدل با استفاده از کلمه بعدی به عنوان برچسب یاد می‌گیرد و این امکان را می‌دهد که روی داده‌های بزرگ بدون برچسب آموزش ببیند.

معماری GPT نسبت به معماری ترنسفورمر اصلی ساده‌تر است و تنها شامل بخش دیکودر است (شکل ۱.۸). مدل‌های دیکودر مانند GPT به صورت خودرگرسیو عمل می‌کنند؛ یعنی کلمه بعدی را با توجه به کلمات قبلی پیش‌بینی می‌کنند که باعث افزایش پیوستگی متن تولیدی می‌شود.

مدل‌هایی مانند GPT-3 بسیار بزرگ‌تر از مدل ترنسفورمر اولیه هستند؛ برای مثال، GPT-3 شامل ۹۶ لایه ترنسفورمر و ۱۷۵ میلیارد پارامتر است، در حالی که مدل ترنسفورمر اولیه ۶ بار بلاک‌های انکودر و دیکودر را تکرار می‌کرد.

مدل GPT-3 در سال ۲۰۲۰ معرفی شد که به لحاظ سرعت پیشرفت‌های حوزه یادگیری عمیق، زمان نسبتاً دوری است. با این حال، معماری‌های جدید مانند LLaMA همچنان بر مبنای همین اصول ساخته شده‌اند و تغییرات جزئی دارند. بنابراین، درک معماری GPT همچنان بسیار مهم است و ما تمرکز خود را روی پیاده‌سازی معماری اصلی GPT می‌گذاریم و به نکات مربوط به مدل‌های دیگر نیز اشاره خواهیم کرد.

اگرچه مدل ترنسفورمر اصلی برای ترجمه زبان طراحی شده بود، مدل‌های GPT با وجود معماری دیکودر ساده‌تر و هدف پیش‌بینی کلمه بعدی، توانایی انجام ترجمه را نیز دارند. این ویژگی که به آن رفتار نوظهور می‌گویند، به صورت مستقیم آموزش داده نشده اما به خاطر مواجهه مدل با داده‌های چندزبانه گسترده و متنوع به وجود آمده است.

این توانایی مدل برای انجام وظایفی که به طور مشخص آموزش داده نشده‌اند، نشان‌دهنده قدرت و قابلیت‌های مدل‌های زبانی بزرگ است. به این ترتیب، می‌توان با یک مدل واحد، وظایف متنوعی را انجام داد بدون اینکه نیاز به مدل‌های متعدد برای هر کار باشد.

---

## ۱.۷ ساخت یک مدل زبان بزرگ

اکنون که مبانی مربوط به مدل‌های زبانی بزرگ (LLM) را فراگرفتیم، قصد داریم یکی از این مدل‌ها را از ابتدا برنامه‌نویسی کنیم. ما ایده اصلی پشت معماری GPT را به عنوان چارچوب کاری در نظر می‌گیریم و این پروژه را در سه مرحله پیش می‌بریم که در شکل ۱.۹ به آن اشاره شده است.

شکل ۱.۹ سه مرحله اصلی کدنویسی یک مدل زبانی بزرگ را نشان می‌دهد:
۱. پیاده‌سازی معماری مدل و آماده‌سازی داده‌ها،
۲. پیش‌آموزش مدل برای ساخت مدل پایه،
۳. تنظیم دقیق (fine-tuning) مدل پایه برای کاربردهای خاص مانند دستیار شخصی یا دسته‌بندی متن.

در مرحله اول، با مراحل اساسی پردازش داده‌ها آشنا می‌شویم و مکانیزم توجه (attention) که در قلب هر مدل زبانی بزرگ قرار دارد را برنامه‌نویسی می‌کنیم. سپس در مرحله دوم، یاد می‌گیریم چگونه یک مدل شبیه به GPT را کدنویسی و پیش‌آموزش دهیم تا قادر به تولید متن‌های جدید باشد. همچنین مفاهیم پایه‌ای ارزیابی مدل‌های زبانی بزرگ را مرور می‌کنیم که برای توسعه سیستم‌های پردازش زبان طبیعی (NLP) ضروری است.

پیش‌آموزش یک مدل بزرگ از ابتدا کاری بزرگ و پرهزینه است که می‌تواند به هزینه‌های هزاران تا میلیون‌ها دلار در زمینه محاسبات ابری منجر شود، به خصوص برای مدل‌هایی مانند GPT. بنابراین تمرکز مرحله دوم بیشتر بر پیاده‌سازی آموزشی با استفاده از مجموعه داده‌های کوچک است. همچنین کدهایی برای بارگذاری وزن‌های مدل‌های پیش‌آموزش یافته به صورت متن‌باز ارائه شده است.

در نهایت، در مرحله سوم، یک مدل پیش‌آموزش یافته را می‌گیریم و آن را برای انجام وظایفی مانند پاسخ به پرسش‌ها یا دسته‌بندی متن‌ها به صورت دقیق تنظیم می‌کنیم که از رایج‌ترین کاربردها در حوزه‌های عملی و پژوهشی هستند.

---

### خلاصه

- مدل‌های زبانی بزرگ تحولی بنیادین در حوزه پردازش زبان طبیعی ایجاد کرده‌اند که پیش‌تر عمدتاً بر اساس سیستم‌های قانون‌محور و روش‌های آماری ساده بود. این مدل‌ها با استفاده از یادگیری عمیق به پیشرفت‌های چشمگیری در درک، تولید و ترجمه زبان انسان دست یافته‌اند.
- آموزش مدل‌های امروزی در دو مرحله اصلی انجام می‌شود:
  ۱. پیش‌آموزش روی مجموعه بزرگی از متن‌های بدون برچسب، با استفاده از وظیفه پیش‌بینی کلمه بعدی در جمله،
  ۲. سپس تنظیم دقیق روی مجموعه داده‌های کوچکتر و برچسب‌دار برای انجام وظایفی مثل دنبال کردن دستورات یا دسته‌بندی.
- مدل‌های زبانی بزرگ مبتنی بر معماری ترنسفورمر هستند. مکانیزم کلیدی این معماری، مکانیزم توجه است که به مدل اجازه می‌دهد هنگام تولید هر کلمه، به کل دنباله ورودی دسترسی انتخابی داشته باشد.
- معماری اصلی ترنسفورمر از دو بخش انکودر (برای تجزیه متن) و دیکودر (برای تولید متن) تشکیل شده است.
- مدل‌های تولید متن و دنبال‌کننده دستورات مثل GPT-3 و ChatGPT تنها بخش دیکودر را پیاده‌سازی می‌کنند که معماری را ساده‌تر می‌کند.
- برای پیش‌آموزش مدل‌های بزرگ، مجموعه داده‌های عظیمی شامل میلیاردها کلمه ضروری است.
- وظیفه کلی پیش‌آموزش برای مدل‌های شبیه GPT، پیش‌بینی کلمه بعدی است، اما این مدل‌ها رفتارهای نوظهوری مانند توانایی دسته‌بندی، ترجمه یا خلاصه‌سازی متن را نیز از خود نشان می‌دهند.
- پس از پیش‌آموزش، مدل پایه حاصل را می‌توان به صورت بهینه‌تر برای وظایف خاص تنظیم کرد.
- مدل‌های زبانی بزرگ تنظیم شده روی داده‌های خاص، معمولاً در آن وظایف خاص بهتر از مدل‌های عمومی عمل می‌کنند.

---

> [ درباره کتاب
     (قبلی) ](README.md) 
     < 1.درک مدل‌های زبانی بزرگ > 
[
    2.کار با داده‌های متنی
(بعدی)
](<02.Working with text data.md>)
