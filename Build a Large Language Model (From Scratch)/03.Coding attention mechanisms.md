<!-- language: rtl -->

# ۳. کدنویسی مکانیزم‌های توجه (Attention Mechanisms)

در این مرحله، شما با نحوه آماده‌سازی متن ورودی برای آموزش مدل‌های زبانی بزرگ (LLM) آشنا شده‌اید؛ به این صورت که متن به توکن‌های کلمه‌ای و زیرکلمه‌ای تقسیم شده و سپس به نمایش‌های برداری (embedding) تبدیل می‌شود.  
اکنون به بخش مهمی از معماری خود LLM، یعنی مکانیزم‌های توجه، می‌پردازیم که در شکل ۳.۱ نشان داده شده است. در این فصل عمدتاً مکانیزم‌های توجه را به صورت جداگانه و در سطح مکانیکی بررسی می‌کنیم. سپس قسمت‌های دیگر مدل اطراف مکانیزم خودتوجه (self-attention) را کدنویسی می‌کنیم تا نحوه عملکرد آن را مشاهده کرده و مدلی برای تولید متن ایجاد کنیم.

> **شکل ۳.۱**  
> سه مرحله اصلی کدنویسی یک LLM. این فصل بر گام دوم از مرحله اول تمرکز دارد: پیاده‌سازی مکانیزم‌های توجه که بخشی اساسی از معماری LLM هستند.

چهار نوع مختلف از مکانیزم‌های توجه را پیاده‌سازی خواهیم کرد که در شکل ۳.۲ نشان داده شده‌اند. این انواع مختلف مکانیزم توجه بر یکدیگر ساخته می‌شوند و هدف رسیدن به یک پیاده‌سازی فشرده و کارآمد از توجه چندسَر (multi-head attention) است که در فصل بعد آن را در معماری LLM به کار خواهیم گرفت.

> **شکل ۳.۲**  
> مکانیزم‌های توجه مختلفی که در این فصل کدنویسی می‌کنیم. ابتدا نسخه ساده‌شده‌ای از خودتوجه (self-attention) را می‌بینیم و سپس وزن‌های قابل آموزش را اضافه می‌کنیم. مکانیزم توجه علّی (causal attention) با افزودن ماسک به خودتوجه اجازه می‌دهد مدل کلمه به کلمه متن تولید کند. در نهایت، توجه چندسر، مکانیزم توجه را به چند سر تقسیم می‌کند تا مدل بتواند جنبه‌های مختلف داده ورودی را به صورت هم‌زمان تحلیل کند.

---

## ۳.۱ مشکل مدل‌سازی دنباله‌های طولانی

قبل از ورود به مکانیزم خودتوجه که هسته اصلی LLMهاست، مسئله معماری‌های پیش از LLM را که مکانیزم توجه ندارند بررسی کنیم.  
فرض کنید می‌خواهیم مدلی برای ترجمه زبان طراحی کنیم که متنی را از یک زبان به زبان دیگر ترجمه کند. همانطور که در شکل ۳.۳ دیده می‌شود، نمی‌توان متن را صرفاً کلمه به کلمه ترجمه کرد، زیرا ساختارهای دستوری زبان مبدأ و مقصد متفاوت هستند.

برای حل این مشکل معمولاً از شبکه‌های عصبی عمیق با دو زیرماژول استفاده می‌شود: کدگذار (encoder) و رمزگشا (decoder). وظیفه کدگذار خواندن و پردازش کل متن است و رمزگشا متن ترجمه‌شده را تولید می‌کند.

قبل از ظهور مدل‌های ترنسفورمر، شبکه‌های عصبی بازگشتی (RNN) محبوب‌ترین معماری کدگذار-رمزگشا برای ترجمه بودند. RNN نوعی شبکه عصبی است که خروجی‌های مراحل قبلی به عنوان ورودی مرحله فعلی وارد می‌شوند، که این ساختار آن را برای داده‌های دنباله‌ای مثل متن مناسب می‌کند.  
اگر با RNN آشنا نیستید نگران نباشید؛ نیازی به دانستن جزئیات داخلی RNN نیست و تمرکز ما بر مفهوم کلی کدگذار-رمزگشا است.

در یک RNN کدگذار-رمزگشا، متن ورودی به کدگذار داده می‌شود که آن را مرحله به مرحله پردازش می‌کند. کدگذار در هر مرحله وضعیت مخفی (hidden state) خود را به‌روزرسانی می‌کند تا معنای کلی جمله را در وضعیت مخفی نهایی جمع‌بندی کند (شکل ۳.۴). سپس رمزگشا با استفاده از این وضعیت مخفی نهایی شروع به تولید ترجمه می‌کند، کلمه به کلمه، و در هر مرحله وضعیت مخفی خود را به‌روزرسانی می‌کند تا زمینه لازم برای پیش‌بینی کلمه بعدی را حفظ کند.

> **شکل ۳.۳**  
> هنگام ترجمه متن از زبانی مانند آلمانی به انگلیسی، ترجمه صرفاً کلمه به کلمه ممکن نیست و نیاز به درک زمینه‌ای و تطابق دستوری وجود دارد.

> **شکل ۳.۴**  
> پیش از ظهور مدل‌های ترنسفورمر، معماری‌های RNN کدگذار-رمزگشا برای ترجمه ماشین بسیار رایج بودند. کدگذار توالی‌ای از توکن‌های زبان مبدأ را می‌گیرد و وضعیت مخفی‌ای ایجاد می‌کند که نمایانگر فشرده کل ورودی است. سپس رمزگشا با وضعیت مخفی فعلی خود شروع به تولید توکن به توکن ترجمه می‌کند.

اگرچه نیازی به درک کامل RNNها نیست، نکته اصلی این است که کدگذار کل متن ورودی را به وضعیت مخفی (یا بردار جاسازی) فشرده تبدیل می‌کند و رمزگشا از این وضعیت برای تولید خروجی استفاده می‌کند.  
محدودیت بزرگ RNNهای کدگذار-رمزگشا این است که در فاز رمزگشایی نمی‌توانند مستقیماً به وضعیت‌های مخفی قبلی کدگذار دسترسی داشته باشند و فقط به وضعیت مخفی فعلی که تمام اطلاعات لازم را جمع کرده است تکیه می‌کنند. این موضوع باعث می‌شود در جملات پیچیده که وابستگی‌های بلندمدت دارند، احتمال از دست رفتن زمینه افزایش یابد.

خوشبختانه برای ساخت یک LLM نیازی به فهم عمیق RNNها نیست. فقط کافی است بدانید که محدودیت‌های RNNها در معماری کدگذار-رمزگشا، انگیزه‌ای برای طراحی مکانیزم‌های توجه بوده است.

## ۳.۲ درک وابستگی‌های داده با مکانیزم‌های توجه

اگرچه RNNها برای ترجمه جملات کوتاه عملکرد قابل قبولی دارند، اما در متن‌های طولانی به خوبی عمل نمی‌کنند، زیرا دسترسی مستقیم به کلمات قبلی در ورودی ندارند. یکی از مشکلات اصلی این روش این است که RNN باید کل ورودی رمزگذاری‌شده را در یک وضعیت مخفی واحد به خاطر بسپارد و سپس آن را به رمزگشا منتقل کند (شکل ۳.۴).

به همین دلیل، پژوهشگران در سال ۲۰۱۴ مکانیزم توجه به نام Bahdanau را برای RNNها معرفی کردند (نام‌گذاری شده به افتخار نویسنده اول مقاله مربوطه؛ برای اطلاعات بیشتر به پیوست B مراجعه کنید). این مکانیزم، معماری کدگذار-رمزگشا را به گونه‌ای تغییر می‌دهد که رمزگشا در هر مرحله رمزگشایی بتواند به صورت انتخابی به بخش‌های مختلف توالی ورودی دسترسی داشته باشد (شکل ۳.۵).

> **شکل ۳.۵**  
> با استفاده از مکانیزم توجه، بخش رمزگشای تولیدکننده متن در شبکه می‌تواند به صورت انتخابی به تمامی توکن‌های ورودی دسترسی پیدا کند. این یعنی برخی توکن‌های ورودی اهمیت بیشتری نسبت به سایرین در تولید یک توکن خروجی خاص دارند. این اهمیت توسط وزن‌های توجه تعیین می‌شود که در ادامه محاسبه خواهند شد. توجه داشته باشید این شکل ایده کلی توجه را نشان می‌دهد و پیاده‌سازی دقیق مکانیزم Bahdanau که یک روش RNN است، در حیطه این کتاب نیست.

جالب این‌که تنها سه سال پس از معرفی مکانیزم توجه Bahdanau، پژوهشگران دریافتند که برای ساخت شبکه‌های عصبی عمیق در پردازش زبان طبیعی نیازی به معماری‌های RNN نیست و معماری ترنسفورمر اصلی (که در فصل ۱ به آن پرداخته شد) را معرفی کردند که شامل مکانیزم خودتوجه (self-attention) است که از مکانیزم توجه Bahdanau الهام گرفته شده است.

خودتوجه مکانیزمی است که به هر موقعیت در توالی ورودی اجازه می‌دهد هنگام محاسبه نمایش آن توالی، به سایر موقعیت‌ها توجه کرده و ارتباط آن‌ها را در نظر بگیرد. خودتوجه جزو کلیدی مدل‌های زبانی بزرگ امروزی مبتنی بر معماری ترنسفورمر مانند سری GPT است.

این فصل بر کدنویسی و درک مکانیزم خودتوجه در مدل‌های مشابه GPT تمرکز دارد (شکل ۳.۶). در فصل بعدی، بخش‌های باقی‌مانده LLM را کدنویسی خواهیم کرد.

## ۳.۳ توجه به بخش‌های مختلف ورودی با مکانیزم خودتوجهی (Self-Attention)

در این بخش، عملکرد داخلی مکانیزم خودتوجهی را بررسی کرده و یاد می‌گیریم چگونه آن را از پایه کدنویسی کنیم. خودتوجهی به‌عنوان سنگ‌بنای هر مدل زبان بزرگ (LLM) مبتنی بر معماری ترنسفورمر شناخته می‌شود. ممکن است این مبحث در ابتدا پیچیده به نظر برسد، اما پس از درک اصول آن، یکی از دشوارترین بخش‌های این کتاب و اجرای LLM را پشت سر گذاشته‌اید.

---

### خودتوجهی چیست؟

مکانیزم خودتوجهی به این معناست که هر موقعیت در یک دنباله ورودی، می‌تواند با تمامی موقعیت‌های دیگر در همان دنباله ارتباط برقرار کرده و اهمیت هر کدام را بسنجد. به عبارت دیگر، "خود" در خودتوجهی به این اشاره دارد که توجه، تنها به عناصر داخل یک دنباله واحد محدود است، برخلاف توجه سنتی که معمولاً میان دو دنباله مختلف برقرار می‌شود (مانند مدل‌های sequence-to-sequence).

---

### نمونه ساده‌ای از خودتوجهی بدون وزن‌های قابل یادگیری

برای شروع، یک نسخه ساده و بدون وزن‌های قابل آموزش از خودتوجهی را پیاده‌سازی می‌کنیم تا مفاهیم کلیدی آن را درک کنیم. در این مدل ساده، هدف محاسبه یک بردار زمینه (context vector) برای هر عنصر ورودی است که شامل ترکیبی از اطلاعات سایر عناصر دنباله باشد.

مثلاً فرض کنید دنباله ورودی متنی "Your journey starts with one step." به بردارهای جاسازی‌شده (embedding) سه‌بعدی تبدیل شده است:

```python
import torch
inputs = torch.tensor([
    [0.43, 0.15, 0.89],  # Your (x^1)
    [0.55, 0.87, 0.66],  # journey (x^2)
    [0.57, 0.85, 0.64],  # starts (x^3)
    [0.22, 0.58, 0.33],  # with (x^4)
    [0.77, 0.25, 0.10],  # one (x^5)
    [0.05, 0.80, 0.55]   # step (x^6)
])
```

گام اول محاسبه امتیازهای توجه (attention scores) است. این امتیازها از طریق ضرب داخلی (dot product) بردار پرسش (query) با هر یک از بردارهای ورودی محاسبه می‌شوند. مثلاً برای عنصر دوم (journey) به صورت زیر:

```python
query = inputs[1]
attn_scores_2 = torch.empty(inputs.shape[0])
for i, x_i in enumerate(inputs):
    attn_scores_2[i] = torch.dot(x_i, query)
print(attn_scores_2)
```

خروجی:

```
tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])
```

---

### درک ضرب داخلی (Dot Product)

ضرب داخلی معیاری برای سنجش شباهت بین دو بردار است که حاصل جمع ضرب مؤلفه‌های متناظر آن‌هاست. مقدار بزرگ‌تر به معنی شباهت بیشتر بردارها است. در خودتوجهی، این ضرب میزان توجه یک عنصر به دیگری را مشخص می‌کند.

---

### نرمال‌سازی امتیازهای توجه

برای تبدیل امتیازهای توجه به وزن‌هایی که مجموع آن‌ها برابر با ۱ باشد، باید آن‌ها را نرمال کنیم. ساده‌ترین روش تقسیم هر امتیاز بر مجموع کل امتیازهاست:

```python
attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()
print("Attention weights:", attn_weights_2_tmp)
print("Sum:", attn_weights_2_tmp.sum())
```

خروجی:

```
Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])
Sum: tensor(1.0000)
```

اما معمولاً از تابع softmax برای نرمال‌سازی استفاده می‌شود که مزایای بیشتری در مدیریت مقادیر بزرگ و گرادیان‌ها دارد:

```python
def softmax_naive(x):
    return torch.exp(x) / torch.exp(x).sum(dim=0)

attn_weights_2_naive = softmax_naive(attn_scores_2)
print("Attention weights:", attn_weights_2_naive)
print("Sum:", attn_weights_2_naive.sum())
```

خروجی:

```
Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])
Sum: tensor(1.)
```

نسخه بهینه و پایدار softmax در PyTorch نیز به همین شکل است:

```python
attn_weights_2 = torch.softmax(attn_scores_2, dim=0)
print("Attention weights:", attn_weights_2)
print("Sum:", attn_weights_2.sum())
```

---

### محاسبه بردار زمینه (Context Vector)

بردار زمینه برای عنصر دوم، حاصل مجموع وزن‌دار بردارهای ورودی است:

```python
query = inputs[1]
context_vec_2 = torch.zeros(query.shape)
for i, x_i in enumerate(inputs):
    context_vec_2 += attn_weights_2[i] * x_i
print(context_vec_2)
```

خروجی:

```
tensor([0.4419, 0.6515, 0.5683])
```

---

### تعمیم به تمام عناصر ورودی

برای محاسبه وزن‌ها و بردارهای زمینه برای همه عناصر ورودی، از یک حلقه دوگانه یا ضرب ماتریسی استفاده می‌کنیم:

```python
attn_scores = torch.empty(6, 6)
for i, x_i in enumerate(inputs):
    for j, x_j in enumerate(inputs):
        attn_scores[i, j] = torch.dot(x_i, x_j)
print(attn_scores)
```

یا به صورت ماتریسی:

```python
attn_scores = inputs @ inputs.T
print(attn_scores)
```

---

### نرمال‌سازی سطر به سطر با softmax

```python
attn_weights = torch.softmax(attn_scores, dim=-1)
print(attn_weights)
```

چک کردن مجموع هر سطر:

```python
print(attn_weights.sum(dim=-1))
```

خروجی:

```
tensor([1., 1., 1., 1., 1., 1.])
```

---

### محاسبه تمام بردارهای زمینه

```python
all_context_vecs = attn_weights @ inputs
print(all_context_vecs)
```

خروجی:

```
tensor([[0.4421, 0.5931, 0.5790],
        [0.4419, 0.6515, 0.5683],
        [0.4431, 0.6496, 0.5671],
        [0.4304, 0.6298, 0.5510],
        [0.4671, 0.5910, 0.5266],
        [0.4177, 0.6503, 0.5645]])
```

# 3.4 پیاده‌سازی مکانیزم Self-Attention با وزن‌های قابل آموزش

گام بعدی ما پیاده‌سازی مکانیزم Self-Attention است که در معماری اصلی ترنسفورمر، مدل‌های GPT و اکثر مدل‌های زبانی بزرگ (LLM) استفاده می‌شود. این مکانیزم همچنین به عنوان **توجه نقطه‌ای مقیاس‌گذاری‌شده (scaled dot-product attention)** شناخته می‌شود. شکل ۳.۱۳ نشان می‌دهد چگونه این مکانیزم در چارچوب کلی پیاده‌سازی یک مدل زبانی بزرگ قرار می‌گیرد.

---

توضیح شکل ۳.۱۳: 
قبلاً یک مکانیزم توجه ساده شده را کد کرده بودیم تا مکانیسم پایه را درک کنیم. اکنون وزن‌های قابل آموزش را به این مکانیزم اضافه می‌کنیم. در ادامه نیز این مکانیزم را با افزودن ماسک علی (causal mask) و چند سر (multi-head) گسترش خواهیم داد.

---

همانطور که در شکل ۳.۱۳ مشاهده می‌شود، مکانیزم Self-Attention با وزن‌های قابل آموزش بر اساس مفاهیم قبلی است: هدف ما محاسبه بردارهای زمینه (context vectors) به صورت مجموع وزنی بر روی بردارهای ورودی مربوط به یک عنصر خاص است. تفاوت اصلی با مکانیزم ساده قبلی، اضافه شدن ماتریس‌های وزن است که در طول آموزش مدل به‌روزرسانی می‌شوند. این ماتریس‌ها نقش حیاتی دارند تا مدل بتواند بردارهای زمینه مناسبی بسازد.

---

### 3.4.1 محاسبه وزن‌های توجه به صورت گام به گام

سه ماتریس وزن قابل آموزش داریم:

- \( W_q \) برای کوئری‌ها (query)
- \( W_k \) برای کی‌ها (key)
- \( W_v \) برای مقدارها (value)

این ماتریس‌ها بردارهای ورودی جاسازی‌شده \( x^{(i)} \) را به بردارهای کوئری، کی و مقدار نگاشت می‌دهند (شکل ۳.۱۴).

### مثال عملی:

ابتدا متغیرهای اولیه را تعریف می‌کنیم:

```python
x_2 = inputs[1]
d_in = inputs.shape[1]
d_out = 2
```

توجه داشته باشید که معمولاً در مدل‌های GPT، ابعاد ورودی و خروجی برابرند، اما برای فهم بهتر، اینجا ابعاد متفاوت در نظر گرفته شده‌اند.

سپس ماتریس‌های وزن را مقداردهی اولیه می‌کنیم:

```python
torch.manual_seed(123)
W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
```

برای سادگی نمایش، گرادیان‌ها غیرفعال شده‌اند (در آموزش واقعی باید فعال باشند).

بردارهای کوئری، کی و مقدار را محاسبه می‌کنیم:

```python
query_2 = x_2 @ W_query
key_2 = x_2 @ W_key
value_2 = x_2 @ W_value
print(query_2)
```

خروجی کوئری یک بردار دو بعدی است:

```
tensor([0.4306, 1.4551])
```

---

### تفاوت وزن‌های پارامتری و وزن‌های توجه

- **وزن‌های پارامتری**: مقادیری هستند که در شبکه عصبی آموزش داده می‌شوند و ساختار مدل را تعریف می‌کنند (ماتریس‌های ( W_q, W_k, W_v )).
- **وزن‌های توجه**: مقادیر پویا و وابسته به متن هستند که نشان می‌دهند هر بخش از ورودی چقدر باید مورد توجه قرار گیرد.

---

### محاسبه کی‌ها و مقدارها برای تمام ورودی‌ها:

```python
keys = inputs @ W_key
values = inputs @ W_value
print("keys.shape:", keys.shape)
print("values.shape:", values.shape)
```

خروجی نشان می‌دهد که شش توکن ورودی از فضای سه بعدی به فضای دو بعدی نگاشت شده‌اند:

```
keys.shape: torch.Size([6, 2])
values.shape: torch.Size([6, 2])
```

---

### محاسبه امتیازهای توجه (attention scores)

محاسبه امتیاز (\omega\_{22}) به صورت ضرب داخلی کوئری و کی متناظر:

```python
keys_2 = keys[1]
attn_score_22 = query_2.dot(keys_2)
print(attn_score_22)
```

خروجی:

```
tensor(1.8524)
```

عمومی کردن محاسبه برای همه امتیازها:

```python
attn_scores_2 = query_2 @ keys.T
print(attn_scores_2)
```

خروجی:

```
tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])
```

---

### محاسبه وزن‌های توجه با نرمال‌سازی و تابع Softmax

برای جلوگیری از مشکلات گرادیان کوچک در آموزش، امتیازها را با جذر بعد کی‌ها نرمال می‌کنیم:

```python
d_k = keys.shape[-1]
attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)
print(attn_weights_2)
```

خروجی وزن‌های توجه:

```
tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])
```

---

### محاسبه بردار زمینه (context vector)

بردار زمینه را به عنوان مجموع وزنی مقدارها محاسبه می‌کنیم:

```python
context_vec_2 = attn_weights_2 @ values
print(context_vec_2)
```

خروجی:

```
tensor([0.3061, 0.8210])
```

---

### چرا کوئری، کی و مقدار؟

این اصطلاحات از حوزه بازیابی اطلاعات و پایگاه داده‌ها گرفته شده‌اند:

- **Query (کوئری)**: پرس‌وجویی که مدل روی آن تمرکز دارد و می‌خواهد بفهمد.
- **Key (کی)**: کلیدی برای ایندکس و جستجوی اطلاعات مرتبط با کوئری.
- **Value (مقدار)**: محتوای واقعی که بر اساس مطابقت کوئری و کی بازیابی می‌شود.

---

### 3.4.2 پیاده‌سازی کلاس Self-Attention به صورت فشرده در پایتون

برای کاربرد در مدل‌های بزرگ، بهتر است کد را به صورت یک کلاس مرتب کنیم:

```python
import torch.nn as nn
class SelfAttention_v1(nn.Module):
    def __init__(self, d_in, d_out):
        super().__init__()
        self.W_query = nn.Parameter(torch.rand(d_in, d_out))
        self.W_key = nn.Parameter(torch.rand(d_in, d_out))
        self.W_value = nn.Parameter(torch.rand(d_in, d_out))

    def forward(self, x):
        keys = x @ self.W_key
        queries = x @ self.W_query
        values = x @ self.W_value
        attn_scores = queries @ keys.T
        attn_weights = torch.softmax(
            attn_scores / keys.shape[-1]**0.5, dim=-1
        )
        context_vec = attn_weights @ values
        return context_vec
```

استفاده از این کلاس:

```python
torch.manual_seed(123)
sa_v1 = SelfAttention_v1(d_in, d_out)
print(sa_v1(inputs))
```

خروجی:

```
tensor([[0.2996, 0.8053],
        [0.3061, 0.8210],
        [0.3058, 0.8203],
        [0.2948, 0.7939],
        [0.2927, 0.7891],
        [0.2990, 0.8040]], grad_fn=<MmBackward0>)
```

---

### بهبود با استفاده از nn.Linear

با استفاده از لایه‌های `nn.Linear` می‌توان وزن‌ها را به صورت بهینه‌تر مقداردهی اولیه و مدیریت کرد:

```python
class SelfAttention_v2(nn.Module):
    def __init__(self, d_in, d_out, qkv_bias=False):
        super().__init__()
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)

    def forward(self, x):
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)
        attn_scores = queries @ keys.T
        attn_weights = torch.softmax(
            attn_scores / keys.shape[-1]**0.5, dim=-1
        )
        context_vec = attn_weights @ values
        return context_vec
```

مثال استفاده:

```python
torch.manual_seed(789)
sa_v2 = SelfAttention_v2(d_in, d_out)
print(sa_v2(inputs))
```

خروجی:

```
tensor([[-0.0739, 0.0713],
        [-0.0748, 0.0703],
        [-0.0749, 0.0702],
        [-0.0760, 0.0685],
        [-0.0763, 0.0679],
        [-0.0754, 0.0693]], grad_fn=<MmBackward0>)
```

---

### تمرین ۳.۱

وزن‌های لایه‌های nn.Linear به صورت ترانهاده ذخیره می‌شوند. وظیفه شما این است که وزن‌های SelfAttention_v2 را به SelfAttention_v1 منتقل کنید تا خروجی‌ها مشابه شوند.

---

## 3.5 پنهان‌سازی کلمات آینده با توجه علّی (Causal Attention)

در بسیاری از وظایف مدل‌های زبانی بزرگ (LLM)، لازم است مکانیزم توجه خود-توجه (self-attention) فقط توکن‌هایی را در نظر بگیرد که قبل از موقعیت فعلی در توالی ظاهر شده‌اند تا توکن بعدی را پیش‌بینی کند. توجه علّی، که به آن توجه ماسک‌شده نیز گفته می‌شود، نوعی تخصصی از مکانیزم خود-توجه است. این نوع توجه مدل را محدود می‌کند تا هنگام محاسبه امتیازهای توجه، تنها ورودی‌های قبلی و جاری در توالی را در نظر بگیرد. این در تضاد با مکانیزم استاندارد خود-توجه است که اجازه دسترسی همزمان به کل توالی ورودی را می‌دهد.

اکنون قصد داریم مکانیزم خود-توجه استاندارد را به گونه‌ای تغییر دهیم که مکانیزم توجه علّی ایجاد شود، که در فصول بعدی برای توسعه مدل‌های زبانی بزرگ ضروری است. در مدل‌های شبیه GPT، برای هر توکن، توکن‌های آینده که پس از توکن فعلی در متن ورودی قرار دارند، ماسک می‌شوند (شکل ۳.۱۹). وزن‌های توجه بالاتر از قطر اصلی ماتریس ماسک می‌شوند و وزن‌های توجه غیرماسک‌شده به گونه‌ای نرمالیزه می‌شوند که جمع وزن‌های توجه در هر سطر برابر با ۱ باشد. بعداً این فرآیند ماسک و نرمال‌سازی را در کد پیاده‌سازی خواهیم کرد.

---

**شکل ۳.۱۹**  
در توجه علّی، وزن‌های توجه بالاتر از قطر اصلی ماتریس ماسک می‌شوند تا مدل نتواند هنگام محاسبه بردارهای زمینه (context vectors) به توکن‌های آینده دسترسی داشته باشد. به عنوان مثال، در ردیف دوم برای کلمه "journey"، فقط وزن‌های توجه مربوط به کلمات قبل ("Your") و در موقعیت جاری ("journey") حفظ شده‌اند.

---

### 3.5.1 اعمال ماسک توجه علّی در کد

گام بعدی، پیاده‌سازی ماسک توجه علّی در کد است. برای اعمال ماسک و به دست آوردن وزن‌های توجه ماسک‌شده، همانطور که در شکل ۳.۲۰ خلاصه شده، با امتیازها و وزن‌های توجه بخش قبلی شروع می‌کنیم.

---

**شکل ۳.۲۰**  
یکی از روش‌های به دست آوردن ماتریس وزن توجه ماسک‌شده در توجه علّی، اعمال تابع softmax به امتیازهای توجه، صفر کردن المان‌های بالاتر از قطر اصلی و نرمال‌سازی ماتریس حاصل است.

---

ابتدا وزن‌های توجه را با استفاده از تابع softmax محاسبه می‌کنیم:

```python
queries = sa_v2.W_query(inputs)
keys = sa_v2.W_key(inputs)
attn_scores = queries @ keys.T
attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
print(attn_weights)
```

خروجی وزن‌های توجه:

```python
tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],
        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],
        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],
        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],
        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],
        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
       grad_fn=<SoftmaxBackward0>)
```

---

گام دوم: ایجاد ماسک با استفاده از تابع `torch.tril` برای صفر کردن مقادیر بالاتر از قطر:

```python
context_length = attn_scores.shape[0]
mask_simple = torch.tril(torch.ones(context_length, context_length))
print(mask_simple)
```

ماسک ایجاد شده:

```python
tensor([[1., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1.]])
```

---

حال ماسک را در وزن‌های توجه ضرب می‌کنیم تا مقادیر بالاتر از قطر صفر شوند:

```python
masked_simple = attn_weights * mask_simple
print(masked_simple)
```

خروجی:

```python
tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],
        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],
        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],
        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
       grad_fn=<MulBackward0>)
```

---

گام سوم: نرمال‌سازی مجدد وزن‌ها به گونه‌ای که جمع هر سطر برابر ۱ شود:

```python
row_sums = masked_simple.sum(dim=-1, keepdim=True)
masked_simple_norm = masked_simple / row_sums
print(masked_simple_norm)
```

نتیجه نهایی ماتریس وزن توجه:

```python
tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],
        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],
        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],
        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
       grad_fn=<DivBackward0>)
```

---

### جلوگیری از نشت اطلاعات

ممکن است به نظر برسد که با اعمال ماسک و نرمال‌سازی مجدد وزن‌ها، اطلاعات توکن‌های آینده که قصد داشتیم ماسک شوند، هنوز هم بر توکن جاری تأثیر می‌گذارند زیرا آن‌ها بخشی از محاسبه softmax هستند. اما نکته کلیدی این است که نرمال‌سازی مجدد در واقع softmax را بر زیرمجموعه‌ای کوچک‌تر از موقعیت‌ها (بدون موقعیت‌های ماسک‌شده) مجدداً محاسبه می‌کند.

زیبایی ریاضی softmax در این است که با وجود حضور همه موقعیت‌ها در مخرج کسر اولیه، پس از ماسک کردن و نرمال‌سازی، اثر موقعیت‌های ماسک‌شده عملاً حذف می‌شود و آن‌ها تأثیری بر امتیاز softmax ندارند.

به بیان ساده‌تر، پس از ماسک و نرمال‌سازی، توزیع وزن‌های توجه گویی فقط بین موقعیت‌های غیرماسک‌شده محاسبه شده است. این تضمین می‌کند که هیچ نشت اطلاعاتی از توکن‌های آینده وجود ندارد.

---

### بهبود کارایی با ماسک‌گذاری قبل از softmax

اگرچه می‌توانیم در همینجا مکانیزم توجه علّی را کامل کنیم، اما می‌توان آن را بهینه‌تر نیز کرد. با استفاده از خاصیت ریاضی تابع softmax، می‌توانیم ماسک‌گذاری را قبل از اعمال softmax انجام دهیم تا تعداد مراحل کاهش یابد (شکل ۳.۲۱).

---

**شکل ۳.۲۱**
روش بهینه‌تر برای به دست آوردن ماتریس وزن توجه ماسک‌شده، ماسک کردن امتیازهای توجه با مقادیر منفی بی‌نهایت (-∞) قبل از اعمال تابع softmax است.

تابع softmax ورودی‌ها را به توزیع احتمال تبدیل می‌کند. هنگامی که مقادیر -∞ در یک ردیف حضور دارند، softmax آن‌ها را به عنوان احتمال صفر تفسیر می‌کند (زیرا e^(-∞) به صفر میل می‌کند).

---

کد پیاده‌سازی این ترفند به صورت زیر است:

```python
mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)
masked = attn_scores.masked_fill(mask.bool(), -torch.inf)
print(masked)
```

نتیجه:

```python
tensor([[0.2899, -inf, -inf, -inf, -inf, -inf],
        [0.4656, 0.1723, -inf, -inf, -inf, -inf],
        [0.4594, 0.1703, 0.1731, -inf, -inf, -inf],
        [0.2642, 0.1024, 0.1036, 0.0186, -inf, -inf],
        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786, -inf],
        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],
       grad_fn=<MaskedFillBackward0>)
```

---

در نهایت، کافی است softmax را روی این امتیازهای ماسک‌شده اعمال کنیم:

```python
attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)
print(attn_weights)
```

خروجی:

```python
tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],
        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],
        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],
        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],
       grad_fn=<SoftmaxBackward0>)
```

جمع مقادیر در هر ردیف برابر ۱ است و نرمال‌سازی اضافی نیاز نیست.

---

در ادامه، می‌توانیم از وزن‌های توجه اصلاح‌شده برای محاسبه بردارهای زمینه استفاده کنیم:

```python
context_vec = attn_weights @ values
```

که در بخش ۳.۴ توضیح داده شد. همچنین، در ادامه به اصلاحات دیگری در مکانیزم توجه علّی خواهیم پرداخت که به کاهش بیش‌برازش (overfitting) در هنگام آموزش مدل‌های زبانی بزرگ کمک می‌کند.


### ۳.۵.۲ ماسک‌گذاری وزن‌های توجه اضافی با استفاده از دراپ‌اوت

دراپ‌اوت (Dropout) در یادگیری عمیق تکنیکی است که در آن، به طور تصادفی برخی واحدهای لایه‌های پنهان در طول آموزش نادیده گرفته می‌شوند، یعنی «حذف» می‌گردند. این روش به جلوگیری از بیش‌برازش کمک می‌کند، زیرا باعث می‌شود مدل به مجموعه خاصی از واحدهای پنهان وابسته نشود. لازم است تأکید شود که دراپ‌اوت تنها در زمان آموزش فعال است و در زمان استنتاج (Inference) غیرفعال می‌شود.

در معماری ترنسفورمر، مانند مدل‌های GPT، دراپ‌اوت معمولاً در دو مرحله از مکانیزم توجه اعمال می‌شود: بعد از محاسبه وزن‌های توجه یا پس از اعمال این وزن‌ها به بردارهای مقدار (value vectors). در اینجا، دراپ‌اوت را بعد از محاسبه وزن‌های توجه اعمال می‌کنیم که رایج‌تر است (شکل ۳.۲۲).

در مثال کد زیر، نرخ دراپ‌اوت را ۵۰٪ در نظر گرفته‌ایم، یعنی نیمی از وزن‌های توجه به صورت تصادفی صفر می‌شوند. (در آموزش مدل GPT در فصول بعدی، از نرخ‌های کمتر مانند ۰.۱ یا ۰.۲ استفاده خواهیم کرد.) ابتدا دراپ‌اوت PyTorch را روی یک تنسور ۶×۶ که تمام مقادیر آن ۱ است اعمال می‌کنیم تا مثال ساده‌تری داشته باشیم:

```python
torch.manual_seed(123)
dropout = torch.nn.Dropout(0.5)
example = torch.ones(6, 6)
print(dropout(example))
````

**شکل ۳.۲۲**: با استفاده از ماسک توجه علی (causal attention mask) (بالا سمت چپ)، یک ماسک دراپ‌اوت اضافی (بالا سمت راست) اعمال می‌کنیم تا وزن‌های توجه بیشتری صفر شوند و بدین ترتیب از بیش‌برازش در طول آموزش جلوگیری شود.

همانطور که می‌بینیم، تقریباً نیمی از مقادیر صفر شده‌اند:

```python
tensor([[2., 2., 0., 2., 2., 0.],
        [0., 0., 0., 2., 0., 2.],
        [2., 2., 2., 2., 0., 2.],
        [0., 2., 2., 0., 0., 2.],
        [0., 2., 0., 2., 0., 2.],
        [0., 2., 2., 2., 2., 0.]])
```

وقتی دراپ‌اوت با نرخ ۵۰٪ روی ماتریس وزن‌های توجه اعمال می‌شود، نیمی از عناصر ماتریس به صورت تصادفی صفر می‌شوند. برای جبران کاهش عناصر فعال، مقادیر باقی‌مانده در ماتریس در ضریب ۲ (که برابر ۱/۰.۵ است) ضرب می‌شوند. این مقیاس‌بندی اهمیت زیادی دارد تا تعادل کلی وزن‌های توجه حفظ شود و تأثیر متوسط مکانیزم توجه در هر دو فاز آموزش و استنتاج ثابت بماند.

حال دراپ‌اوت را روی خود ماتریس وزن‌های توجه اعمال می‌کنیم:

```python
torch.manual_seed(123)
print(dropout(attn_weights))
```

ماتریس وزن توجه حاصل، دارای عناصر صفر شده بیشتری است و مقادیر ۱ باقی‌مانده به طور مناسب مقیاس‌بندی شده‌اند:

```python
tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],
        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],
       grad_fn=<MulBackward0>)
```

توجه داشته باشید که خروجی دراپ‌اوت ممکن است بسته به سیستم عامل شما متفاوت باشد. برای اطلاعات بیشتر می‌توانید به [اینجا](https://github.com/pytorch/pytorch/issues/121595) مراجعه کنید.

---

### ۳.۵.۳ پیاده‌سازی یک کلاس مختصر برای توجه علی (Causal Attention)

پس از آشنایی با مفهوم توجه علی و ماسک‌گذاری دراپ‌اوت، اکنون یک کلاس پایتون جمع و جور تعریف می‌کنیم که این دو تکنیک را به صورت کارآمد پیاده‌سازی کند.

ابتدا لازم است مطمئن شویم کد قابلیت پشتیبانی از دسته‌های ورودی (batch) بیش از یک نمونه را دارد؛ بدین ترتیب کلاس `CausalAttention` از خروجی‌های دسته‌ای (batch) داده‌هایی که در فصل ۲ پیاده‌سازی کردیم، پشتیبانی خواهد کرد.

برای ساده‌سازی، برای شبیه‌سازی این دسته ورودی، ورودی متن را دو بار تکرار می‌کنیم:

```python
batch = torch.stack((inputs, inputs), dim=0)
print(batch.shape)
```

این منجر به یک تنسور سه‌بعدی می‌شود که شامل دو متن ورودی با شش توکن است و هر توکن دارای بردار جاسازی سه‌بعدی است:

```python
torch.Size([2, 6, 3])
```

کلاس `CausalAttention` زیر مشابه کلاس `SelfAttention` است که پیش‌تر پیاده‌سازی کردیم، با این تفاوت که بخش‌های دراپ‌اوت و ماسک توجه علی به آن اضافه شده‌اند.

```python
class CausalAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length,
                 dropout, qkv_bias=False):
        super().__init__()
        self.d_out = d_out
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.dropout = nn.Dropout(dropout)
        self.register_buffer(
            'mask', torch.triu(torch.ones(context_length, context_length),
                               diagonal=1)
        )

    def forward(self, x):
        b, num_tokens, d_in = x.shape
        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)
        attn_scores = queries @ keys.transpose(1, 2)
        attn_scores.masked_fill_(
            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)
        attn_weights = torch.softmax(
            attn_scores / keys.shape[-1]**0.5, dim=-1
        )
        attn_weights = self.dropout(attn_weights)
        context_vec = attn_weights @ values
        return context_vec
```

اگرچه خطوط جدید اضافه شده باید برای شما آشنا باشند، اما این بار متد `register_buffer` را در `__init__` مشاهده می‌کنیم. استفاده از `register_buffer` در PyTorch برای همه موارد ضروری نیست، اما مزایای مهمی دارد. مثلاً وقتی این کلاس را در مدل زبان بزرگ (LLM) خود به کار می‌بریم، این بافرها به طور خودکار به دستگاه (CPU یا GPU) مناسب منتقل می‌شوند، که در هنگام آموزش مدل اهمیت دارد و از خطاهای مربوط به ناسازگاری دستگاه‌ها جلوگیری می‌کند.

می‌توانیم از کلاس `CausalAttention` به شیوه‌ای مشابه `SelfAttention` قبلی استفاده کنیم:

```python
torch.manual_seed(123)
context_length = batch.shape[1]
ca = CausalAttention(d_in, d_out, context_length, 0.0)
context_vecs = ca(batch)
print("context_vecs.shape:", context_vecs.shape)
```

بردارهای متنی به دست آمده به صورت یک تنسور سه‌بعدی است که هر توکن را با یک جاسازی دوبعدی نمایش می‌دهد:

```python
context_vecs.shape: torch.Size([2, 6, 2])
```

---

**شکل ۳.۲۳** خلاصه‌ای از روند کاری است که تاکنون پیش رفته‌ایم. ما با یک مکانیزم توجه ساده شروع کردیم، وزن‌های قابل آموزش را افزودیم، سپس ماسک توجه علی را اضافه کردیم. در ادامه، این مکانیزم توجه علی را گسترش می‌دهیم و ماژول توجه چندسر (multi-head attention) را پیاده‌سازی می‌کنیم که چندین مکانیزم توجه علی را به طور موازی اجرا می‌کند و در مدل زبان بزرگ خود استفاده خواهیم کرد.

---
## ۳.۶ توسعه توجه تک‌سر به توجه چندسر

گام نهایی ما، توسعه کلاس توجه علی (causal attention) پیاده‌سازی شده پیشین به حالت چندسر (multi-head attention) است.

عبارت «چندسر» به تقسیم مکانیزم توجه به چند «سر» مستقل اشاره دارد که هر کدام به صورت جداگانه کار می‌کنند. در این زمینه، یک ماژول توجه علی تک‌سر، تنها دارای یک مجموعه وزن توجه است که به صورت ترتیبی ورودی را پردازش می‌کند.

ما این گسترش را از توجه علی تک‌سر به چندسر انجام خواهیم داد. ابتدا یک ماژول چندسر توجه را به صورت شهودی با ترکیب چندین ماژول `CausalAttention` می‌سازیم، سپس همین ماژول را به روشی پیچیده‌تر ولی کارآمدتر پیاده‌سازی خواهیم کرد.

---

### ۳.۶.۱ تجمیع چند لایه توجه تک‌سر

در عمل، پیاده‌سازی توجه چندسر شامل ایجاد چند نمونه از مکانیزم توجه خودی (self-attention) (شکل ۳.۱۸)، هر کدام با وزن‌های مستقل، و سپس ترکیب خروجی‌های آن‌ها است. استفاده از چند نمونه مکانیزم توجه می‌تواند محاسباتی سنگین باشد، اما برای شناسایی الگوهای پیچیده که مدل‌های مبتنی بر ترنسفورمر به آن‌ها شناخته شده‌اند، ضروری است.

**شکل ۳.۲۴** ساختار ماژول توجه چندسر را نشان می‌دهد که از چند ماژول توجه تک‌سر تشکیل شده‌اند و مانند شکل ۳.۱۸ روی هم قرار گرفته‌اند.

> **شکل ۳.۲۴**: ماژول توجه چندسر شامل دو ماژول توجه تک‌سر است که روی هم انباشته شده‌اند. به جای استفاده از یک ماتریس وزن `Wv` برای محاسبه ماتریس مقدار (value)، در ماژول چندسر توجه با دو سر، دو ماتریس وزن مقدار `Wv1` و `Wv2` داریم. همین موضوع برای ماتریس‌های وزن دیگر `WQ` و `WK` نیز صدق می‌کند. در نهایت، دو مجموعه بردار متناظر با زمینه `Z1` و `Z2` به دست می‌آید که می‌توان آن‌ها را در یک ماتریس بردار زمینه `Z` ترکیب کرد.

ایده اصلی توجه چندسر این است که مکانیزم توجه چند بار به صورت موازی با تبدیل‌های خطی متفاوت و قابل یادگیری اجرا شود؛ یعنی نتایج ضرب ورودی‌ها (مانند بردارهای پرسش، کلید و مقدار) در ماتریس وزن متفاوت. در کد می‌توان این کار را با پیاده‌سازی یک کلاس ساده `MultiHeadAttentionWrapper` انجام داد که چند نمونه از کلاس `CausalAttention` قبلی را انباشته می‌کند.

---

### مثال کد: کلاس Wrapper برای پیاده‌سازی توجه چندسر

```python
class MultiHeadAttentionWrapper(nn.Module):
    def __init__(self, d_in, d_out, context_length,
                 dropout, num_heads, qkv_bias=False):
        super().__init__()
        self.heads = nn.ModuleList(
            [CausalAttention(
                d_in, d_out, context_length, dropout, qkv_bias
            ) for _ in range(num_heads)]
        )

    def forward(self, x):
        return torch.cat([head(x) for head in self.heads], dim=-1)
````

برای مثال، اگر این کلاس `MultiHeadAttentionWrapper` را با دو سر توجه (`num_heads=2`) و خروجی توجه تک‌سر `d_out=2` استفاده کنیم، بردار زمینه خروجی چهار بعدی (۲ × ۲ = ۴) خواهد بود، همانطور که در شکل ۳.۲۵ نشان داده شده است.

---

**شکل ۳.۲۵**: با استفاده از `MultiHeadAttentionWrapper` تعداد سرهای توجه را مشخص کردیم (`num_heads`). اگر `num_heads=2` باشد، یک تنسور شامل دو مجموعه ماتریس بردار زمینه به دست می‌آید. در هر ماتریس، ردیف‌ها بردارهای زمینه مربوط به توکن‌ها و ستون‌ها ابعاد جاسازی مشخص شده توسط `d_out=2` هستند. این ماتریس‌ها در طول ستون به هم متصل شده‌اند و ابعاد جاسازی نهایی برابر با ۴ خواهد بود.

---

### نمونه استفاده از کلاس MultiHeadAttentionWrapper

```python
torch.manual_seed(123)
context_length = batch.shape[1]  # تعداد توکن‌ها
d_in, d_out = 3, 2

mha = MultiHeadAttentionWrapper(
    d_in, d_out, context_length, 0.0, num_heads=2
)
context_vecs = mha(batch)
print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)
```

خروجی:

```python
tensor([[[-0.4519, 0.2216, 0.4772, 0.1063],
         [-0.5874, 0.0058, 0.5891, 0.3257],
         [-0.6300, -0.0632, 0.6202, 0.3860],
         [-0.5675, -0.0843, 0.5478, 0.3589],
         [-0.5526, -0.0981, 0.5321, 0.3428],
         [-0.5299, -0.1081, 0.5077, 0.3493]],

        [[-0.4519, 0.2216, 0.4772, 0.1063],
         [-0.5874, 0.0058, 0.5891, 0.3257],
         [-0.6300, -0.0632, 0.6202, 0.3860],
         [-0.5675, -0.0843, 0.5478, 0.3589],
         [-0.5526, -0.0981, 0.5321, 0.3428],
         [-0.5299, -0.1081, 0.5077, 0.3493]]], grad_fn=<CatBackward0>)

context_vecs.shape: torch.Size([2, 6, 4])
```

بعد اول این تنسور مربوط به دو متن ورودی است (چون ورودی دو بار تکرار شده‌اند، بردارهای زمینه دقیقا مشابه‌اند). بعد دوم مربوط به ۶ توکن در هر متن است. بعد سوم به ابعاد جاسازی چهار بعدی هر توکن اشاره دارد.

---

### تمرین ۳.۲: برگرداندن بردارهای جاسازی دو بعدی

پارامتر ورودی `num_heads=2` در فراخوانی `MultiHeadAttentionWrapper(...)` را طوری تغییر دهید که بردارهای زمینه خروجی دو بعدی (و نه چهار بعدی) باشند، در حالی که `num_heads=2` حفظ شود. نکته: نیازی به تغییر در پیاده‌سازی کلاس نیست؛ فقط کافیست یکی از پارامترهای ورودی دیگر را تغییر دهید.

---

### بهینه‌سازی پیاده‌سازی چندسر

تا اینجا، ما یک `MultiHeadAttentionWrapper` پیاده‌سازی کردیم که چند ماژول توجه تک‌سر را ترکیب می‌کند. اما این ماژول‌ها به صورت ترتیبی با `[head(x) for head in self.heads]` در متد `forward` اجرا می‌شوند. می‌توان این اجرا را به صورت موازی بهبود داد. یکی از روش‌ها، محاسبه خروجی‌های تمام سرهای توجه به طور همزمان با ضرب ماتریسی است که بسیار کارآمدتر است و در ادامه پیاده‌سازی خواهیم کرد.

---

### ۳.۶.۲ پیاده‌سازی توجه چندسر (Multi-Head Attention) با تقسیم وزن‌ها

تا اینجا، ما یک کلاس `MultiHeadAttentionWrapper` ساخته‌ایم که توجه چندسر را با قرار دادن چند ماژول توجه تک‌سر پیاده‌سازی می‌کند. این کار با ایجاد و ترکیب چند شیء از کلاس `CausalAttention` انجام شده است.

به جای داشتن دو کلاس جداگانه (`MultiHeadAttentionWrapper` و `CausalAttention`)، می‌توان این مفاهیم را در یک کلاس واحد به نام `MultiHeadAttention` ترکیب کرد. علاوه بر این، در کنار ادغام کدهای `MultiHeadAttentionWrapper` و `CausalAttention`، تغییرات دیگری نیز اعمال می‌کنیم تا توجه چندسر به صورت بهینه‌تر اجرا شود.

در `MultiHeadAttentionWrapper`، هر سر توجه به صورت یک شیء مستقل از `CausalAttention` پیاده‌سازی شده و نتایج هر سر پس از محاسبه به هم چسبانده می‌شوند. اما در کلاس `MultiHeadAttention` جدید، عملکرد چندسر در یک کلاس واحد ترکیب شده است. این کلاس با تغییر شکل (reshape) تنسورهای پروجکت شده کوئری (query)، کلید (key) و مقدار (value)، ورودی را به چند سر تقسیم می‌کند و سپس پس از محاسبه توجه، نتایج سرها را دوباره ترکیب می‌کند.

---

### کد کلاس MultiHeadAttention

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert (d_out % num_heads == 0), "d_out must be divisible by num_heads"
        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)
        self.dropout = nn.Dropout(dropout)

        self.register_buffer(
            "mask",
            torch.triu(torch.ones(context_length, context_length), diagonal=1)
        )

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)
        queries = self.W_query(x)
        values = self.W_value(x)

        # تغییر شکل تنسورها به ابعاد (batch, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # جابجایی ابعاد برای تسهیل ضرب ماتریسی در سرهای مختلف
        keys = keys.transpose(1, 2)      # (b, num_heads, num_tokens, head_dim)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # محاسبه نمرات توجه (attention scores)
        attn_scores = queries @ keys.transpose(2, 3)

        # اعمال ماسک برای جلوگیری از نگاه به آینده (causal mask)
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        # محاسبه وزن‌های توجه با نرم‌افزار softmax
        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # ضرب وزن‌های توجه در مقادیر (values) و بازگشت به شکل اصلی
        context_vec = (attn_weights @ values).transpose(1, 2)
        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)

        # اعمال لایه خروجی (projection)
        context_vec = self.out_proj(context_vec)

        return context_vec
````

---

### توضیحات کلیدی

* تغییر شکل (reshape) و جابجایی (transpose) تنسورها در این کلاس ممکن است در ابتدا پیچیده به نظر برسد، اما اساساً همان عملکرد کلاس قبلی `MultiHeadAttentionWrapper` را با رویکردی یکپارچه‌تر و کاراتر پیاده‌سازی می‌کند.
* در `MultiHeadAttentionWrapper`، چندین لایه توجه تک‌سر ساخته و به صورت مستقل اجرا می‌شدند، اما در این کلاس جدید، ابتدا لایه چندسر ساخته شده و سپس ورودی به صورت داخلی به چند سر تقسیم می‌شود.
* تقسیم تنسورهای کوئری، کلید و مقدار با استفاده از `.view` و `.transpose` انجام می‌شود که شکل (b, num_tokens, d_out) را به (b, num_heads, num_tokens, head_dim) تبدیل می‌کند، که در آن `head_dim = d_out / num_heads`.
* ضرب ماتریسی با استفاده از عملیات بچ (batch) در PyTorch انجام می‌شود که برای هر سر به صورت موازی محاسبه می‌کند.
* پس از محاسبه وزن‌های توجه و بردارهای کانتکست، نتایج از همه سرها دوباره به شکل (b, num_tokens, d_out) ترکیب می‌شوند.
* لایه خروجی `out_proj` برای ترکیب و تبدیل نهایی خروجی‌ها استفاده می‌شود. این لایه در کلاس `CausalAttention` وجود ندارد ولی در معماری‌های بزرگ مدل‌های زبان (LLM) معمول است.
* این روش از نظر محاسباتی بهینه‌تر است، چون به جای چند بار ضرب ماتریس برای هر سر، تنها یک ضرب ماتریس برای کل ورودی انجام می‌شود که هزینه محاسباتی را کاهش می‌دهد.

---

### مثال ساده از ضرب ماتریس بچ (Batched Matrix Multiplication)

فرض کنید تنسوری به شکل زیر داریم:

```python
a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],
                    [0.8993, 0.0390, 0.9268, 0.7388],
                    [0.7179, 0.7058, 0.9156, 0.4340]],
                   [[0.0772, 0.3565, 0.1479, 0.5331],
                    [0.4066, 0.2318, 0.4545, 0.9737],
                    [0.4606, 0.5159, 0.4220, 0.5786]]]])
```

حالا عملیات ضرب ماتریس بچ بین این تنسور و ترانهاده‌ی آخرین دو بعد آن را انجام می‌دهیم:

```python
print(a @ a.transpose(2, 3))
```

نتیجه:

```python
tensor([[[[1.3208, 1.1631, 1.2879],
          [1.1631, 2.2150, 1.8424],
          [1.2879, 1.8424, 2.0402]],
         [[0.4391, 0.7003, 0.5903],
          [0.7003, 1.3737, 1.0620],
          [0.5903, 1.0620, 0.9912]]]])
```

این عملیات به صورت موازی برای هر سر (head) به صورت جداگانه انجام می‌شود. برای اثبات می‌توان ضرب ماتریسی را برای هر سر به صورت جداگانه محاسبه کرد:

```python
first_head = a[0, 0, :, :]
first_res = first_head @ first_head.T
print("First head:\n", first_res)

second_head = a[0, 1, :, :]
second_res = second_head @ second_head.T
print("\nSecond head:\n", second_res)
```

که نتایج مشابه همان ضرب ماتریس بچ خواهد بود.

---

### استفاده از کلاس MultiHeadAttention

می‌توانیم کلاس `MultiHeadAttention` را مشابه کلاس‌های `SelfAttention` و `CausalAttention` استفاده کنیم:

```python
torch.manual_seed(123)
batch_size, context_length, d_in = batch.shape
d_out = 2
mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)
context_vecs = mha(batch)
print(context_vecs)
print("context_vecs.shape:", context_vecs.shape)
```

خروجی نشان می‌دهد که بعد خروجی به صورت مستقیم توسط `d_out` کنترل می‌شود:

```text
tensor([[[0.3190, 0.4858],
         [0.2943, 0.3897],
         [0.2856, 0.3593],
         [0.2693, 0.3873],
         [0.2639, 0.3928],
         [0.2575, 0.4028]],
        [[0.3190, 0.4858],
         [0.2943, 0.3897],
         [0.2856, 0.3593],
         [0.2693, 0.3873],
         [0.2639, 0.3928],
         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)
context_vecs.shape: torch.Size([2, 6, 2])
```

---

### نکات نهایی

* ما اکنون کلاس `MultiHeadAttention` را پیاده‌سازی کرده‌ایم که برای آموزش مدل‌های بزرگ زبان (LLM) استفاده خواهد شد.
* در مثال‌های اینجا از ابعاد کوچک‌تر و تعداد سرهای کم استفاده شده تا خروجی‌ها قابل مشاهده و بررسی باشند.
* برای مقایسه، مدل GPT-2 کوچک‌ترین نسخه‌اش ۱۲ سر توجه و اندازه تعبیه (embedding size) ۷۶۸ دارد، و بزرگ‌ترین نسخه ۲۵ سر توجه و اندازه تعبیه ۱۶۰۰ دارد.
* در مدل‌های GPT، اندازه تعبیه ورودی و خروجی برابر است (`d_in = d_out`).

---

### تمرین ۳.۳: مقداردهی اولیه ماژول‌های توجه مطابق اندازه GPT-2

با استفاده از کلاس `MultiHeadAttention`، یک ماژول توجه چندسر با ۱۲ سر توجه مانند کوچک‌ترین مدل GPT-2 مقداردهی کنید. همچنین باید اندازه تعبیه ورودی و خروجی برابر ۷۶۸ باشد. دقت کنید که طول کانتکست (context length) برابر ۱۰۲۴ توکن است.

```
# راهنمایی تمرین:
mha = MultiHeadAttention(d_in=768, d_out=768, context_length=1024, dropout=0.0, num_heads=12)
```

---
## خلاصه

- مکانیزم‌های توجه (Attention) ورودی‌ها را به بردارهای کانتکست بهبود یافته تبدیل می‌کنند که اطلاعات تمامی ورودی‌ها را در بر دارند.
- مکانیزم خودتوجه (Self-Attention) بردار کانتکست را به صورت مجموع وزندار ورودی‌ها محاسبه می‌کند.
- در یک مکانیزم توجه ساده‌شده، وزن‌های توجه با استفاده از ضرب داخلی (dot product) محاسبه می‌شوند.
- ضرب داخلی به معنای ضرب عنصر به عنصر دو بردار و سپس جمع کردن نتایج است.
- ضرب ماتریسی، اگرچه الزامی نیست، اما با جایگزینی حلقه‌های تو در تو، محاسبات را به شکل کارآمدتر و فشرده‌تر انجام می‌دهد.
- در مکانیزم‌های خودتوجه مورد استفاده در مدل‌های زبان بزرگ (LLM)، که به توجه ضرب داخلی مقیاس‌دار (scaled-dot product attention) نیز معروف است، از ماتریس‌های وزن آموزش‌پذیر برای تبدیل‌های میانی ورودی‌ها شامل کوئری‌ها، کلیدها و مقدارها استفاده می‌شود.
- در مدل‌های زبانی که متن را از چپ به راست می‌خوانند و تولید می‌کنند، از ماسک توجه علی (causal attention mask) برای جلوگیری از دسترسی مدل به توکن‌های آینده استفاده می‌شود.
- علاوه بر ماسک توجه علی که وزن‌های توجه را صفر می‌کند، می‌توان ماسک دراپ‌اوت (dropout mask) نیز اضافه کرد تا از بیش‌برازش (overfitting) جلوگیری شود.
- ماژول‌های توجه در مدل‌های مبتنی بر ترنسفورمر شامل چندین نمونه از توجه علی هستند که به آن توجه چندسر (multi-head attention) گفته می‌شود.
- می‌توان ماژول توجه چندسر را با ترکیب چندین نمونه توجه علی ساخت.
- روشی کارآمدتر برای ساخت ماژول‌های توجه چندسر استفاده از ضرب ماتریسی بچ (batched matrix multiplication) است.
---

> [ 2.کار با داده‌های متنی
     (قبلی) ](<02.Working with text data.md>) <3.کدنویسی مکانیزم‌های توجه> 
[
    4.پیاده‌سازی مدل GPT از ابتدا برای تولید متن
(بعدی)
<<<<<<< HEAD
](<04.Implementing a GPT model from scratch to generate text.md>)
=======
](<04.Implementing a GPT model from scratch to generate text.md>)

>>>>>>> 07930280b26ee87b7659f10a58ba283baab3b365
