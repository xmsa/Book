<!-- language: rtl -->

# 6.ریزتنظیم مدل زبان برای طبقه‌بندی

تا اینجا، معماری مدل زبان بزرگ (LLM) را پیاده‌سازی کرده‌ایم، آن را از ابتدا آموزش داده‌ایم و یاد گرفته‌ایم چگونه وزن‌های آموزش‌دیده را از منابع خارجی (مثل OpenAI) وارد مدل خود کنیم. حالا وقت آن رسیده است که نتیجه‌ی این تلاش‌ها را ببینیم؛ با **ریزتنظیم مدل LLM برای انجام یک وظیفه‌ی خاص** مانند طبقه‌بندی متون. در مثال عملی این فصل، پیام‌های متنی را به دو دسته‌ی «اسپم» و «غیراسپم» (مجاز) تقسیم می‌کنیم.

**شکل ۶.۱** دو روش اصلی ریزتنظیم LLM را نشان می‌دهد:

- ریزتنظیم برای طبقه‌بندی (مرحله ۸)
- ریزتنظیم برای پیروی از دستورات (مرحله ۹)

> **شکل ۶.۱** مراحل سه‌گانه‌ی پیاده‌سازی یک مدل LLM را نمایش می‌دهد. تمرکز این فصل بر مرحله‌ی سوم (مرحله ۸) است: ریزتنظیم یک مدل آموزش‌دیده به‌عنوان یک طبقه‌بند.

---

## ۶.۱ دسته‌بندی‌های مختلف ریزتنظیم

دو روش رایج برای ریزتنظیم مدل‌های زبانی وجود دارد:

- **ریزتنظیم بر اساس دستور (Instruction fine-tuning)**
- **ریزتنظیم برای طبقه‌بندی (Classification fine-tuning)**

### ریزتنظیم بر اساس دستور

در این روش، مدل با مجموعه‌ای از وظایف که به شکل دستورات زبانی طبیعی بیان شده‌اند آموزش داده می‌شود تا توانایی آن در درک و اجرای وظایف مختلف بهبود یابد.  
در **شکل ۶.۲**، دو سناریوی مختلف از این نوع ریزتنظیم نمایش داده شده است:

- در بالا، از مدل خواسته شده است تشخیص دهد که آیا یک متن اسپم است یا نه.
- در پایین، مدل موظف است جمله‌ای انگلیسی را به آلمانی ترجمه کند.

### ریزتنظیم برای طبقه‌بندی

در این نوع ریزتنظیم، که برای کسانی با پیش‌زمینه‌ی یادگیری ماشین آشناست، مدل برای شناسایی یک مجموعه‌ی مشخص از برچسب‌ها (labelها) آموزش داده می‌شود؛ مثلاً «اسپم» و «غیراسپم».

نمونه‌هایی از وظایف طبقه‌بندی:

- شناسایی گونه‌های گیاهان از روی تصاویر
- دسته‌بندی اخبار بر اساس موضوع (ورزش، سیاست، فناوری و ...)
- تشخیص تومورهای خوش‌خیم و بدخیم در تصاویر پزشکی

> **نکته کلیدی:** مدل‌هایی که برای طبقه‌بندی آموزش داده شده‌اند، **فقط می‌توانند برچسب‌هایی را پیش‌بینی کنند که در طول آموزش دیده‌اند**.  
> مثلاً، مدلی که برای طبقه‌بندی اسپم آموزش دیده، فقط می‌تواند خروجی‌هایی مثل «اسپم» یا «غیراسپم» بدهد — نه بیشتر.

**شکل ۶.۳** این سناریو را نشان می‌دهد.  
در حالی که مدل‌های ریزتنظیم‌شده با دستور قادر به پاسخ‌گویی به دامنه‌ی وسیعی از وظایف هستند، مدل‌های ریزتنظیم‌شده برای طبقه‌بندی معمولاً **تخصصی‌تر** و ساده‌تر هستند.

---

### انتخاب رویکرد مناسب

| نوع ریزتنظیم                   | ویژگی‌ها                                                                                                                                                                     |
| ------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Instruction fine-tuning**    | مناسب برای مدل‌هایی که باید دستورهای متنوع و پیچیده‌ی کاربران را درک و اجرا کنند. نیازمند داده‌های بیشتر و قدرت محاسباتی بالاتر است.                                         |
| **Classification fine-tuning** | مناسب برای پروژه‌هایی با نیاز به طبقه‌بندی دقیق در دسته‌های مشخص، مثل تحلیل احساسات یا تشخیص اسپم. به داده و منابع محاسباتی کمتری نیاز دارد اما دامنه‌ی عملکردش محدودتر است. |

---

## ۶.۲ آماده‌سازی مجموعه‌داده

در ادامه، مدل GPT پیاده‌سازی‌شده را برای طبقه‌بندی ریزتنظیم می‌کنیم. ابتدا باید مجموعه‌داده‌ای مناسب را بارگیری و آماده‌سازی کنیم.

ما از مجموعه‌داده‌ای متنی شامل پیام‌های **اسپم و غیراسپم** استفاده می‌کنیم.

> **شکل ۶.۴** فرآیند سه‌مرحله‌ای برای ریزتنظیم یک مدل زبان برای طبقه‌بندی را نشان می‌دهد:  
> ۱. آماده‌سازی داده  
> ۲. پیکربندی مدل  
> ۳. ریزتنظیم و ارزیابی

> **یادآوری:** پیام‌های متنی معمولاً از طریق تلفن ارسال می‌شوند، نه ایمیل. ولی مراحل آموزش برای هر دو نوع مشابه هستند. برای داده‌های مربوط به اسپم ایمیلی می‌توانید به پیوست B مراجعه کنید.

---

### بارگیری و استخراج مجموعه‌داده

```python
import urllib.request
import zipfile
import os
from pathlib import Path

url = "https://archive.ics.uci.edu/static/public/۲۲۸/sms+spam+collection.zip"
zip_path = "sms_spam_collection.zip"
extracted_path = "sms_spam_collection"
data_file_path = Path(extracted_path) / "SMSSpamCollection.tsv"

def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):
    if data_file_path.exists():
        print(f"{data_file_path} already exists. Skipping download "
              "and extraction.")
        return
    with urllib.request.urlopen(url) as response:    #۱
        with open(zip_path, "wb") as out_file:
            out_file.write(response.read())
    with zipfile.ZipFile(zip_path, "r") as zip_ref:    #۲
        zip_ref.extractall(extracted_path)
    original_file_path = Path(extracted_path) / "SMSSpamCollection"
    os.rename(original_file_path, data_file_path)      #۳
    print(f"File downloaded and saved as {data_file_path}")

download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)
```

**توضیح مراحل:**

۱. فایل را از URL مشخص دانلود می‌کند.

۲. فایل ZIP را استخراج می‌کند.

۳. پسوند `.tsv` به فایل اضافه می‌شود.

---

### بارگذاری داده در یک DataFrame

```python
import pandas as pd
df = pd.read_csv(
    data_file_path, sep="\t", header=None, names=["Label", "Text"]
)
df  # نمایش DataFrame
```

> **شکل ۶.۵** نمایی از مجموعه‌داده‌ی SMSSpamCollection در قالب یک DataFrame پانداس است، شامل دو ستون: برچسب ("ham" یا "spam") و پیام متنی.

---

### بررسی توزیع برچسب‌ها

```python
print(df["Label"].value_counts())
```

خروجی:

```
ham     ۴۸۲۵
spam     ۷۴۷
Name: count, dtype: int۶۴
```

داده‌های "ham" (غیراسپم) بسیار بیشتر از داده‌های "spam" هستند.  
برای سادگی و سرعت در ریزتنظیم مدل، تصمیم می‌گیریم که مجموعه‌داده را **متعادل (balanced)** کنیم، به‌طوری‌که از هر دسته ۷۴۷ نمونه وجود داشته باشد.

---

### ایجاد مجموعه‌داده متعادل

```python
def create_balanced_dataset(df):
    num_spam = df[df["Label"] == "spam"].shape[۰]     #۱
    ham_subset = df[df["Label"] == "ham"].sample(
        num_spam, random_state=۱۲۳
    )                                                 #۲
    balanced_df = pd.concat([
        ham_subset, df[df["Label"] == "spam"]
    ])                                                #۳
    return balanced_df

balanced_df = create_balanced_dataset(df)
print(balanced_df["Label"].value_counts())
```

خروجی:

```
ham     ۷۴۷
spam    ۷۴۷
Name: count, dtype: int۶۴
```

---

### تبدیل برچسب‌های متنی به عددی

```python
balanced_df["Label"] = balanced_df["Label"].map({"ham": ۰, "spam": ۱})
```

> این کار مشابه تبدیل متن به شناسه‌های توکن است، ولی فقط با دو برچسب عددی (۰ و ۱) سروکار داریم، نه ۵۰هزار واژه مثل واژگان GPT.

---

### تقسیم داده به سه بخش: آموزش، اعتبارسنجی، آزمون

```python
def random_split(df, train_frac, validation_frac):
    df = df.sample(frac=۱, random_state=۱۲۳).reset_index(drop=True)  #۱
    train_end = int(len(df) * train_frac)                            #۲
    validation_end = train_end + int(len(df) * validation_frac)      #۳
    train_df = df[:train_end]
    validation_df = df[train_end:validation_end]
    test_df = df[validation_end:]
    return train_df, validation_df, test_df

train_df, validation_df, test_df = random_split(
    balanced_df, ۰.۷, ۰.۱)  # ۷۰% آموزش، ۱۰% اعتبارسنجی، ۲۰% آزمون
```

---

### ذخیره مجموعه‌داده در فایل‌های CSV

```python
train_df.to_csv("train.csv", index=None)
validation_df.to_csv("validation.csv", index=None)
test_df.to_csv("test.csv", index=None)
```

> تا اینجا، مجموعه‌داده را بارگیری، متعادل، و به سه بخش تقسیم کردیم. در مرحله بعدی، داده‌ها را برای آموزش مدل با PyTorch آماده‌سازی می‌کنیم.

## ۶.۳ ایجاد DataLoader‌ ها

در این بخش، داده‌بارهای (DataLoader) مورد نیاز برای آموزش مدل را با استفاده از PyTorch پیاده‌سازی می‌کنیم. این مرحله مشابه آن چیزی است که در فصل‌های قبل هنگام کار با داده‌های متنی انجام دادیم. پیش‌تر، با استفاده از **تکنیک پنجره‌ی لغزان (sliding window)**، قطعات متنی هم‌اندازه ایجاد کرده و آن‌ها را برای آموزش به‌صورت batch درآوردیم. هر قطعه به‌عنوان یک نمونه آموزشی عمل می‌کرد.

اما این‌بار با مجموعه‌ای از پیام‌های متنی با **طول‌های متفاوت** سروکار داریم. برای ایجاد batchهایی از این پیام‌ها، دو گزینه اصلی پیش‌رو داریم:

۱. **کوتاه کردن همه‌ی پیام‌ها به اندازه‌ی کوتاه‌ترین پیام**

۲. **پُر کردن (pad) همه‌ی پیام‌ها تا اندازه‌ی بلندترین پیام**

گزینه‌ی اول از نظر محاسباتی سبک‌تر است، اما اگر پیام‌های کوتاه‌تر به‌طور قابل توجهی از متوسط یا بیشینه طول پیام‌ها کوتاه‌تر باشند، ممکن است اطلاعات مهمی از بین برود و عملکرد مدل کاهش یابد.

بنابراین، ما گزینه‌ی دوم را انتخاب می‌کنیم:

> **همه‌ی پیام‌ها را تا اندازه‌ی بلندترین پیام با توکن پرکننده (padding) پُر می‌کنیم.**

### انتخاب توکن پرکننده (padding token)

برای این کار، از توکن ویژه‌ی `<|endoftext|>` استفاده می‌کنیم.  
اما به جای چسباندن این رشته‌ی متنی به پیام‌ها، شناسه‌ی توکن معادل آن را به پیام‌های رمزگذاری‌شده اضافه می‌کنیم.

برای اطمینان از اینکه شناسه‌ی توکن صحیح است، می‌توانیم از توکنایزر `tiktoken` برای مدل GPT-۲ استفاده کنیم:

```python
import tiktoken
tokenizer = tiktoken.get_encoding("gpt۲")
print(tokenizer.encode("<|endoftext|>", allowed_special={"<|endoftext|>"}))
```

> این کد خروجی `[۵۰۲۵۶]` را تولید می‌کند، که نشان می‌دهد شناسه‌ی توکن `<|endoftext|>` برابر ۵۰۲۵۶ است.

---

### فرآیند آماده‌سازی ورودی (شکل ۶.۶)

۱. هر پیام متنی به یک دنباله از شناسه‌های توکن تبدیل می‌شود.

۲. برای اطمینان از یکسان بودن طول دنباله‌ها، پیام‌های کوتاه‌تر با توکن ۵۰۲۵۶ پُر می‌شوند تا با طول بلندترین پیام برابر شوند.

---

### تعریف کلاس Dataset برای PyTorch

```python
import torch
from torch.utils.data import Dataset

class SpamDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=۵۰۲۵۶):
        self.data = pd.read_csv(csv_file)  #۱

        self.encoded_texts = [
            tokenizer.encode(text) for text in self.data["Text"]
        ]

        if max_length is None:
            self.max_length = self._longest_encoded_length()
        else:
            self.max_length = max_length
            self.encoded_texts = [
                encoded_text[:self.max_length]
                for encoded_text in self.encoded_texts
            ]  #۲

        self.encoded_texts = [
            encoded_text + [pad_token_id] *
            (self.max_length - len(encoded_text))
            for encoded_text in self.encoded_texts
        ]  #۳

    def __getitem__(self, index):
        encoded = self.encoded_texts[index]
        label = self.data.iloc[index]["Label"]
        return (
            torch.tensor(encoded, dtype=torch.long),
            torch.tensor(label, dtype=torch.long)
        )

    def __len__(self):
        return len(self.data)

    def _longest_encoded_length(self):
        max_length = ۰
        for encoded_text in self.encoded_texts:
            if len(encoded_text) > max_length:
                max_length = len(encoded_text)
        return max_length
```

**توضیحات:**

- `#۱`: پیام‌ها را token‌سازی می‌کند.
- `#۲`: اگر دنباله‌ای طولانی‌تر از `max_length` باشد، آن را کوتاه می‌کند.
- `#۳`: دنباله‌های کوتاه‌تر را با توکن پرکننده تکمیل می‌کند.

---

### ایجاد شیء Dataset برای مجموعه آموزش

```python
train_dataset = SpamDataset(
    csv_file="train.csv",
    max_length=None,
    tokenizer=tokenizer
)
```

اگر بخواهید طول بلندترین پیام را ببینید:

```python
print(train_dataset.max_length)
```

> خروجی: `۱۲۰`  
> این یعنی طولانی‌ترین پیام حداکثر ۱۲۰ توکن دارد — عددی منطقی برای پیامک‌ها.

در صورتی که داده‌هایتان طولانی‌تر هستند، می‌توانید مقدار `max_length=۱۰۲۴` را تعیین کنید تا از حد مجاز مدل (context length) فراتر نرود.

---

### ایجاد مجموعه‌های اعتبارسنجی و آزمون

برای اطمینان از سازگاری، مجموعه‌های اعتبارسنجی و آزمون را با همان طول دنباله‌ای که در آموزش استفاده شد، pad می‌کنیم:

```python
val_dataset = SpamDataset(
    csv_file="validation.csv",
    max_length=train_dataset.max_length,
    tokenizer=tokenizer
)

test_dataset = SpamDataset(
    csv_file="test.csv",
    max_length=train_dataset.max_length,
    tokenizer=tokenizer
)
```

> اگر داده‌های اعتبارسنجی/آزمون از حد مجاز فراتر رفتند، توسط `encoded_text[:self.max_length]` کوتاه خواهند شد.

---

### تمرین ۶.۱: افزایش طول کانتکست

می‌توانید ورودی‌ها را تا بیشینه‌ی تعداد توکن قابل پشتیبانی توسط مدل (مثلاً ۱۰۲۴) پُر کنید و ببینید این تغییر چه اثری بر دقت مدل دارد.

---

### ایجاد DataLoaderها

حالا می‌توانیم با استفاده از مجموعه‌داده‌ها، داده‌بارهایی برای آموزش، اعتبارسنجی و آزمون بسازیم:

```python
from torch.utils.data import DataLoader

num_workers = ۰  # سازگاری با اغلب سیستم‌ها
batch_size = ۸
torch.manual_seed(۱۲۳)

train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=num_workers,
    drop_last=True,
)

val_loader = DataLoader(
    dataset=val_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    drop_last=False,
)

test_loader = DataLoader(
    dataset=test_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    drop_last=False,
)
```

---

### بررسی ابعاد خروجی یک batch

```python
for input_batch, target_batch in train_loader:
    pass
print("Input batch dimensions:", input_batch.shape)
print("Label batch dimensions", target_batch.shape)
```

خروجی:

```
Input batch dimensions: torch.Size([۸, ۱۲۰])
Label batch dimensions torch.Size([۸])
```

این خروجی نشان می‌دهد که هر batch شامل ۸ پیام متنی (هرکدام با ۱۲۰ توکن) به همراه برچسب مربوط به هر پیام است.

---

### شمارش تعداد batchها در هر مجموعه

```python
print(f"{len(train_loader)} training batches")
print(f"{len(val_loader)} validation batches")
print(f"{len(test_loader)} test batches")
```

خروجی:

```
۱۳۰ training batches
۱۹ validation batches
۳۸ test batches
```

## ۶.۴ مقداردهی اولیه مدل با وزن‌های پیش‌آموزش‌دیده

اکنون که داده‌ها را آماده کرده‌ایم، باید مدل را برای **ریزتنظیم (fine-tuning)** در وظیفه‌ی **طبقه‌بندی پیام‌های اسپم** آماده کنیم. در این مرحله، کار خود را با مقداردهی اولیه‌ی مدلی آغاز می‌کنیم که از پیش آموزش داده شده است، همان‌طور که در شکل ۶.۸ نشان داده شده است.

> 🔹 **شکل ۶.۸**: فرآیند سه‌مرحله‌ای ریزتنظیم مدل زبانی برای طبقه‌بندی. پس از مرحله اول (آماده‌سازی داده)، اکنون وارد مرحله دوم (مقداردهی اولیه مدل) می‌شویم.

---

### تنظیمات پایه مدل

ابتدا، از همان پیکربندی‌هایی استفاده می‌کنیم که هنگام پیش‌آموزش مدل با داده‌های بدون برچسب به کار رفته بود:

```python
CHOOSE_MODEL = "gpt۲-small (۱۲۴M)"
INPUT_PROMPT = "Every effort moves"
BASE_CONFIG = {
    "vocab_size": ۵۰۲۵۷,          #۱
    "context_length": ۱۰۲۴,       #۲
    "drop_rate": ۰.۰,             #۳
    "qkv_bias": True              #۴
}
model_configs = {
    "gpt۲-small (۱۲۴M)": {"emb_dim": ۷۶۸, "n_layers": ۱۲, "n_heads": ۱۲},
    "gpt۲-medium (۳۵۵M)": {"emb_dim": ۱۰۲۴, "n_layers": ۲۴, "n_heads": ۱۶},
    "gpt۲-large (۷۷۴M)": {"emb_dim": ۱۲۸۰, "n_layers": ۳۶, "n_heads": ۲۰},
    "gpt۲-xl (۱۵۵۸M)": {"emb_dim": ۱۶۰۰, "n_layers": ۴۸, "n_heads": ۲۵},
}
BASE_CONFIG.update(model_configs[CHOOSE_MODEL])
```

**توضیحات پیکربندی:**

- `#۱`: اندازه واژگان (vocabulary) مدل (۵۰٬۲۵۷ توکن، مطابق با GPT-۲).
- `#۲`: حداکثر طول ورودی (context length)، برابر با ۱۰۲۴ توکن.
- `#۳`: نرخ Dropout روی صفر تنظیم شده.
- `#۴`: اعمال bias در لایه‌های Query-Key-Value.

---

### بارگذاری مدل و وزن‌های پیش‌آموزش‌دیده

در این مرحله، از توابع و کلاس‌هایی که در فصل ۵ پیاده‌سازی کردیم استفاده می‌کنیم:

```python
from gpt_download import download_and_load_gpt۲
from chapter۰۵ import GPTModel, load_weights_into_gpt

model_size = CHOOSE_MODEL.split(" ")[-۱].lstrip("(").rstrip(")")
settings, params = download_and_load_gpt۲(
    model_size=model_size, models_dir="gpt۲"
)

model = GPTModel(BASE_CONFIG)
load_weights_into_gpt(model, params)
model.eval()
```

> این کد، وزن‌های مدل GPT-۲ را از فایل‌های بارگیری‌شده می‌خواند و آن‌ها را در ساختار مدل `GPTModel` بارگذاری می‌کند.

---

### بررسی درستی بارگذاری وزن‌ها

برای اطمینان از اینکه وزن‌ها به‌درستی بارگذاری شده‌اند، از تابع تولید متن ساده که در فصل‌های ۴ و ۵ ایجاد کرده بودیم استفاده می‌کنیم:

```python
from chapter۰۴ import generate_text_simple
from chapter۰۵ import text_to_token_ids, token_ids_to_text

text_۱ = "Every effort moves you"
token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(text_۱, tokenizer),
    max_new_tokens=۱۵,
    context_size=BASE_CONFIG["context_length"]
)
print(token_ids_to_text(token_ids, tokenizer))
```

**خروجی نمونه:**

```
Every effort moves you forward.
The first step is to understand the importance of your work
```

> این نتیجه نشان می‌دهد که مدل به‌درستی عمل می‌کند و قادر به تولید متنی منسجم است. بنابراین وزن‌ها به‌درستی بارگذاری شده‌اند.

---

### بررسی توانایی مدل در طبقه‌بندی اسپم (قبل از ریزتنظیم)

پیش از شروع فرآیند ریزتنظیم، بیایید بررسی کنیم آیا مدل در حالت فعلی (فقط پیش‌آموزش‌دیده، بدون ریزتنظیم) می‌تواند پیام‌های اسپم را تشخیص دهد یا خیر.

```python
text_۲ = (
    "Is the following text 'spam'? Answer with 'yes' or 'no':"
    " 'You are a winner you have been specially"
    " selected to receive $۱۰۰۰ cash or a $۲۰۰۰ award.'"
)

token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(text_۲, tokenizer),
    max_new_tokens=۲۳,
    context_size=BASE_CONFIG["context_length"]
)
print(token_ids_to_text(token_ids, tokenizer))
```

**خروجی مدل:**

```
Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner
you have been specially selected to receive $۱۰۰۰ cash
or a $۲۰۰۰ award.'
The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner
```

> بر اساس خروجی بالا، مدل هنوز قادر نیست به‌طور معنادار به دستور پاسخ دهد. به عبارتی، نمی‌تواند پاسخ بله یا خیر بدهد و تنها ورودی را تکرار می‌کند.

این کاملاً **انتظار می‌رود**، زیرا این مدل فقط **پیش‌آموزش** دیده و هنوز **ریزتنظیم بر اساس دستورالعمل‌ها یا وظایف خاص (مانند طبقه‌بندی اسپم)** نشده است.

### نتیجه‌گیری

بنابراین، گام بعدی این است که مدل را برای **ریزتنظیم در وظیفه‌ی طبقه‌بندی اسپم** آماده کنیم. در این مرحله، یک لایه‌ی طبقه‌بندی به مدل اضافه می‌کنیم تا بتواند پیام‌ها را به صورت ۰ (ham) یا ۱ (spam) دسته‌بندی کند.

## ۶.۵ افزودن یک سر طبقه‌بندی به مدل

اکنون باید مدل زبان بزرگ (LLM) از پیش‌آموزش‌دیده را برای **ریزتنظیم در وظیفه‌ی طبقه‌بندی** آماده کنیم. برای این کار، لازم است که **لایه خروجی اصلی مدل** را با یک لایه جدید جایگزین کنیم؛ لایه‌ای که به‌جای پیش‌بینی توکن‌های بعدی، کلاس‌های خروجی را پیش‌بینی کند.

---

### 🎯 هدف: تبدیل مدل GPT از پیش‌بینی توکن‌ها به طبقه‌بندی اسپم

همان‌طور که در شکل ۶.۹ نشان داده شده است:

> 🔹 مدل اصلی GPT-۲ خروجی‌ای با اندازهٔ واژگان (۵۰٬۲۵۷ توکن) تولید می‌کند.  
> 🔹 در طبقه‌بندی اسپم، ما فقط به دو کلاس نیاز داریم:  
> ‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌🔸 ۰ = «not spam»  
> ‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌‌🔸 ۱ = «spam»  
> 🔹 پس، لایه‌ی خروجی را به یک لایه خطی (Linear) با **۲ نورون خروجی** تغییر می‌دهیم.

---

### چرا ۲ نورون خروجی و نه فقط یک؟

از آنجا که مسئله‌ی ما **دودویی (باینری)** است، می‌توانستیم فقط یک نود خروجی استفاده کنیم و مثلاً از تابع آستانه‌ای (مثل sigmoid) بهره ببریم. اما در اینجا **رویکرد عمومی‌تری** اتخاذ شده است:

- استفاده از **۲ نورون خروجی** با **تابع loss `CrossEntropyLoss`** که برای چندکلاسه طراحی شده.
- این باعث می‌شود که به‌راحتی بتوان مدل را به طبقه‌بندی‌های چندکلاسه نیز تعمیم داد.

---

### 🔍 بررسی معماری مدل قبل از تغییر

قبل از اعمال تغییرات، معماری فعلی مدل را با دستور `print(model)` بررسی می‌کنیم. خروجی به شکل زیر خواهد بود:

```
GPTModel(
  (tok_emb): Embedding(۵۰۲۵۷, ۷۶۸)
  (pos_emb): Embedding(۱۰۲۴, ۷۶۸)
  ...
  (trf_blocks): Sequential(
    (۱۱): TransformerBlock(...)
  )
  (final_norm): LayerNorm()
  (out_head): Linear(in_features=۷۶۸, out_features=۵۰۲۵۷)
)
```

در اینجا می‌بینیم که لایه‌ی خروجی (`out_head`) ورودی‌هایی با ۷۶۸ ویژگی را به خروجی‌هایی با ۵۰٬۲۵۷ ویژگی (تعداد توکن‌ها) نگاشت می‌کند. حال می‌خواهیم آن را به **۲ ویژگی (کلاس)** کاهش دهیم.

---

### 🧠 مراحل آماده‌سازی مدل برای طبقه‌بندی

### ۱. فریز کردن تمام وزن‌ها (غیرفعال کردن آموزش)

در ابتدا، تمام وزن‌های مدل را **فریز** می‌کنیم تا فقط بخش‌های موردنظرمان در آموزش تغییر یابند:

```python
for param in model.parameters():
    param.requires_grad = False
```

---

### ۲. جایگزینی لایه خروجی با لایه طبقه‌بندی جدید

```python
import torch

torch.manual_seed(۱۲۳)
num_classes = ۲

model.out_head = torch.nn.Linear(
    in_features=BASE_CONFIG["emb_dim"],  # ۷۶۸ for GPT-۲ small
    out_features=num_classes
)
```

نکته:

- `BASE_CONFIG["emb_dim"]` برابر با ۷۶۸ در مدل GPT-۲ Small است.
- این کد با مدل‌های بزرگ‌تر نیز سازگار است.

---

### ۳. قابل آموزش کردن لایه‌های نهایی مدل

همچنین تصمیم می‌گیریم **به‌جز لایه خروجی، آخرین بلاک ترنسفورمر و لایه نرمال‌سازی نهایی** نیز قابل آموزش باشند:

```python
for param in model.trf_blocks[-۱].parameters():
    param.requires_grad = True

for param in model.final_norm.parameters():
    param.requires_grad = True
```

> 🔸 این کار باعث می‌شود مدل علاوه بر خروجی، از ویژگی‌های سطح بالا نیز در یادگیری طبقه‌بندی بهره بگیرد.  
> 🔸 بقیه‌ی لایه‌ها همچنان ثابت (فریز شده) باقی می‌مانند.

---

### 🧪 بررسی نحوه‌ی استفاده از مدل اصلاح‌شده

### ورودی نمونه:

```python
inputs = tokenizer.encode("Do you have time")
inputs = torch.tensor(inputs).unsqueeze(۰)  # اضافه کردن بعد batch
print("Inputs:", inputs)
print("Inputs dimensions:", inputs.shape)
```

خروجی:

```
Inputs: tensor([[۵۲۱۱,  ۳۴۵,  ۴۲۳,  ۶۴۰]])
Inputs dimensions: torch.Size([۱, ۴])
```

---

### عبور دادن ورودی از مدل:

```python
with torch.no_grad():
    outputs = model(inputs)
print("Outputs:\n", outputs)
print("Outputs dimensions:", outputs.shape)
```

خروجی نمونه:

```
Outputs:
 tensor([[[-۱.۵۸۵۴,  ۰.۹۹۰۴],
          [-۳.۷۲۳۵,  ۷.۴۵۴۸],
          [-۲.۲۶۶۱,  ۶.۶۰۴۹],
          [-۳.۵۹۸۳,  ۳.۹۹۰۲]]])
Outputs dimensions: torch.Size([۱, ۴, ۲])
```

> اکنون خروجی مدل به‌جای `[۱, ۴, ۵۰۲۵۷]` برابر با `[۱, ۴, ۲]` است، زیرا فقط دو کلاس داریم.

---

### استخراج خروجی مربوط به آخرین توکن

در مدل‌های مبتنی بر **self-attention علّی (causal)**، هر توکن فقط به خودش و توکن‌های قبلی دسترسی دارد. بنابراین:

✅ **آخرین توکن** تنها توکنی است که اطلاعات تمام توکن‌های قبلی را در اختیار دارد.  
⛔ پس بررسی توکن‌های اول یا میانی بی‌معنی است چون دید کامل ندارند.

کد برای استخراج آخرین خروجی:

```python
print("Last output token:", outputs[:, -۱, :])
```

خروجی:

```
Last output token: tensor([[-۳.۵۹۸۳,  ۳.۹۹۰۲]])
```

---

### 📘 نتیجه‌گیری

مدل اکنون آماده است تا در فرآیند ریزتنظیم یاد بگیرد که آیا یک متن ورودی، **اسپم** است یا **نه**. در ادامه:

- از **تابع loss مثل `CrossEntropyLoss`** استفاده می‌کنیم.
- فقط از **خروجی آخرین توکن** برای محاسبه loss استفاده می‌شود.

---

## 📌 تمرین‌های پیشنهادی

🔶 **تمرین ۶.۲**: به‌جای فقط آموزش دادن بلاک نهایی، کل مدل را آموزش دهید و تأثیر آن را بر عملکرد بررسی کنید.

🔷 **تمرین ۶.۳**: به‌جای استفاده از آخرین توکن، از اولین توکن خروجی برای طبقه‌بندی استفاده کنید و نتایج را مقایسه نمایید.

## ۶.۶ – محاسبه خطا (Loss) و دقت (Accuracy) طبقه‌بندی

## 🎯 هدف این بخش:

ما می‌خواهیم مدل را قبل، حین، و بعد از آموزش ارزیابی کنیم تا ببینیم چقدر خوب می‌تواند پیام‌های اسپم را تشخیص دهد.  
برای این کار، دو تابع اصلی تعریف می‌کنیم:

۱. تابع محاسبه دقت (Accuracy) مدل

۲. تابع محاسبه تابع خطا (Loss)

---

### ✅ ۱. استخراج برچسب از خروجی مدل

خروجی مدل ما، همانطور که دیدیم، برای هر ورودی یک بردار دو‌بعدی مانند زیر تولید می‌کند:

```python
Last output token: tensor([[-۳.۵۹۸۳,  ۳.۹۹۰۲]])
```

این یعنی مدل برای کلاس "not spam" امتیاز `-۳.۵۹` و برای "spam" امتیاز `۳.۹۹` داده است. برای به‌دست آوردن پیش‌بینی نهایی، از `argmax` استفاده می‌کنیم:

```python
logits = outputs[:, -۱, :]
label = torch.argmax(logits)
print("Class label:", label.item())  # خروجی: ۱ (یعنی اسپم)
```

> 🔸 از `softmax` هم می‌توان استفاده کرد، ولی چون `argmax` فقط بیشترین مقدار را می‌خواهد، نیازی نیست.

---

### ✅ ۲. تابع محاسبه دقت (Accuracy)

برای محاسبه دقت مدل روی یک دیتاست، باید خروجی مدل را با برچسب واقعی مقایسه کنیم:

```python
def calc_accuracy_loader(data_loader, model, device, num_batches=None):
    model.eval()  # مدل را در حالت ارزیابی قرار می‌دهد (بدون dropout)
    correct_predictions, num_examples = ۰, ۰

    # محدود کردن تعداد batchها (برای سرعت)
    if num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))

    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            input_batch = input_batch.to(device)
            target_batch = target_batch.to(device)
            with torch.no_grad():
                logits = model(input_batch)[:, -۱, :]  # فقط خروجی آخر
            predicted_labels = torch.argmax(logits, dim=-۱)
            num_examples += predicted_labels.shape[۰]
            correct_predictions += (
                (predicted_labels == target_batch).sum().item()
            )
        else:
            break
    return correct_predictions / num_examples
```

---

## ✅ ۳. اجرای تابع دقت

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

torch.manual_seed(۱۲۳)

train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=۱۰)
val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=۱۰)
test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=۱۰)

print(f"Training accuracy: {train_accuracy*۱۰۰:.۲f}%")
print(f"Validation accuracy: {val_accuracy*۱۰۰:.۲f}%")
print(f"Test accuracy: {test_accuracy*۱۰۰:.۲f}%")
```

### 📌 خروجی:

```
Training accuracy: ۴۶.۲۵%
Validation accuracy: ۴۵.۰۰%
Test accuracy: ۴۸.۷۵%
```

> 🔸 مدل تقریباً مثل یک مدل تصادفی عمل می‌کند (حدود ۵۰٪)، چون هنوز آموزش ندیده.

---

### ✅ ۴. تعریف تابع Loss برای آموزش

دقت تابعی نیست که بتوان آن را به‌طور مستقیم بهینه کرد. برای آموزش، باید تابعی استفاده کنیم که قابل مشتق‌گیری باشد.  
در طبقه‌بندی، از **cross-entropy loss** استفاده می‌کنیم.

### تابع محاسبه loss برای یک batch:

```python
def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch = input_batch.to(device)
    target_batch = target_batch.to(device)
    logits = model(input_batch)[:, -۱, :]  # فقط آخرین توکن مهم است
    loss = torch.nn.functional.cross_entropy(logits, target_batch)
    return loss
```

---

### تابع محاسبه loss روی کل دیتاست (یا n تا batch):

```python
def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = ۰.
    if len(data_loader) == ۰:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))

    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches
```

---

### اجرای محاسبه‌ی loss:

```python
with torch.no_grad():  # چون آموزش نمی‌دهیم، گرادیان لازم نیست
    train_loss = calc_loss_loader(train_loader, model, device, num_batches=۵)
    val_loss = calc_loss_loader(val_loader, model, device, num_batches=۵)
    test_loss = calc_loss_loader(test_loader, model, device, num_batches=۵)

print(f"Training loss: {train_loss:.۳f}")
print(f"Validation loss: {val_loss:.۳f}")
print(f"Test loss: {test_loss:.۳f}")
```

### 📌 خروجی:

```
Training loss: ۲.۴۵۳
Validation loss: ۲.۵۸۳
Test loss: ۲.۳۲۲
```

> 🔸 این مقادیر بالا هستند، که نشانه‌ای از عدم یادگیری مدل است.

---

## 📌 جمع‌بندی

در این بخش ما:

- خروجی مدل را به برچسب تبدیل کردیم (با `argmax`)
- دقت مدل را محاسبه کردیم (`calc_accuracy_loader`)
- تابع loss مناسب (cross-entropy) تعریف کردیم
- مقدار اولیه‌ی loss را اندازه گرفتیم

## ۶.۷ - ریزتنظیم مدل روی داده‌های برچسب‌دار (supervised)

ما می‌خواهیم مدل زبانی از قبل آموزش‌دیده (pretrained LLM) را روی یک مجموعه داده‌ی طبقه‌بندی‌شده برای تشخیص اسپم **آموزش نهایی** بدهیم تا عملکرد آن در طبقه‌بندی بهبود یابد.

---

### حلقه آموزش (Training Loop)

#### 🎯 هدف:

استفاده از داده‌های آموزشی برای:

- محاسبه loss
- به‌روزرسانی وزن‌های مدل
- محاسبه دقت و loss بعد از هر epoch

---

### ✅ کد تابع آموزش:

```python
def train_classifier_simple(
        model, train_loader, val_loader, optimizer, device,
        num_epochs, eval_freq, eval_iter):

    train_losses, val_losses, train_accs, val_accs = [], [], [], []
    examples_seen, global_step = ۰, -۱

    for epoch in range(num_epochs):
        model.train()
        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward()
            optimizer.step()

            examples_seen += input_batch.shape[۰]
            global_step += ۱

            # ارزیابی مرحله‌ای
            if global_step % eval_freq == ۰:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                print(f"Ep {epoch+۱} (Step {global_step:۰۶d}): "
                      f"Train loss {train_loss:.۳f}, "
                      f"Val loss {val_loss:.۳f}")

        # ارزیابی پس از پایان هر epoch
        train_accuracy = calc_accuracy_loader(
            train_loader, model, device, num_batches=eval_iter)
        val_accuracy = calc_accuracy_loader(
            val_loader, model, device, num_batches=eval_iter)
        print(f"Training accuracy: {train_accuracy*۱۰۰:.۲f}% | ", end="")
        print(f"Validation accuracy: {val_accuracy*۱۰۰:.۲f}%")

        train_accs.append(train_accuracy)
        val_accs.append(val_accuracy)

    return train_losses, val_losses, train_accs, val_accs, examples_seen
```

---

### 🔎 تابع ارزیابی loss:

```python
def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss
```

---

## ⚙️ اجرای آموزش:

```python
import time

start_time = time.time()
torch.manual_seed(۱۲۳)

optimizer = torch.optim.AdamW(model.parameters(), lr=۵e-۵, weight_decay=۰.۱)

num_epochs = ۵
train_losses, val_losses, train_accs, val_accs, examples_seen = \
    train_classifier_simple(
        model, train_loader, val_loader, optimizer, device,
        num_epochs=num_epochs, eval_freq=۵۰,
        eval_iter=۵
    )
end_time = time.time()

print(f"Training completed in {(end_time - start_time) / ۶۰:.۲f} minutes.")
```

---

## ✅ خروجی‌های آموزش:

```
Ep ۱ (Step ۰۰۰۰۰۰): Train loss ۲.۱۵۳, Val loss ۲.۳۹۲
Ep ۱ (Step ۰۰۰۰۵۰): Train loss ۰.۶۱۷, Val loss ۰.۶۳۷
Ep ۱ (Step ۰۰۰۱۰۰): Train loss ۰.۵۲۳, Val loss ۰.۵۵۷
Training accuracy: ۷۰.۰۰% | Validation accuracy: ۷۲.۵۰%

Ep ۲ ...
Training accuracy: ۸۲.۵۰% | Validation accuracy: ۸۵.۰۰%

...

Ep ۵ ...
Training accuracy: ۱۰۰.۰۰% | Validation accuracy: ۹۷.۵۰%

Training completed in ۵.۶۵ minutes.
```

> 🔍 دقت‌ها در حال بهبود هستند و نشانه‌ای از **یادگیری خوب بدون بیش‌برازش (overfitting)** وجود دارد.

---

## 📉 ترسیم نمودار Loss:

```python
import matplotlib.pyplot as plt

def plot_values(epochs_seen, examples_seen, train_values, val_values, label="loss"):
    fig, ax۱ = plt.subplots(figsize=(۵, ۳))
    ax۱.plot(epochs_seen, train_values, label=f"Training {label}")
    ax۱.plot(epochs_seen, val_values, linestyle="-.", label=f"Validation {label}")
    ax۱.set_xlabel("Epochs")
    ax۱.set_ylabel(label.capitalize())
    ax۱.legend()

    ax۲ = ax۱.twiny()
    ax۲.plot(examples_seen, train_values, alpha=۰)
    ax۲.set_xlabel("Examples seen")

    fig.tight_layout()
    plt.savefig(f"{label}-plot.pdf")
    plt.show()

# رسم نمودار loss:
epochs_tensor = torch.linspace(۰, num_epochs, len(train_losses))
examples_seen_tensor = torch.linspace(۰, examples_seen, len(train_losses))
plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)
```

### 📌 تفسیر نمودار:

- هر دو loss (آموزش و اعتبارسنجی) کاهش یکنواختی دارند.
- شکافی بین دو منحنی وجود ندارد ⇒ **بدون overfitting**.

---

#### 📈 ترسیم نمودار دقت (Accuracy):

```python
epochs_tensor = torch.linspace(۰, num_epochs, len(train_accs))
examples_seen_tensor = torch.linspace(۰, examples_seen, len(train_accs))
plot_values(
    epochs_tensor, examples_seen_tensor, train_accs, val_accs,
    label="accuracy"
)
```

> نتیجه: دقت به سرعت در اوایل آموزش افزایش می‌یابد و در مراحل پایانی تقریباً به ۱.۰ می‌رسد.

---

### 🎯 ارزیابی نهایی روی کل داده‌ها (بدون eval_iter):

```python
train_accuracy = calc_accuracy_loader(train_loader, model, device)
val_accuracy = calc_accuracy_loader(val_loader, model, device)
test_accuracy = calc_accuracy_loader(test_loader, model, device)

print(f"Training accuracy: {train_accuracy*۱۰۰:.۲f}%")
print(f"Validation accuracy: {val_accuracy*۱۰۰:.۲f}%")
print(f"Test accuracy: {test_accuracy*۱۰۰:.۲f}%")
```

### ✅ خروجی نهایی:

```
Training accuracy: ۹۷.۲۱%
Validation accuracy: ۹۷.۳۲%
Test accuracy: ۹۵.۶۷%
```

> این مقادیر بالا نشان می‌دهند که مدل با دقت بسیار خوبی پیام‌های اسپم را تشخیص می‌دهد. اختلاف کم بین train/val/test نشان‌دهنده **عمومی‌سازی خوب** است.

---

## 🧩 نکات پایانی:

- اگر اختلاف زیادی بین دقت train و test مشاهده شود، **احتمال overfitting** وجود دارد.
- برای بهبود عملکرد:
  - می‌توان dropout یا weight decay را افزایش داد.
  - تعداد epochها را تنظیم کرد.
- در این پروژه، **۵ epoch کافی بود** چون مدل به سرعت یاد گرفت و overfit نکرد.

---

## 📌 خلاصه تصویری:

| مرحله                              | عملکرد                                  |
| ---------------------------------- | --------------------------------------- |
| ✅ آموزش مدل با داده‌های برچسب‌دار | ریزتنظیم کامل شد                        |
| 📉 کاهش loss                       | در هر epoch دیده شد                     |
| 📈 افزایش دقت                      | تا ۹۷٪ رسید                             |
| 🔬 ارزیابی نهایی                   | بدون overfitting، نتایج خوب در test set |

## ۶.۸ استفاده از LLM به‌عنوان کلاسه‌بند اسپم

بعد از اینکه مدل را ریزتنظیم (fine-tune) و ارزیابی کردیم، آماده‌ایم که از آن برای **طبقه‌بندی پیام‌های جدید** به اسپم یا غیر اسپم استفاده کنیم (شکل ۶.۱۸).

---

### 🛠️ تابع `classify_review`

این تابع متن جدید را می‌گیرد و پس از پردازش مشابه مرحله‌ی آماده‌سازی داده‌ها (tokenization و padding)، با مدل پیش‌بینی انجام می‌دهد و برچسب "spam" یا "not spam" را برمی‌گرداند.

```python
def classify_review(
        text, model, tokenizer, device, max_length=None,
        pad_token_id=۵۰۲۵۶):
    model.eval()
    input_ids = tokenizer.encode(text)          #۱

    supported_context_length = model.pos_emb.weight.shape[۱]

    input_ids = input_ids[:min(                   #۲
        max_length, supported_context_length
    )]

    input_ids += [pad_token_id] * (max_length - len(input_ids))    #۳

    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(۰)   #۴

    with torch.no_grad():                                #۵
        logits = model(input_tensor)[:, -۱, :]     #۶

    predicted_label = torch.argmax(logits, dim=-۱).item()

    return "spam" if predicted_label == ۱ else "not spam"     #۷
```

### توضیحات کد:

- (۱) متن ورودی را به توکن‌ها تبدیل می‌کند.
- (۲) اگر طول توکن‌ها از `max_length` یا طول پشتیبانی مدل بیشتر بود، کوتاه می‌کند.
- (۳) توکن‌ها را تا طول `max_length` با توکن‌های padding پر می‌کند.
- (۴) توکن‌ها را به Tensor تبدیل و به شکل batch در می‌آورد.
- (۵) مدل را در حالت inference بدون محاسبه گرادیان قرار می‌دهد.
- (۶) لاجیت‌های آخرین توکن را می‌گیرد (خروجی مدل).
- (۷) بر اساس بیشینه مقدار لاجیت، برچسب پیش‌بینی را تعیین می‌کند.

---

### تست تابع روی نمونه‌ها

```python
text_۱ = (
    "You are a winner you have been specially"
    " selected to receive $۱۰۰۰ cash or a $۲۰۰۰ award."
)
print(classify_review(text_۱, model, tokenizer, device, max_length=train_dataset.max_length))
# خروجی: "spam"

text_۲ = (
    "Hey, just wanted to check if we're still on"
    " for dinner tonight? Let me know!"
)
print(classify_review(text_۲, model, tokenizer, device, max_length=train_dataset.max_length))
# خروجی: "not spam"
```

---

### ذخیره و بارگذاری مدل

برای استفاده مجدد بدون نیاز به آموزش مجدد:

```python
torch.save(model.state_dict(), "review_classifier.pth")
```

برای بارگذاری مدل ذخیره‌شده:

```python
model_state_dict = torch.load("review_classifier.pth", map_location=device)
model.load_state_dict(model_state_dict)
```

---

## 📝 خلاصه نکات مهم:

- **ریزتنظیم طبقه‌بندی** (classification fine-tuning) شامل جایگزینی لایه خروجی مدل زبانی با لایه‌ای کوچک‌تر برای طبقه‌بندی است (مثلاً ۲ خروجی برای اسپم/غیر اسپم).
- در اینجا برخلاف پیش‌آموزش که مدل پیش‌بینی توکن بعدی بود، مدل اکنون برچسب کلاس (spam/not spam) را پیش‌بینی می‌کند.
- ورودی مدل، مانند قبل، متن به توکن‌های عددی تبدیل شده است.
- قبل از ریزتنظیم، مدل پایه pretrained بارگذاری می‌شود.
- ارزیابی مدل طبقه‌بندی با محاسبه دقت انجام می‌شود.
- تابع loss در ریزتنظیم طبقه‌بندی همان تابع cross entropy است که در پیش‌آموزش استفاده می‌شد.
# فصل ۶: 


> [ 
    5.پیش‌آموزش روی داده‌های بدون برچسب
     (قبلی) ](
        <05.Pretraining on unlabeled data.md>
        ) <6.ریزتنظیم مدل زبان برای طبقه‌بندی>
    [
    7.تنظیم دقیق برای پیروی از دستورالعمل‌ها
(بعدی)
](<07.Fine-tuning to follow instructions.md>)