<!-- language: rtl -->

# 6.Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ

ØªØ§ Ø§ÛŒÙ†Ø¬Ø§ØŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† Ø¨Ø²Ø±Ú¯ (LLM) Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒÙ…ØŒ Ø¢Ù† Ø±Ø§ Ø§Ø² Ø§Ø¨ØªØ¯Ø§ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒÙ… Ùˆ ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ‡â€ŒØ§ÛŒÙ… Ú†Ú¯ÙˆÙ†Ù‡ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø±Ø§ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø§Ø±Ø¬ÛŒ (Ù…Ø«Ù„ OpenAI) ÙˆØ§Ø±Ø¯ Ù…Ø¯Ù„ Ø®ÙˆØ¯ Ú©Ù†ÛŒÙ…. Ø­Ø§Ù„Ø§ ÙˆÙ‚Øª Ø¢Ù† Ø±Ø³ÛŒØ¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ù†ØªÛŒØ¬Ù‡â€ŒÛŒ Ø§ÛŒÙ† ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø¨ÛŒÙ†ÛŒÙ…Ø› Ø¨Ø§ **Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ù…Ø¯Ù„ LLM Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… ÛŒÚ© ÙˆØ¸ÛŒÙÙ‡â€ŒÛŒ Ø®Ø§Øµ** Ù…Ø§Ù†Ù†Ø¯ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ØªÙˆÙ†. Ø¯Ø± Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÛŒ Ø§ÛŒÙ† ÙØµÙ„ØŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¯Ùˆ Ø¯Ø³ØªÙ‡â€ŒÛŒ Â«Ø§Ø³Ù¾Ù…Â» Ùˆ Â«ØºÛŒØ±Ø§Ø³Ù¾Ù…Â» (Ù…Ø¬Ø§Ø²) ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….

**Ø´Ú©Ù„ Û¶.Û±** Ø¯Ùˆ Ø±ÙˆØ´ Ø§ØµÙ„ÛŒ Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… LLM Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯:

- Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (Ù…Ø±Ø­Ù„Ù‡ Û¸)
- Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ±ÙˆÛŒ Ø§Ø² Ø¯Ø³ØªÙˆØ±Ø§Øª (Ù…Ø±Ø­Ù„Ù‡ Û¹)

> **Ø´Ú©Ù„ Û¶.Û±** Ù…Ø±Ø§Ø­Ù„ Ø³Ù‡â€ŒÚ¯Ø§Ù†Ù‡â€ŒÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÛŒÚ© Ù…Ø¯Ù„ LLM Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. ØªÙ…Ø±Ú©Ø² Ø§ÛŒÙ† ÙØµÙ„ Ø¨Ø± Ù…Ø±Ø­Ù„Ù‡â€ŒÛŒ Ø³ÙˆÙ… (Ù…Ø±Ø­Ù„Ù‡ Û¸) Ø§Ø³Øª: Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… ÛŒÚ© Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† ÛŒÚ© Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯.

---

## Û¶.Û± Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ…

Ø¯Ùˆ Ø±ÙˆØ´ Ø±Ø§ÛŒØ¬ Ø¨Ø±Ø§ÛŒ Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø²Ø¨Ø§Ù†ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯:

- **Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø³ØªÙˆØ± (Instruction fine-tuning)**
- **Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (Classification fine-tuning)**

### Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø³ØªÙˆØ±

Ø¯Ø± Ø§ÛŒÙ† Ø±ÙˆØ´ØŒ Ù…Ø¯Ù„ Ø¨Ø§ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² ÙˆØ¸Ø§ÛŒÙ Ú©Ù‡ Ø¨Ù‡ Ø´Ú©Ù„ Ø¯Ø³ØªÙˆØ±Ø§Øª Ø²Ø¨Ø§Ù†ÛŒ Ø·Ø¨ÛŒØ¹ÛŒ Ø¨ÛŒØ§Ù† Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ø¢Ù† Ø¯Ø± Ø¯Ø±Ú© Ùˆ Ø§Ø¬Ø±Ø§ÛŒ ÙˆØ¸Ø§ÛŒÙ Ù…Ø®ØªÙ„Ù Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§Ø¨Ø¯.  
Ø¯Ø± **Ø´Ú©Ù„ Û¶.Û²**ØŒ Ø¯Ùˆ Ø³Ù†Ø§Ø±ÛŒÙˆÛŒ Ù…Ø®ØªÙ„Ù Ø§Ø² Ø§ÛŒÙ† Ù†ÙˆØ¹ Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª:

- Ø¯Ø± Ø¨Ø§Ù„Ø§ØŒ Ø§Ø² Ù…Ø¯Ù„ Ø®ÙˆØ§Ø³ØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ ÛŒÚ© Ù…ØªÙ† Ø§Ø³Ù¾Ù… Ø§Ø³Øª ÛŒØ§ Ù†Ù‡.
- Ø¯Ø± Ù¾Ø§ÛŒÛŒÙ†ØŒ Ù…Ø¯Ù„ Ù…ÙˆØ¸Ù Ø§Ø³Øª Ø¬Ù…Ù„Ù‡â€ŒØ§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¢Ù„Ù…Ø§Ù†ÛŒ ØªØ±Ø¬Ù…Ù‡ Ú©Ù†Ø¯.

### Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ

Ø¯Ø± Ø§ÛŒÙ† Ù†ÙˆØ¹ Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ…ØŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ú©Ø³Ø§Ù†ÛŒ Ø¨Ø§ Ù¾ÛŒØ´â€ŒØ²Ù…ÛŒÙ†Ù‡â€ŒÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø¢Ø´Ù†Ø§Ø³ØªØŒ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÛŒ Ù…Ø´Ø®Øµ Ø§Ø² Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ (labelÙ‡Ø§) Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯Ø› Ù…Ø«Ù„Ø§Ù‹ Â«Ø§Ø³Ù¾Ù…Â» Ùˆ Â«ØºÛŒØ±Ø§Ø³Ù¾Ù…Â».

Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø§Ø² ÙˆØ¸Ø§ÛŒÙ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:

- Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú¯ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯ÛŒØ§Ù‡Ø§Ù† Ø§Ø² Ø±ÙˆÛŒ ØªØµØ§ÙˆÛŒØ±
- Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ÙˆØ¶ÙˆØ¹ (ÙˆØ±Ø²Ø´ØŒ Ø³ÛŒØ§Ø³ØªØŒ ÙÙ†Ø§ÙˆØ±ÛŒ Ùˆ ...)
- ØªØ´Ø®ÛŒØµ ØªÙˆÙ…ÙˆØ±Ù‡Ø§ÛŒ Ø®ÙˆØ´â€ŒØ®ÛŒÙ… Ùˆ Ø¨Ø¯Ø®ÛŒÙ… Ø¯Ø± ØªØµØ§ÙˆÛŒØ± Ù¾Ø²Ø´Ú©ÛŒ

> **Ù†Ú©ØªÙ‡ Ú©Ù„ÛŒØ¯ÛŒ:** Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ **ÙÙ‚Ø· Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒÛŒ Ø±Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù†Ù†Ø¯ Ú©Ù‡ Ø¯Ø± Ø·ÙˆÙ„ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡â€ŒØ§Ù†Ø¯**.  
> Ù…Ø«Ù„Ø§Ù‹ØŒ Ù…Ø¯Ù„ÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³Ù¾Ù… Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ØŒ ÙÙ‚Ø· Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø«Ù„ Â«Ø§Ø³Ù¾Ù…Â» ÛŒØ§ Â«ØºÛŒØ±Ø§Ø³Ù¾Ù…Â» Ø¨Ø¯Ù‡Ø¯ â€” Ù†Ù‡ Ø¨ÛŒØ´ØªØ±.

**Ø´Ú©Ù„ Û¶.Û³** Ø§ÛŒÙ† Ø³Ù†Ø§Ø±ÛŒÙˆ Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.  
Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ…â€ŒØ´Ø¯Ù‡ Ø¨Ø§ Ø¯Ø³ØªÙˆØ± Ù‚Ø§Ø¯Ø± Ø¨Ù‡ Ù¾Ø§Ø³Ø®â€ŒÚ¯ÙˆÛŒÛŒ Ø¨Ù‡ Ø¯Ø§Ù…Ù†Ù‡â€ŒÛŒ ÙˆØ³ÛŒØ¹ÛŒ Ø§Ø² ÙˆØ¸Ø§ÛŒÙ Ù‡Ø³ØªÙ†Ø¯ØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ…â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ **ØªØ®ØµØµÛŒâ€ŒØªØ±** Ùˆ Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ù‡Ø³ØªÙ†Ø¯.

---

### Ø§Ù†ØªØ®Ø§Ø¨ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ù…Ù†Ø§Ø³Ø¨

| Ù†ÙˆØ¹ Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ…                   | ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§                                                                                                                                                                     |
| ------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Instruction fine-tuning**    | Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¯Ø³ØªÙˆØ±Ù‡Ø§ÛŒ Ù…ØªÙ†ÙˆØ¹ Ùˆ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø±Ø§ Ø¯Ø±Ú© Ùˆ Ø§Ø¬Ø±Ø§ Ú©Ù†Ù†Ø¯. Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ùˆ Ù‚Ø¯Ø±Øª Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒ Ø¨Ø§Ù„Ø§ØªØ± Ø§Ø³Øª.                                         |
| **Classification fine-tuning** | Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø¯Ø± Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®ØµØŒ Ù…Ø«Ù„ ØªØ­Ù„ÛŒÙ„ Ø§Ø­Ø³Ø§Ø³Ø§Øª ÛŒØ§ ØªØ´Ø®ÛŒØµ Ø§Ø³Ù¾Ù…. Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡ Ùˆ Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒ Ú©Ù…ØªØ±ÛŒ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯ Ø§Ù…Ø§ Ø¯Ø§Ù…Ù†Ù‡â€ŒÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯Ø´ Ù…Ø­Ø¯ÙˆØ¯ØªØ± Ø§Ø³Øª. |

---

## Û¶.Û² Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡

Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ØŒ Ù…Ø¯Ù„ GPT Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒâ€ŒØ´Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ÛŒØ¯ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø±Ø§ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒÙ….

Ù…Ø§ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ù…ØªÙ†ÛŒ Ø´Ø§Ù…Ù„ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ **Ø§Ø³Ù¾Ù… Ùˆ ØºÛŒØ±Ø§Ø³Ù¾Ù…** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….

> **Ø´Ú©Ù„ Û¶.Û´** ÙØ±Ø¢ÛŒÙ†Ø¯ Ø³Ù‡â€ŒÙ…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… ÛŒÚ© Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯:  
> Û±. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡  
> Û². Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„  
> Û³. Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ

> **ÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒ:** Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø§Ø² Ø·Ø±ÛŒÙ‚ ØªÙ„ÙÙ† Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ØŒ Ù†Ù‡ Ø§ÛŒÙ…ÛŒÙ„. ÙˆÙ„ÛŒ Ù…Ø±Ø§Ø­Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯Ùˆ Ù†ÙˆØ¹ Ù…Ø´Ø§Ø¨Ù‡ Ù‡Ø³ØªÙ†Ø¯. Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§Ø³Ù¾Ù… Ø§ÛŒÙ…ÛŒÙ„ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ Ù¾ÛŒÙˆØ³Øª B Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯.

---

### Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡

```python
import urllib.request
import zipfile
import os
from pathlib import Path

url = "https://archive.ics.uci.edu/static/public/Û²Û²Û¸/sms+spam+collection.zip"
zip_path = "sms_spam_collection.zip"
extracted_path = "sms_spam_collection"
data_file_path = Path(extracted_path) / "SMSSpamCollection.tsv"

def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):
    if data_file_path.exists():
        print(f"{data_file_path} already exists. Skipping download "
              "and extraction.")
        return
    with urllib.request.urlopen(url) as response:    #Û±
        with open(zip_path, "wb") as out_file:
            out_file.write(response.read())
    with zipfile.ZipFile(zip_path, "r") as zip_ref:    #Û²
        zip_ref.extractall(extracted_path)
    original_file_path = Path(extracted_path) / "SMSSpamCollection"
    os.rename(original_file_path, data_file_path)      #Û³
    print(f"File downloaded and saved as {data_file_path}")

download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)
```

**ØªÙˆØ¶ÛŒØ­ Ù…Ø±Ø§Ø­Ù„:**

Û±. ÙØ§ÛŒÙ„ Ø±Ø§ Ø§Ø² URL Ù…Ø´Ø®Øµ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

Û². ÙØ§ÛŒÙ„ ZIP Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

Û³. Ù¾Ø³ÙˆÙ†Ø¯ `.tsv` Ø¨Ù‡ ÙØ§ÛŒÙ„ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

---

### Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± ÛŒÚ© DataFrame

```python
import pandas as pd
df = pd.read_csv(
    data_file_path, sep="\t", header=None, names=["Label", "Text"]
)
df  # Ù†Ù…Ø§ÛŒØ´ DataFrame
```

> **Ø´Ú©Ù„ Û¶.Ûµ** Ù†Ù…Ø§ÛŒÛŒ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÛŒ SMSSpamCollection Ø¯Ø± Ù‚Ø§Ù„Ø¨ ÛŒÚ© DataFrame Ù¾Ø§Ù†Ø¯Ø§Ø³ Ø§Ø³ØªØŒ Ø´Ø§Ù…Ù„ Ø¯Ùˆ Ø³ØªÙˆÙ†: Ø¨Ø±Ú†Ø³Ø¨ ("ham" ÛŒØ§ "spam") Ùˆ Ù¾ÛŒØ§Ù… Ù…ØªÙ†ÛŒ.

---

### Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§

```python
print(df["Label"].value_counts())
```

Ø®Ø±ÙˆØ¬ÛŒ:

```
ham     Û´Û¸Û²Ûµ
spam     Û·Û´Û·
Name: count, dtype: intÛ¶Û´
```

Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ "ham" (ØºÛŒØ±Ø§Ø³Ù¾Ù…) Ø¨Ø³ÛŒØ§Ø± Ø¨ÛŒØ´ØªØ± Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ "spam" Ù‡Ø³ØªÙ†Ø¯.  
Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¯Ú¯ÛŒ Ùˆ Ø³Ø±Ø¹Øª Ø¯Ø± Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ù…Ø¯Ù„ØŒ ØªØµÙ…ÛŒÙ… Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ… Ú©Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ø±Ø§ **Ù…ØªØ¹Ø§Ø¯Ù„ (balanced)** Ú©Ù†ÛŒÙ…ØŒ Ø¨Ù‡â€ŒØ·ÙˆØ±ÛŒâ€ŒÚ©Ù‡ Ø§Ø² Ù‡Ø± Ø¯Ø³ØªÙ‡ Û·Û´Û· Ù†Ù…ÙˆÙ†Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.

---

### Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ù…ØªØ¹Ø§Ø¯Ù„

```python
def create_balanced_dataset(df):
    num_spam = df[df["Label"] == "spam"].shape[Û°]     #Û±
    ham_subset = df[df["Label"] == "ham"].sample(
        num_spam, random_state=Û±Û²Û³
    )                                                 #Û²
    balanced_df = pd.concat([
        ham_subset, df[df["Label"] == "spam"]
    ])                                                #Û³
    return balanced_df

balanced_df = create_balanced_dataset(df)
print(balanced_df["Label"].value_counts())
```

Ø®Ø±ÙˆØ¬ÛŒ:

```
ham     Û·Û´Û·
spam    Û·Û´Û·
Name: count, dtype: intÛ¶Û´
```

---

### ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø¨Ù‡ Ø¹Ø¯Ø¯ÛŒ

```python
balanced_df["Label"] = balanced_df["Label"].map({"ham": Û°, "spam": Û±})
```

> Ø§ÛŒÙ† Ú©Ø§Ø± Ù…Ø´Ø§Ø¨Ù‡ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÚ©Ù† Ø§Ø³ØªØŒ ÙˆÙ„ÛŒ ÙÙ‚Ø· Ø¨Ø§ Ø¯Ùˆ Ø¨Ø±Ú†Ø³Ø¨ Ø¹Ø¯Ø¯ÛŒ (Û° Ùˆ Û±) Ø³Ø±ÙˆÚ©Ø§Ø± Ø¯Ø§Ø±ÛŒÙ…ØŒ Ù†Ù‡ ÛµÛ°Ù‡Ø²Ø§Ø± ÙˆØ§Ú˜Ù‡ Ù…Ø«Ù„ ÙˆØ§Ú˜Ú¯Ø§Ù† GPT.

---

### ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ Ø³Ù‡ Ø¨Ø®Ø´: Ø¢Ù…ÙˆØ²Ø´ØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒØŒ Ø¢Ø²Ù…ÙˆÙ†

```python
def random_split(df, train_frac, validation_frac):
    df = df.sample(frac=Û±, random_state=Û±Û²Û³).reset_index(drop=True)  #Û±
    train_end = int(len(df) * train_frac)                            #Û²
    validation_end = train_end + int(len(df) * validation_frac)      #Û³
    train_df = df[:train_end]
    validation_df = df[train_end:validation_end]
    test_df = df[validation_end:]
    return train_df, validation_df, test_df

train_df, validation_df, test_df = random_split(
    balanced_df, Û°.Û·, Û°.Û±)  # Û·Û°% Ø¢Ù…ÙˆØ²Ø´ØŒ Û±Û°% Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒØŒ Û²Û°% Ø¢Ø²Ù…ÙˆÙ†
```

---

### Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CSV

```python
train_df.to_csv("train.csv", index=None)
validation_df.to_csv("validation.csv", index=None)
test_df.to_csv("test.csv", index=None)
```

> ØªØ§ Ø§ÛŒÙ†Ø¬Ø§ØŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒØŒ Ù…ØªØ¹Ø§Ø¯Ù„ØŒ Ùˆ Ø¨Ù‡ Ø³Ù‡ Ø¨Ø®Ø´ ØªÙ‚Ø³ÛŒÙ… Ú©Ø±Ø¯ÛŒÙ…. Ø¯Ø± Ù…Ø±Ø­Ù„Ù‡ Ø¨Ø¹Ø¯ÛŒØŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø§ PyTorch Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….

## Û¶.Û³ Ø§ÛŒØ¬Ø§Ø¯ DataLoaderâ€Œ Ù‡Ø§

Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ Ø¯Ø§Ø¯Ù‡â€ŒØ¨Ø§Ø±Ù‡Ø§ÛŒ (DataLoader) Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² PyTorch Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ù…Ø´Ø§Ø¨Ù‡ Ø¢Ù† Ú†ÛŒØ²ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± ÙØµÙ„â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ Ù‡Ù†Ú¯Ø§Ù… Ú©Ø§Ø± Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯ÛŒÙ…. Ù¾ÛŒØ´â€ŒØªØ±ØŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **ØªÚ©Ù†ÛŒÚ© Ù¾Ù†Ø¬Ø±Ù‡â€ŒÛŒ Ù„ØºØ²Ø§Ù† (sliding window)**ØŒ Ù‚Ø·Ø¹Ø§Øª Ù…ØªÙ†ÛŒ Ù‡Ù…â€ŒØ§Ù†Ø¯Ø§Ø²Ù‡ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ù‡â€ŒØµÙˆØ±Øª batch Ø¯Ø±Ø¢ÙˆØ±Ø¯ÛŒÙ…. Ù‡Ø± Ù‚Ø·Ø¹Ù‡ Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ø±Ø¯.

Ø§Ù…Ø§ Ø§ÛŒÙ†â€ŒØ¨Ø§Ø± Ø¨Ø§ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø¨Ø§ **Ø·ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙØ§ÙˆØª** Ø³Ø±ÙˆÚ©Ø§Ø± Ø¯Ø§Ø±ÛŒÙ…. Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ batchÙ‡Ø§ÛŒÛŒ Ø§Ø² Ø§ÛŒÙ† Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ØŒ Ø¯Ùˆ Ú¯Ø²ÛŒÙ†Ù‡ Ø§ØµÙ„ÛŒ Ù¾ÛŒØ´â€ŒØ±Ùˆ Ø¯Ø§Ø±ÛŒÙ…:

Û±. **Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† Ù‡Ù…Ù‡â€ŒÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÛŒ Ú©ÙˆØªØ§Ù‡â€ŒØªØ±ÛŒÙ† Ù¾ÛŒØ§Ù…**

Û². **Ù¾ÙØ± Ú©Ø±Ø¯Ù† (pad) Ù‡Ù…Ù‡â€ŒÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ ØªØ§ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÛŒ Ø¨Ù„Ù†Ø¯ØªØ±ÛŒÙ† Ù¾ÛŒØ§Ù…**

Ú¯Ø²ÛŒÙ†Ù‡â€ŒÛŒ Ø§ÙˆÙ„ Ø§Ø² Ù†Ø¸Ø± Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒ Ø³Ø¨Ú©â€ŒØªØ± Ø§Ø³ØªØŒ Ø§Ù…Ø§ Ø§Ú¯Ø± Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ø¨Ù‡â€ŒØ·ÙˆØ± Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø§Ø² Ù…ØªÙˆØ³Ø· ÛŒØ§ Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ø·ÙˆÙ„ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ø¨Ø§Ø´Ù†Ø¯ØŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù‡Ù…ÛŒ Ø§Ø² Ø¨ÛŒÙ† Ø¨Ø±ÙˆØ¯ Ùˆ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ú©Ø§Ù‡Ø´ ÛŒØ§Ø¨Ø¯.

Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ Ù…Ø§ Ú¯Ø²ÛŒÙ†Ù‡â€ŒÛŒ Ø¯ÙˆÙ… Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:

> **Ù‡Ù…Ù‡â€ŒÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø±Ø§ ØªØ§ Ø§Ù†Ø¯Ø§Ø²Ù‡â€ŒÛŒ Ø¨Ù„Ù†Ø¯ØªØ±ÛŒÙ† Ù¾ÛŒØ§Ù… Ø¨Ø§ ØªÙˆÚ©Ù† Ù¾Ø±Ú©Ù†Ù†Ø¯Ù‡ (padding) Ù¾ÙØ± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….**

### Ø§Ù†ØªØ®Ø§Ø¨ ØªÙˆÚ©Ù† Ù¾Ø±Ú©Ù†Ù†Ø¯Ù‡ (padding token)

Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ú©Ø§Ø±ØŒ Ø§Ø² ØªÙˆÚ©Ù† ÙˆÛŒÚ˜Ù‡â€ŒÛŒ `<|endoftext|>` Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….  
Ø§Ù…Ø§ Ø¨Ù‡ Ø¬Ø§ÛŒ Ú†Ø³Ø¨Ø§Ù†Ø¯Ù† Ø§ÛŒÙ† Ø±Ø´ØªÙ‡â€ŒÛŒ Ù…ØªÙ†ÛŒ Ø¨Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ØŒ Ø´Ù†Ø§Ø³Ù‡â€ŒÛŒ ØªÙˆÚ©Ù† Ù…Ø¹Ø§Ø¯Ù„ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒâ€ŒØ´Ø¯Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….

Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø´Ù†Ø§Ø³Ù‡â€ŒÛŒ ØªÙˆÚ©Ù† ØµØ­ÛŒØ­ Ø§Ø³ØªØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø§Ø² ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± `tiktoken` Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ GPT-Û² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…:

```python
import tiktoken
tokenizer = tiktoken.get_encoding("gptÛ²")
print(tokenizer.encode("<|endoftext|>", allowed_special={"<|endoftext|>"}))
```

> Ø§ÛŒÙ† Ú©Ø¯ Ø®Ø±ÙˆØ¬ÛŒ `[ÛµÛ°Û²ÛµÛ¶]` Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ú©Ù‡ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ø´Ù†Ø§Ø³Ù‡â€ŒÛŒ ØªÙˆÚ©Ù† `<|endoftext|>` Ø¨Ø±Ø§Ø¨Ø± ÛµÛ°Û²ÛµÛ¶ Ø§Ø³Øª.

---

### ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆØ±ÙˆØ¯ÛŒ (Ø´Ú©Ù„ Û¶.Û¶)

Û±. Ù‡Ø± Ù¾ÛŒØ§Ù… Ù…ØªÙ†ÛŒ Ø¨Ù‡ ÛŒÚ© Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ø§Ø² Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙˆÚ©Ù† ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

Û². Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÛŒÚ©Ø³Ø§Ù† Ø¨ÙˆØ¯Ù† Ø·ÙˆÙ„ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ØŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ø¨Ø§ ØªÙˆÚ©Ù† ÛµÛ°Û²ÛµÛ¶ Ù¾ÙØ± Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ ØªØ§ Ø¨Ø§ Ø·ÙˆÙ„ Ø¨Ù„Ù†Ø¯ØªØ±ÛŒÙ† Ù¾ÛŒØ§Ù… Ø¨Ø±Ø§Ø¨Ø± Ø´ÙˆÙ†Ø¯.

---

### ØªØ¹Ø±ÛŒÙ Ú©Ù„Ø§Ø³ Dataset Ø¨Ø±Ø§ÛŒ PyTorch

```python
import torch
from torch.utils.data import Dataset

class SpamDataset(Dataset):
    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=ÛµÛ°Û²ÛµÛ¶):
        self.data = pd.read_csv(csv_file)  #Û±

        self.encoded_texts = [
            tokenizer.encode(text) for text in self.data["Text"]
        ]

        if max_length is None:
            self.max_length = self._longest_encoded_length()
        else:
            self.max_length = max_length
            self.encoded_texts = [
                encoded_text[:self.max_length]
                for encoded_text in self.encoded_texts
            ]  #Û²

        self.encoded_texts = [
            encoded_text + [pad_token_id] *
            (self.max_length - len(encoded_text))
            for encoded_text in self.encoded_texts
        ]  #Û³

    def __getitem__(self, index):
        encoded = self.encoded_texts[index]
        label = self.data.iloc[index]["Label"]
        return (
            torch.tensor(encoded, dtype=torch.long),
            torch.tensor(label, dtype=torch.long)
        )

    def __len__(self):
        return len(self.data)

    def _longest_encoded_length(self):
        max_length = Û°
        for encoded_text in self.encoded_texts:
            if len(encoded_text) > max_length:
                max_length = len(encoded_text)
        return max_length
```

**ØªÙˆØ¶ÛŒØ­Ø§Øª:**

- `#Û±`: Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø±Ø§ tokenâ€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
- `#Û²`: Ø§Ú¯Ø± Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒØ§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ± Ø§Ø² `max_length` Ø¨Ø§Ø´Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ú©ÙˆØªØ§Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
- `#Û³`: Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ø±Ø§ Ø¨Ø§ ØªÙˆÚ©Ù† Ù¾Ø±Ú©Ù†Ù†Ø¯Ù‡ ØªÚ©Ù…ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

---

### Ø§ÛŒØ¬Ø§Ø¯ Ø´ÛŒØ¡ Dataset Ø¨Ø±Ø§ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¢Ù…ÙˆØ²Ø´

```python
train_dataset = SpamDataset(
    csv_file="train.csv",
    max_length=None,
    tokenizer=tokenizer
)
```

Ø§Ú¯Ø± Ø¨Ø®ÙˆØ§Ù‡ÛŒØ¯ Ø·ÙˆÙ„ Ø¨Ù„Ù†Ø¯ØªØ±ÛŒÙ† Ù¾ÛŒØ§Ù… Ø±Ø§ Ø¨Ø¨ÛŒÙ†ÛŒØ¯:

```python
print(train_dataset.max_length)
```

> Ø®Ø±ÙˆØ¬ÛŒ: `Û±Û²Û°`  
> Ø§ÛŒÙ† ÛŒØ¹Ù†ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±ÛŒÙ† Ù¾ÛŒØ§Ù… Ø­Ø¯Ø§Ú©Ø«Ø± Û±Û²Û° ØªÙˆÚ©Ù† Ø¯Ø§Ø±Ø¯ â€” Ø¹Ø¯Ø¯ÛŒ Ù…Ù†Ø·Ù‚ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ù…Ú©â€ŒÙ‡Ø§.

Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒØªØ§Ù† Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ± Ù‡Ø³ØªÙ†Ø¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ù‚Ø¯Ø§Ø± `max_length=Û±Û°Û²Û´` Ø±Ø§ ØªØ¹ÛŒÛŒÙ† Ú©Ù†ÛŒØ¯ ØªØ§ Ø§Ø² Ø­Ø¯ Ù…Ø¬Ø§Ø² Ù…Ø¯Ù„ (context length) ÙØ±Ø§ØªØ± Ù†Ø±ÙˆØ¯.

---

### Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ Ø¢Ø²Ù…ÙˆÙ†

Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒØŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ Ø¢Ø²Ù…ÙˆÙ† Ø±Ø§ Ø¨Ø§ Ù‡Ù…Ø§Ù† Ø·ÙˆÙ„ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯ØŒ pad Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:

```python
val_dataset = SpamDataset(
    csv_file="validation.csv",
    max_length=train_dataset.max_length,
    tokenizer=tokenizer
)

test_dataset = SpamDataset(
    csv_file="test.csv",
    max_length=train_dataset.max_length,
    tokenizer=tokenizer
)
```

> Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ/Ø¢Ø²Ù…ÙˆÙ† Ø§Ø² Ø­Ø¯ Ù…Ø¬Ø§Ø² ÙØ±Ø§ØªØ± Ø±ÙØªÙ†Ø¯ØŒ ØªÙˆØ³Ø· `encoded_text[:self.max_length]` Ú©ÙˆØªØ§Ù‡ Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø´Ø¯.

---

### ØªÙ…Ø±ÛŒÙ† Û¶.Û±: Ø§ÙØ²Ø§ÛŒØ´ Ø·ÙˆÙ„ Ú©Ø§Ù†ØªÚ©Ø³Øª

Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ ØªØ§ Ø¨ÛŒØ´ÛŒÙ†Ù‡â€ŒÛŒ ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù† Ù‚Ø§Ø¨Ù„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ ØªÙˆØ³Ø· Ù…Ø¯Ù„ (Ù…Ø«Ù„Ø§Ù‹ Û±Û°Û²Û´) Ù¾ÙØ± Ú©Ù†ÛŒØ¯ Ùˆ Ø¨Ø¨ÛŒÙ†ÛŒØ¯ Ø§ÛŒÙ† ØªØºÛŒÛŒØ± Ú†Ù‡ Ø§Ø«Ø±ÛŒ Ø¨Ø± Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø¯Ø§Ø±Ø¯.

---

### Ø§ÛŒØ¬Ø§Ø¯ DataLoaderÙ‡Ø§

Ø­Ø§Ù„Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ØŒ Ø¯Ø§Ø¯Ù‡â€ŒØ¨Ø§Ø±Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ØŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ùˆ Ø¢Ø²Ù…ÙˆÙ† Ø¨Ø³Ø§Ø²ÛŒÙ…:

```python
from torch.utils.data import DataLoader

num_workers = Û°  # Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ø§ØºÙ„Ø¨ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§
batch_size = Û¸
torch.manual_seed(Û±Û²Û³)

train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=num_workers,
    drop_last=True,
)

val_loader = DataLoader(
    dataset=val_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    drop_last=False,
)

test_loader = DataLoader(
    dataset=test_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    drop_last=False,
)
```

---

### Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¨Ø¹Ø§Ø¯ Ø®Ø±ÙˆØ¬ÛŒ ÛŒÚ© batch

```python
for input_batch, target_batch in train_loader:
    pass
print("Input batch dimensions:", input_batch.shape)
print("Label batch dimensions", target_batch.shape)
```

Ø®Ø±ÙˆØ¬ÛŒ:

```
Input batch dimensions: torch.Size([Û¸, Û±Û²Û°])
Label batch dimensions torch.Size([Û¸])
```

Ø§ÛŒÙ† Ø®Ø±ÙˆØ¬ÛŒ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ù‡Ø± batch Ø´Ø§Ù…Ù„ Û¸ Ù¾ÛŒØ§Ù… Ù…ØªÙ†ÛŒ (Ù‡Ø±Ú©Ø¯Ø§Ù… Ø¨Ø§ Û±Û²Û° ØªÙˆÚ©Ù†) Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø±Ú†Ø³Ø¨ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù‡Ø± Ù¾ÛŒØ§Ù… Ø§Ø³Øª.

---

### Ø´Ù…Ø§Ø±Ø´ ØªØ¹Ø¯Ø§Ø¯ batchÙ‡Ø§ Ø¯Ø± Ù‡Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡

```python
print(f"{len(train_loader)} training batches")
print(f"{len(val_loader)} validation batches")
print(f"{len(test_loader)} test batches")
```

Ø®Ø±ÙˆØ¬ÛŒ:

```
Û±Û³Û° training batches
Û±Û¹ validation batches
Û³Û¸ test batches
```

## Û¶.Û´ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ø¯Ù„ Ø¨Ø§ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡

Ø§Ú©Ù†ÙˆÙ† Ú©Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒÙ…ØŒ Ø¨Ø§ÛŒØ¯ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø±Ø§ÛŒ **Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… (fine-tuning)** Ø¯Ø± ÙˆØ¸ÛŒÙÙ‡â€ŒÛŒ **Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾Ù…** Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ù†ÛŒÙ…. Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ØŒ Ú©Ø§Ø± Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡â€ŒÛŒ Ù…Ø¯Ù„ÛŒ Ø¢ØºØ§Ø² Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø§Ø² Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³ØªØŒ Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯Ø± Ø´Ú©Ù„ Û¶.Û¸ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª.

> ğŸ”¹ **Ø´Ú©Ù„ Û¶.Û¸**: ÙØ±Ø¢ÛŒÙ†Ø¯ Ø³Ù‡â€ŒÙ…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ. Ù¾Ø³ Ø§Ø² Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„ (Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡)ØŒ Ø§Ú©Ù†ÙˆÙ† ÙˆØ§Ø±Ø¯ Ù…Ø±Ø­Ù„Ù‡ Ø¯ÙˆÙ… (Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ø¯Ù„) Ù…ÛŒâ€ŒØ´ÙˆÛŒÙ….

---

### ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾Ø§ÛŒÙ‡ Ù…Ø¯Ù„

Ø§Ø¨ØªØ¯Ø§ØŒ Ø§Ø² Ù‡Ù…Ø§Ù† Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ù‡Ù†Ú¯Ø§Ù… Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨ Ø¨Ù‡ Ú©Ø§Ø± Ø±ÙØªÙ‡ Ø¨ÙˆØ¯:

```python
CHOOSE_MODEL = "gptÛ²-small (Û±Û²Û´M)"
INPUT_PROMPT = "Every effort moves"
BASE_CONFIG = {
    "vocab_size": ÛµÛ°Û²ÛµÛ·,          #Û±
    "context_length": Û±Û°Û²Û´,       #Û²
    "drop_rate": Û°.Û°,             #Û³
    "qkv_bias": True              #Û´
}
model_configs = {
    "gptÛ²-small (Û±Û²Û´M)": {"emb_dim": Û·Û¶Û¸, "n_layers": Û±Û², "n_heads": Û±Û²},
    "gptÛ²-medium (Û³ÛµÛµM)": {"emb_dim": Û±Û°Û²Û´, "n_layers": Û²Û´, "n_heads": Û±Û¶},
    "gptÛ²-large (Û·Û·Û´M)": {"emb_dim": Û±Û²Û¸Û°, "n_layers": Û³Û¶, "n_heads": Û²Û°},
    "gptÛ²-xl (Û±ÛµÛµÛ¸M)": {"emb_dim": Û±Û¶Û°Û°, "n_layers": Û´Û¸, "n_heads": Û²Ûµ},
}
BASE_CONFIG.update(model_configs[CHOOSE_MODEL])
```

**ØªÙˆØ¶ÛŒØ­Ø§Øª Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ:**

- `#Û±`: Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù† (vocabulary) Ù…Ø¯Ù„ (ÛµÛ°Ù¬Û²ÛµÛ· ØªÙˆÚ©Ù†ØŒ Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ø§ GPT-Û²).
- `#Û²`: Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ ÙˆØ±ÙˆØ¯ÛŒ (context length)ØŒ Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§ Û±Û°Û²Û´ ØªÙˆÚ©Ù†.
- `#Û³`: Ù†Ø±Ø® Dropout Ø±ÙˆÛŒ ØµÙØ± ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡.
- `#Û´`: Ø§Ø¹Ù…Ø§Ù„ bias Ø¯Ø± Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Query-Key-Value.

---

### Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡

Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ØŒ Ø§Ø² ØªÙˆØ§Ø¨Ø¹ Ùˆ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± ÙØµÙ„ Ûµ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø±Ø¯ÛŒÙ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:

```python
from gpt_download import download_and_load_gptÛ²
from chapterÛ°Ûµ import GPTModel, load_weights_into_gpt

model_size = CHOOSE_MODEL.split(" ")[-Û±].lstrip("(").rstrip(")")
settings, params = download_and_load_gptÛ²(
    model_size=model_size, models_dir="gptÛ²"
)

model = GPTModel(BASE_CONFIG)
load_weights_into_gpt(model, params)
model.eval()
```

> Ø§ÛŒÙ† Ú©Ø¯ØŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„ GPT-Û² Ø±Ø§ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒâ€ŒØ´Ø¯Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯ Ùˆ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¯Ù„ `GPTModel` Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

---

### Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø±Ø³ØªÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§

Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¨Ù‡â€ŒØ¯Ø±Ø³ØªÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ø§Ø² ØªØ§Ø¨Ø¹ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† Ø³Ø§Ø¯Ù‡ Ú©Ù‡ Ø¯Ø± ÙØµÙ„â€ŒÙ‡Ø§ÛŒ Û´ Ùˆ Ûµ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø±Ø¯Ù‡ Ø¨ÙˆØ¯ÛŒÙ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:

```python
from chapterÛ°Û´ import generate_text_simple
from chapterÛ°Ûµ import text_to_token_ids, token_ids_to_text

text_Û± = "Every effort moves you"
token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(text_Û±, tokenizer),
    max_new_tokens=Û±Ûµ,
    context_size=BASE_CONFIG["context_length"]
)
print(token_ids_to_text(token_ids, tokenizer))
```

**Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù…ÙˆÙ†Ù‡:**

```
Every effort moves you forward.
The first step is to understand the importance of your work
```

> Ø§ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ù…Ø¯Ù„ Ø¨Ù‡â€ŒØ¯Ø±Ø³ØªÛŒ Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ù‚Ø§Ø¯Ø± Ø¨Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ†ÛŒ Ù…Ù†Ø³Ø¬Ù… Ø§Ø³Øª. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¨Ù‡â€ŒØ¯Ø±Ø³ØªÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.

---

### Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ù…Ø¯Ù„ Ø¯Ø± Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³Ù¾Ù… (Ù‚Ø¨Ù„ Ø§Ø² Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ…)

Ù¾ÛŒØ´ Ø§Ø² Ø´Ø±ÙˆØ¹ ÙØ±Ø¢ÛŒÙ†Ø¯ Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ…ØŒ Ø¨ÛŒØ§ÛŒÛŒØ¯ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒÙ… Ø¢ÛŒØ§ Ù…Ø¯Ù„ Ø¯Ø± Ø­Ø§Ù„Øª ÙØ¹Ù„ÛŒ (ÙÙ‚Ø· Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ØŒ Ø¨Ø¯ÙˆÙ† Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ…) Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾Ù… Ø±Ø§ ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯ ÛŒØ§ Ø®ÛŒØ±.

```python
text_Û² = (
    "Is the following text 'spam'? Answer with 'yes' or 'no':"
    " 'You are a winner you have been specially"
    " selected to receive $Û±Û°Û°Û° cash or a $Û²Û°Û°Û° award.'"
)

token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(text_Û², tokenizer),
    max_new_tokens=Û²Û³,
    context_size=BASE_CONFIG["context_length"]
)
print(token_ids_to_text(token_ids, tokenizer))
```

**Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„:**

```
Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner
you have been specially selected to receive $Û±Û°Û°Û° cash
or a $Û²Û°Û°Û° award.'
The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner
```

> Ø¨Ø± Ø§Ø³Ø§Ø³ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§Ù„Ø§ØŒ Ù…Ø¯Ù„ Ù‡Ù†ÙˆØ² Ù‚Ø§Ø¯Ø± Ù†ÛŒØ³Øª Ø¨Ù‡â€ŒØ·ÙˆØ± Ù…Ø¹Ù†Ø§Ø¯Ø§Ø± Ø¨Ù‡ Ø¯Ø³ØªÙˆØ± Ù¾Ø§Ø³Ø® Ø¯Ù‡Ø¯. Ø¨Ù‡ Ø¹Ø¨Ø§Ø±ØªÛŒØŒ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù¾Ø§Ø³Ø® Ø¨Ù„Ù‡ ÛŒØ§ Ø®ÛŒØ± Ø¨Ø¯Ù‡Ø¯ Ùˆ ØªÙ†Ù‡Ø§ ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ ØªÚ©Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

Ø§ÛŒÙ† Ú©Ø§Ù…Ù„Ø§Ù‹ **Ø§Ù†ØªØ¸Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯**ØŒ Ø²ÛŒØ±Ø§ Ø§ÛŒÙ† Ù…Ø¯Ù„ ÙÙ‚Ø· **Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´** Ø¯ÛŒØ¯Ù‡ Ùˆ Ù‡Ù†ÙˆØ² **Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„â€ŒÙ‡Ø§ ÛŒØ§ ÙˆØ¸Ø§ÛŒÙ Ø®Ø§Øµ (Ù…Ø§Ù†Ù†Ø¯ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³Ù¾Ù…)** Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.

### Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ

Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ Ú¯Ø§Ù… Ø¨Ø¹Ø¯ÛŒ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø±Ø§ÛŒ **Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ø¯Ø± ÙˆØ¸ÛŒÙÙ‡â€ŒÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³Ù¾Ù…** Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ù†ÛŒÙ…. Ø¯Ø± Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ØŒ ÛŒÚ© Ù„Ø§ÛŒÙ‡â€ŒÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡ Ù…Ø¯Ù„ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ Ø¨ØªÙˆØ§Ù†Ø¯ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Û° (ham) ÛŒØ§ Û± (spam) Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù†Ø¯.

## Û¶.Ûµ Ø§ÙØ²ÙˆØ¯Ù† ÛŒÚ© Ø³Ø± Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡ Ù…Ø¯Ù„

Ø§Ú©Ù†ÙˆÙ† Ø¨Ø§ÛŒØ¯ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† Ø¨Ø²Ø±Ú¯ (LLM) Ø§Ø² Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ Ø±Ø§ Ø¨Ø±Ø§ÛŒ **Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ø¯Ø± ÙˆØ¸ÛŒÙÙ‡â€ŒÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ** Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ù†ÛŒÙ…. Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ú©Ø§Ø±ØŒ Ù„Ø§Ø²Ù… Ø§Ø³Øª Ú©Ù‡ **Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ Ø§ØµÙ„ÛŒ Ù…Ø¯Ù„** Ø±Ø§ Ø¨Ø§ ÛŒÚ© Ù„Ø§ÛŒÙ‡ Ø¬Ø¯ÛŒØ¯ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒÙ…Ø› Ù„Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ú©Ù‡ Ø¨Ù‡â€ŒØ¬Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒØŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø±Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù†Ø¯.

---

### ğŸ¯ Ù‡Ø¯Ù: ØªØ¨Ø¯ÛŒÙ„ Ù…Ø¯Ù„ GPT Ø§Ø² Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø¨Ù‡ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³Ù¾Ù…

Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯Ø± Ø´Ú©Ù„ Û¶.Û¹ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª:

> ğŸ”¹ Ù…Ø¯Ù„ Ø§ØµÙ„ÛŒ GPT-Û² Ø®Ø±ÙˆØ¬ÛŒâ€ŒØ§ÛŒ Ø¨Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡Ù” ÙˆØ§Ú˜Ú¯Ø§Ù† (ÛµÛ°Ù¬Û²ÛµÛ· ØªÙˆÚ©Ù†) ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.  
> ğŸ”¹ Ø¯Ø± Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³Ù¾Ù…ØŒ Ù…Ø§ ÙÙ‚Ø· Ø¨Ù‡ Ø¯Ùˆ Ú©Ù„Ø§Ø³ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±ÛŒÙ…:  
> â€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€ŒğŸ”¸ Û° = Â«not spamÂ»  
> â€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€Œâ€ŒğŸ”¸ Û± = Â«spamÂ»  
> ğŸ”¹ Ù¾Ø³ØŒ Ù„Ø§ÛŒÙ‡â€ŒÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ø±Ø§ Ø¨Ù‡ ÛŒÚ© Ù„Ø§ÛŒÙ‡ Ø®Ø·ÛŒ (Linear) Ø¨Ø§ **Û² Ù†ÙˆØ±ÙˆÙ† Ø®Ø±ÙˆØ¬ÛŒ** ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ….

---

### Ú†Ø±Ø§ Û² Ù†ÙˆØ±ÙˆÙ† Ø®Ø±ÙˆØ¬ÛŒ Ùˆ Ù†Ù‡ ÙÙ‚Ø· ÛŒÚ©ØŸ

Ø§Ø² Ø¢Ù†Ø¬Ø§ Ú©Ù‡ Ù…Ø³Ø¦Ù„Ù‡â€ŒÛŒ Ù…Ø§ **Ø¯ÙˆØ¯ÙˆÛŒÛŒ (Ø¨Ø§ÛŒÙ†Ø±ÛŒ)** Ø§Ø³ØªØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø³ØªÛŒÙ… ÙÙ‚Ø· ÛŒÚ© Ù†ÙˆØ¯ Ø®Ø±ÙˆØ¬ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ… Ùˆ Ù…Ø«Ù„Ø§Ù‹ Ø§Ø² ØªØ§Ø¨Ø¹ Ø¢Ø³ØªØ§Ù†Ù‡â€ŒØ§ÛŒ (Ù…Ø«Ù„ sigmoid) Ø¨Ù‡Ø±Ù‡ Ø¨Ø¨Ø±ÛŒÙ…. Ø§Ù…Ø§ Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ **Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø¹Ù…ÙˆÙ…ÛŒâ€ŒØªØ±ÛŒ** Ø§ØªØ®Ø§Ø° Ø´Ø¯Ù‡ Ø§Ø³Øª:

- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **Û² Ù†ÙˆØ±ÙˆÙ† Ø®Ø±ÙˆØ¬ÛŒ** Ø¨Ø§ **ØªØ§Ø¨Ø¹ loss `CrossEntropyLoss`** Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ú†Ù†Ø¯Ú©Ù„Ø§Ø³Ù‡ Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡.
- Ø§ÛŒÙ† Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø¨Ù‡â€ŒØ±Ø§Ø­ØªÛŒ Ø¨ØªÙˆØ§Ù† Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ù‡ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ú©Ù„Ø§Ø³Ù‡ Ù†ÛŒØ² ØªØ¹Ù…ÛŒÙ… Ø¯Ø§Ø¯.

---

### ğŸ” Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ù‚Ø¨Ù„ Ø§Ø² ØªØºÛŒÛŒØ±

Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø¹Ù…Ø§Ù„ ØªØºÛŒÛŒØ±Ø§ØªØŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ ÙØ¹Ù„ÛŒ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ Ø¯Ø³ØªÙˆØ± `print(model)` Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ù‡ Ø´Ú©Ù„ Ø²ÛŒØ± Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯:

```
GPTModel(
  (tok_emb): Embedding(ÛµÛ°Û²ÛµÛ·, Û·Û¶Û¸)
  (pos_emb): Embedding(Û±Û°Û²Û´, Û·Û¶Û¸)
  ...
  (trf_blocks): Sequential(
    (Û±Û±): TransformerBlock(...)
  )
  (final_norm): LayerNorm()
  (out_head): Linear(in_features=Û·Û¶Û¸, out_features=ÛµÛ°Û²ÛµÛ·)
)
```

Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØ¨ÛŒÙ†ÛŒÙ… Ú©Ù‡ Ù„Ø§ÛŒÙ‡â€ŒÛŒ Ø®Ø±ÙˆØ¬ÛŒ (`out_head`) ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Û·Û¶Û¸ ÙˆÛŒÚ˜Ú¯ÛŒ Ø±Ø§ Ø¨Ù‡ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ ÛµÛ°Ù¬Û²ÛµÛ· ÙˆÛŒÚ˜Ú¯ÛŒ (ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§) Ù†Ú¯Ø§Ø´Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø­Ø§Ù„ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø¢Ù† Ø±Ø§ Ø¨Ù‡ **Û² ÙˆÛŒÚ˜Ú¯ÛŒ (Ú©Ù„Ø§Ø³)** Ú©Ø§Ù‡Ø´ Ø¯Ù‡ÛŒÙ….

---

### ğŸ§  Ù…Ø±Ø§Ø­Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ

### Û±. ÙØ±ÛŒØ² Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… ÙˆØ²Ù†â€ŒÙ‡Ø§ (ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† Ø¢Ù…ÙˆØ²Ø´)

Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ØŒ ØªÙ…Ø§Ù… ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„ Ø±Ø§ **ÙØ±ÛŒØ²** Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ ÙÙ‚Ø· Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯Ù†Ø¸Ø±Ù…Ø§Ù† Ø¯Ø± Ø¢Ù…ÙˆØ²Ø´ ØªØºÛŒÛŒØ± ÛŒØ§Ø¨Ù†Ø¯:

```python
for param in model.parameters():
    param.requires_grad = False
```

---

### Û². Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§ Ù„Ø§ÛŒÙ‡ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¬Ø¯ÛŒØ¯

```python
import torch

torch.manual_seed(Û±Û²Û³)
num_classes = Û²

model.out_head = torch.nn.Linear(
    in_features=BASE_CONFIG["emb_dim"],  # Û·Û¶Û¸ for GPT-Û² small
    out_features=num_classes
)
```

Ù†Ú©ØªÙ‡:

- `BASE_CONFIG["emb_dim"]` Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§ Û·Û¶Û¸ Ø¯Ø± Ù…Ø¯Ù„ GPT-Û² Small Ø§Ø³Øª.
- Ø§ÛŒÙ† Ú©Ø¯ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯â€ŒØªØ± Ù†ÛŒØ² Ø³Ø§Ø²Ú¯Ø§Ø± Ø§Ø³Øª.

---

### Û³. Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´ Ú©Ø±Ø¯Ù† Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ø¯Ù„

Ù‡Ù…Ú†Ù†ÛŒÙ† ØªØµÙ…ÛŒÙ… Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ… **Ø¨Ù‡â€ŒØ¬Ø² Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒØŒ Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù„Ø§Ú© ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø± Ùˆ Ù„Ø§ÛŒÙ‡ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ** Ù†ÛŒØ² Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§Ø´Ù†Ø¯:

```python
for param in model.trf_blocks[-Û±].parameters():
    param.requires_grad = True

for param in model.final_norm.parameters():
    param.requires_grad = True
```

> ğŸ”¸ Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù…Ø¯Ù„ Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± Ø®Ø±ÙˆØ¬ÛŒØŒ Ø§Ø² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø·Ø­ Ø¨Ø§Ù„Ø§ Ù†ÛŒØ² Ø¯Ø± ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡Ø±Ù‡ Ø¨Ú¯ÛŒØ±Ø¯.  
> ğŸ”¸ Ø¨Ù‚ÛŒÙ‡â€ŒÛŒ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ù‡Ù…Ú†Ù†Ø§Ù† Ø«Ø§Ø¨Øª (ÙØ±ÛŒØ² Ø´Ø¯Ù‡) Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ù†Ø¯.

---

### ğŸ§ª Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø­ÙˆÙ‡â€ŒÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø§ØµÙ„Ø§Ø­â€ŒØ´Ø¯Ù‡

### ÙˆØ±ÙˆØ¯ÛŒ Ù†Ù…ÙˆÙ†Ù‡:

```python
inputs = tokenizer.encode("Do you have time")
inputs = torch.tensor(inputs).unsqueeze(Û°)  # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ø¹Ø¯ batch
print("Inputs:", inputs)
print("Inputs dimensions:", inputs.shape)
```

Ø®Ø±ÙˆØ¬ÛŒ:

```
Inputs: tensor([[ÛµÛ²Û±Û±,  Û³Û´Ûµ,  Û´Û²Û³,  Û¶Û´Û°]])
Inputs dimensions: torch.Size([Û±, Û´])
```

---

### Ø¹Ø¨ÙˆØ± Ø¯Ø§Ø¯Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø§Ø² Ù…Ø¯Ù„:

```python
with torch.no_grad():
    outputs = model(inputs)
print("Outputs:\n", outputs)
print("Outputs dimensions:", outputs.shape)
```

Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù…ÙˆÙ†Ù‡:

```
Outputs:
 tensor([[[-Û±.ÛµÛ¸ÛµÛ´,  Û°.Û¹Û¹Û°Û´],
          [-Û³.Û·Û²Û³Ûµ,  Û·.Û´ÛµÛ´Û¸],
          [-Û².Û²Û¶Û¶Û±,  Û¶.Û¶Û°Û´Û¹],
          [-Û³.ÛµÛ¹Û¸Û³,  Û³.Û¹Û¹Û°Û²]]])
Outputs dimensions: torch.Size([Û±, Û´, Û²])
```

> Ø§Ú©Ù†ÙˆÙ† Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ Ø¨Ù‡â€ŒØ¬Ø§ÛŒ `[Û±, Û´, ÛµÛ°Û²ÛµÛ·]` Ø¨Ø±Ø§Ø¨Ø± Ø¨Ø§ `[Û±, Û´, Û²]` Ø§Ø³ØªØŒ Ø²ÛŒØ±Ø§ ÙÙ‚Ø· Ø¯Ùˆ Ú©Ù„Ø§Ø³ Ø¯Ø§Ø±ÛŒÙ….

---

### Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¢Ø®Ø±ÛŒÙ† ØªÙˆÚ©Ù†

Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± **self-attention Ø¹Ù„Ù‘ÛŒ (causal)**ØŒ Ù‡Ø± ØªÙˆÚ©Ù† ÙÙ‚Ø· Ø¨Ù‡ Ø®ÙˆØ¯Ø´ Ùˆ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¯Ø§Ø±Ø¯. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†:

âœ… **Ø¢Ø®Ø±ÛŒÙ† ØªÙˆÚ©Ù†** ØªÙ†Ù‡Ø§ ØªÙˆÚ©Ù†ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÙ…Ø§Ù… ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø¯Ø§Ø±Ø¯.  
â›” Ù¾Ø³ Ø¨Ø±Ø±Ø³ÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ ÛŒØ§ Ù…ÛŒØ§Ù†ÛŒ Ø¨ÛŒâ€ŒÙ…Ø¹Ù†ÛŒ Ø§Ø³Øª Ú†ÙˆÙ† Ø¯ÛŒØ¯ Ú©Ø§Ù…Ù„ Ù†Ø¯Ø§Ø±Ù†Ø¯.

Ú©Ø¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¢Ø®Ø±ÛŒÙ† Ø®Ø±ÙˆØ¬ÛŒ:

```python
print("Last output token:", outputs[:, -Û±, :])
```

Ø®Ø±ÙˆØ¬ÛŒ:

```
Last output token: tensor([[-Û³.ÛµÛ¹Û¸Û³,  Û³.Û¹Û¹Û°Û²]])
```

---

### ğŸ“˜ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ

Ù…Ø¯Ù„ Ø§Ú©Ù†ÙˆÙ† Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª ØªØ§ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ ÛŒÚ© Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒØŒ **Ø§Ø³Ù¾Ù…** Ø§Ø³Øª ÛŒØ§ **Ù†Ù‡**. Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡:

- Ø§Ø² **ØªØ§Ø¨Ø¹ loss Ù…Ø«Ù„ `CrossEntropyLoss`** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
- ÙÙ‚Ø· Ø§Ø² **Ø®Ø±ÙˆØ¬ÛŒ Ø¢Ø®Ø±ÛŒÙ† ØªÙˆÚ©Ù†** Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ loss Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

---

## ğŸ“Œ ØªÙ…Ø±ÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ

ğŸ”¶ **ØªÙ…Ø±ÛŒÙ† Û¶.Û²**: Ø¨Ù‡â€ŒØ¬Ø§ÛŒ ÙÙ‚Ø· Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù† Ø¨Ù„Ø§Ú© Ù†Ù‡Ø§ÛŒÛŒØŒ Ú©Ù„ Ù…Ø¯Ù„ Ø±Ø§ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ ØªØ£Ø«ÛŒØ± Ø¢Ù† Ø±Ø§ Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.

ğŸ”· **ØªÙ…Ø±ÛŒÙ† Û¶.Û³**: Ø¨Ù‡â€ŒØ¬Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¢Ø®Ø±ÛŒÙ† ØªÙˆÚ©Ù†ØŒ Ø§Ø² Ø§ÙˆÙ„ÛŒÙ† ØªÙˆÚ©Ù† Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù†Ù…Ø§ÛŒÛŒØ¯.

## Û¶.Û¶ â€“ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®Ø·Ø§ (Loss) Ùˆ Ø¯Ù‚Øª (Accuracy) Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ

## ğŸ¯ Ù‡Ø¯Ù Ø§ÛŒÙ† Ø¨Ø®Ø´:

Ù…Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ù…Ø¯Ù„ Ø±Ø§ Ù‚Ø¨Ù„ØŒ Ø­ÛŒÙ†ØŒ Ùˆ Ø¨Ø¹Ø¯ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†ÛŒÙ… ØªØ§ Ø¨Ø¨ÛŒÙ†ÛŒÙ… Ú†Ù‚Ø¯Ø± Ø®ÙˆØ¨ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾Ù… Ø±Ø§ ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯.  
Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ú©Ø§Ø±ØŒ Ø¯Ùˆ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ ØªØ¹Ø±ÛŒÙ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:

Û±. ØªØ§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚Øª (Accuracy) Ù…Ø¯Ù„

Û². ØªØ§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ§Ø¨Ø¹ Ø®Ø·Ø§ (Loss)

---

### âœ… Û±. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø±Ú†Ø³Ø¨ Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„

Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ Ù…Ø§ØŒ Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ø¯ÛŒØ¯ÛŒÙ…ØŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆØ±ÙˆØ¯ÛŒ ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± Ø¯Ùˆâ€ŒØ¨Ø¹Ø¯ÛŒ Ù…Ø§Ù†Ù†Ø¯ Ø²ÛŒØ± ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:

```python
Last output token: tensor([[-Û³.ÛµÛ¹Û¸Û³,  Û³.Û¹Û¹Û°Û²]])
```

Ø§ÛŒÙ† ÛŒØ¹Ù†ÛŒ Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³ "not spam" Ø§Ù…ØªÛŒØ§Ø² `-Û³.ÛµÛ¹` Ùˆ Ø¨Ø±Ø§ÛŒ "spam" Ø§Ù…ØªÛŒØ§Ø² `Û³.Û¹Û¹` Ø¯Ø§Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ Ø¨Ù‡â€ŒØ¯Ø³Øª Ø¢ÙˆØ±Ø¯Ù† Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ù‡Ø§ÛŒÛŒØŒ Ø§Ø² `argmax` Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…:

```python
logits = outputs[:, -Û±, :]
label = torch.argmax(logits)
print("Class label:", label.item())  # Ø®Ø±ÙˆØ¬ÛŒ: Û± (ÛŒØ¹Ù†ÛŒ Ø§Ø³Ù¾Ù…)
```

> ğŸ”¸ Ø§Ø² `softmax` Ù‡Ù… Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ØŒ ÙˆÙ„ÛŒ Ú†ÙˆÙ† `argmax` ÙÙ‚Ø· Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ø¯ØŒ Ù†ÛŒØ§Ø²ÛŒ Ù†ÛŒØ³Øª.

---

### âœ… Û². ØªØ§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚Øª (Accuracy)

Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø±ÙˆÛŒ ÛŒÚ© Ø¯ÛŒØªØ§Ø³ØªØŒ Ø¨Ø§ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ ÙˆØ§Ù‚Ø¹ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒÙ…:

```python
def calc_accuracy_loader(data_loader, model, device, num_batches=None):
    model.eval()  # Ù…Ø¯Ù„ Ø±Ø§ Ø¯Ø± Ø­Ø§Ù„Øª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ (Ø¨Ø¯ÙˆÙ† dropout)
    correct_predictions, num_examples = Û°, Û°

    # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ batchÙ‡Ø§ (Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª)
    if num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))

    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            input_batch = input_batch.to(device)
            target_batch = target_batch.to(device)
            with torch.no_grad():
                logits = model(input_batch)[:, -Û±, :]  # ÙÙ‚Ø· Ø®Ø±ÙˆØ¬ÛŒ Ø¢Ø®Ø±
            predicted_labels = torch.argmax(logits, dim=-Û±)
            num_examples += predicted_labels.shape[Û°]
            correct_predictions += (
                (predicted_labels == target_batch).sum().item()
            )
        else:
            break
    return correct_predictions / num_examples
```

---

## âœ… Û³. Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ø¯Ù‚Øª

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

torch.manual_seed(Û±Û²Û³)

train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=Û±Û°)
val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=Û±Û°)
test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=Û±Û°)

print(f"Training accuracy: {train_accuracy*Û±Û°Û°:.Û²f}%")
print(f"Validation accuracy: {val_accuracy*Û±Û°Û°:.Û²f}%")
print(f"Test accuracy: {test_accuracy*Û±Û°Û°:.Û²f}%")
```

### ğŸ“Œ Ø®Ø±ÙˆØ¬ÛŒ:

```
Training accuracy: Û´Û¶.Û²Ûµ%
Validation accuracy: Û´Ûµ.Û°Û°%
Test accuracy: Û´Û¸.Û·Ûµ%
```

> ğŸ”¸ Ù…Ø¯Ù„ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ù…Ø«Ù„ ÛŒÚ© Ù…Ø¯Ù„ ØªØµØ§Ø¯ÙÛŒ Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ø­Ø¯ÙˆØ¯ ÛµÛ°Ùª)ØŒ Ú†ÙˆÙ† Ù‡Ù†ÙˆØ² Ø¢Ù…ÙˆØ²Ø´ Ù†Ø¯ÛŒØ¯Ù‡.

---

### âœ… Û´. ØªØ¹Ø±ÛŒÙ ØªØ§Ø¨Ø¹ Loss Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´

Ø¯Ù‚Øª ØªØ§Ø¨Ø¹ÛŒ Ù†ÛŒØ³Øª Ú©Ù‡ Ø¨ØªÙˆØ§Ù† Ø¢Ù† Ø±Ø§ Ø¨Ù‡â€ŒØ·ÙˆØ± Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù‡ÛŒÙ†Ù‡ Ú©Ø±Ø¯. Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ØŒ Ø¨Ø§ÛŒØ¯ ØªØ§Ø¨Ø¹ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ… Ú©Ù‡ Ù‚Ø§Ø¨Ù„ Ù…Ø´ØªÙ‚â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø§Ø´Ø¯.  
Ø¯Ø± Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒØŒ Ø§Ø² **cross-entropy loss** Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….

### ØªØ§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ù‡ loss Ø¨Ø±Ø§ÛŒ ÛŒÚ© batch:

```python
def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch = input_batch.to(device)
    target_batch = target_batch.to(device)
    logits = model(input_batch)[:, -Û±, :]  # ÙÙ‚Ø· Ø¢Ø®Ø±ÛŒÙ† ØªÙˆÚ©Ù† Ù…Ù‡Ù… Ø§Ø³Øª
    loss = torch.nn.functional.cross_entropy(logits, target_batch)
    return loss
```

---

### ØªØ§Ø¨Ø¹ Ù…Ø­Ø§Ø³Ø¨Ù‡ loss Ø±ÙˆÛŒ Ú©Ù„ Ø¯ÛŒØªØ§Ø³Øª (ÛŒØ§ n ØªØ§ batch):

```python
def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = Û°.
    if len(data_loader) == Û°:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))

    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches
```

---

### Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡â€ŒÛŒ loss:

```python
with torch.no_grad():  # Ú†ÙˆÙ† Ø¢Ù…ÙˆØ²Ø´ Ù†Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…ØŒ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ù„Ø§Ø²Ù… Ù†ÛŒØ³Øª
    train_loss = calc_loss_loader(train_loader, model, device, num_batches=Ûµ)
    val_loss = calc_loss_loader(val_loader, model, device, num_batches=Ûµ)
    test_loss = calc_loss_loader(test_loader, model, device, num_batches=Ûµ)

print(f"Training loss: {train_loss:.Û³f}")
print(f"Validation loss: {val_loss:.Û³f}")
print(f"Test loss: {test_loss:.Û³f}")
```

### ğŸ“Œ Ø®Ø±ÙˆØ¬ÛŒ:

```
Training loss: Û².Û´ÛµÛ³
Validation loss: Û².ÛµÛ¸Û³
Test loss: Û².Û³Û²Û²
```

> ğŸ”¸ Ø§ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ø§Ù„Ø§ Ù‡Ø³ØªÙ†Ø¯ØŒ Ú©Ù‡ Ù†Ø´Ø§Ù†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ø¹Ø¯Ù… ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„ Ø§Ø³Øª.

---

## ğŸ“Œ Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ

Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…Ø§:

- Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ù‡ Ø¨Ø±Ú†Ø³Ø¨ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ø¯ÛŒÙ… (Ø¨Ø§ `argmax`)
- Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø±Ø¯ÛŒÙ… (`calc_accuracy_loader`)
- ØªØ§Ø¨Ø¹ loss Ù…Ù†Ø§Ø³Ø¨ (cross-entropy) ØªØ¹Ø±ÛŒÙ Ú©Ø±Ø¯ÛŒÙ…
- Ù…Ù‚Ø¯Ø§Ø± Ø§ÙˆÙ„ÛŒÙ‡â€ŒÛŒ loss Ø±Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú¯Ø±ÙØªÛŒÙ…

## Û¶.Û· - Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ¯Ø§Ø± (supervised)

Ù…Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ Ø§Ø² Ù‚Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´â€ŒØ¯ÛŒØ¯Ù‡ (pretrained LLM) Ø±Ø§ Ø±ÙˆÛŒ ÛŒÚ© Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø§Ø³Ù¾Ù… **Ø¢Ù…ÙˆØ²Ø´ Ù†Ù‡Ø§ÛŒÛŒ** Ø¨Ø¯Ù‡ÛŒÙ… ØªØ§ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¢Ù† Ø¯Ø± Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§Ø¨Ø¯.

---

### Ø­Ù„Ù‚Ù‡ Ø¢Ù…ÙˆØ²Ø´ (Training Loop)

#### ğŸ¯ Ù‡Ø¯Ù:

Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ø±Ø§ÛŒ:

- Ù…Ø­Ø§Ø³Ø¨Ù‡ loss
- Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„
- Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚Øª Ùˆ loss Ø¨Ø¹Ø¯ Ø§Ø² Ù‡Ø± epoch

---

### âœ… Ú©Ø¯ ØªØ§Ø¨Ø¹ Ø¢Ù…ÙˆØ²Ø´:

```python
def train_classifier_simple(
        model, train_loader, val_loader, optimizer, device,
        num_epochs, eval_freq, eval_iter):

    train_losses, val_losses, train_accs, val_accs = [], [], [], []
    examples_seen, global_step = Û°, -Û±

    for epoch in range(num_epochs):
        model.train()
        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward()
            optimizer.step()

            examples_seen += input_batch.shape[Û°]
            global_step += Û±

            # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ
            if global_step % eval_freq == Û°:
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                print(f"Ep {epoch+Û±} (Step {global_step:Û°Û¶d}): "
                      f"Train loss {train_loss:.Û³f}, "
                      f"Val loss {val_loss:.Û³f}")

        # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù¾Ø³ Ø§Ø² Ù¾Ø§ÛŒØ§Ù† Ù‡Ø± epoch
        train_accuracy = calc_accuracy_loader(
            train_loader, model, device, num_batches=eval_iter)
        val_accuracy = calc_accuracy_loader(
            val_loader, model, device, num_batches=eval_iter)
        print(f"Training accuracy: {train_accuracy*Û±Û°Û°:.Û²f}% | ", end="")
        print(f"Validation accuracy: {val_accuracy*Û±Û°Û°:.Û²f}%")

        train_accs.append(train_accuracy)
        val_accs.append(val_accuracy)

    return train_losses, val_losses, train_accs, val_accs, examples_seen
```

---

### ğŸ” ØªØ§Ø¨Ø¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ loss:

```python
def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss
```

---

## âš™ï¸ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´:

```python
import time

start_time = time.time()
torch.manual_seed(Û±Û²Û³)

optimizer = torch.optim.AdamW(model.parameters(), lr=Ûµe-Ûµ, weight_decay=Û°.Û±)

num_epochs = Ûµ
train_losses, val_losses, train_accs, val_accs, examples_seen = \
    train_classifier_simple(
        model, train_loader, val_loader, optimizer, device,
        num_epochs=num_epochs, eval_freq=ÛµÛ°,
        eval_iter=Ûµ
    )
end_time = time.time()

print(f"Training completed in {(end_time - start_time) / Û¶Û°:.Û²f} minutes.")
```

---

## âœ… Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´:

```
Ep Û± (Step Û°Û°Û°Û°Û°Û°): Train loss Û².Û±ÛµÛ³, Val loss Û².Û³Û¹Û²
Ep Û± (Step Û°Û°Û°Û°ÛµÛ°): Train loss Û°.Û¶Û±Û·, Val loss Û°.Û¶Û³Û·
Ep Û± (Step Û°Û°Û°Û±Û°Û°): Train loss Û°.ÛµÛ²Û³, Val loss Û°.ÛµÛµÛ·
Training accuracy: Û·Û°.Û°Û°% | Validation accuracy: Û·Û².ÛµÛ°%

Ep Û² ...
Training accuracy: Û¸Û².ÛµÛ°% | Validation accuracy: Û¸Ûµ.Û°Û°%

...

Ep Ûµ ...
Training accuracy: Û±Û°Û°.Û°Û°% | Validation accuracy: Û¹Û·.ÛµÛ°%

Training completed in Ûµ.Û¶Ûµ minutes.
```

> ğŸ” Ø¯Ù‚Øªâ€ŒÙ‡Ø§ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ù‡Ø¨ÙˆØ¯ Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ù†Ø´Ø§Ù†Ù‡â€ŒØ§ÛŒ Ø§Ø² **ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¨ Ø¨Ø¯ÙˆÙ† Ø¨ÛŒØ´â€ŒØ¨Ø±Ø§Ø²Ø´ (overfitting)** ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯.

---

## ğŸ“‰ ØªØ±Ø³ÛŒÙ… Ù†Ù…ÙˆØ¯Ø§Ø± Loss:

```python
import matplotlib.pyplot as plt

def plot_values(epochs_seen, examples_seen, train_values, val_values, label="loss"):
    fig, axÛ± = plt.subplots(figsize=(Ûµ, Û³))
    axÛ±.plot(epochs_seen, train_values, label=f"Training {label}")
    axÛ±.plot(epochs_seen, val_values, linestyle="-.", label=f"Validation {label}")
    axÛ±.set_xlabel("Epochs")
    axÛ±.set_ylabel(label.capitalize())
    axÛ±.legend()

    axÛ² = axÛ±.twiny()
    axÛ².plot(examples_seen, train_values, alpha=Û°)
    axÛ².set_xlabel("Examples seen")

    fig.tight_layout()
    plt.savefig(f"{label}-plot.pdf")
    plt.show()

# Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± loss:
epochs_tensor = torch.linspace(Û°, num_epochs, len(train_losses))
examples_seen_tensor = torch.linspace(Û°, examples_seen, len(train_losses))
plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)
```

### ğŸ“Œ ØªÙØ³ÛŒØ± Ù†Ù…ÙˆØ¯Ø§Ø±:

- Ù‡Ø± Ø¯Ùˆ loss (Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ) Ú©Ø§Ù‡Ø´ ÛŒÚ©Ù†ÙˆØ§Ø®ØªÛŒ Ø¯Ø§Ø±Ù†Ø¯.
- Ø´Ú©Ø§ÙÛŒ Ø¨ÛŒÙ† Ø¯Ùˆ Ù…Ù†Ø­Ù†ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ â‡’ **Ø¨Ø¯ÙˆÙ† overfitting**.

---

#### ğŸ“ˆ ØªØ±Ø³ÛŒÙ… Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ù‚Øª (Accuracy):

```python
epochs_tensor = torch.linspace(Û°, num_epochs, len(train_accs))
examples_seen_tensor = torch.linspace(Û°, examples_seen, len(train_accs))
plot_values(
    epochs_tensor, examples_seen_tensor, train_accs, val_accs,
    label="accuracy"
)
```

> Ù†ØªÛŒØ¬Ù‡: Ø¯Ù‚Øª Ø¨Ù‡ Ø³Ø±Ø¹Øª Ø¯Ø± Ø§ÙˆØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ Ø§ÙØ²Ø§ÛŒØ´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯ Ùˆ Ø¯Ø± Ù…Ø±Ø§Ø­Ù„ Ù¾Ø§ÛŒØ§Ù†ÛŒ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø¨Ù‡ Û±.Û° Ù…ÛŒâ€ŒØ±Ø³Ø¯.

---

### ğŸ¯ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¨Ø¯ÙˆÙ† eval_iter):

```python
train_accuracy = calc_accuracy_loader(train_loader, model, device)
val_accuracy = calc_accuracy_loader(val_loader, model, device)
test_accuracy = calc_accuracy_loader(test_loader, model, device)

print(f"Training accuracy: {train_accuracy*Û±Û°Û°:.Û²f}%")
print(f"Validation accuracy: {val_accuracy*Û±Û°Û°:.Û²f}%")
print(f"Test accuracy: {test_accuracy*Û±Û°Û°:.Û²f}%")
```

### âœ… Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ:

```
Training accuracy: Û¹Û·.Û²Û±%
Validation accuracy: Û¹Û·.Û³Û²%
Test accuracy: Û¹Ûµ.Û¶Û·%
```

> Ø§ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ø§Ù„Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯ Ú©Ù‡ Ù…Ø¯Ù„ Ø¨Ø§ Ø¯Ù‚Øª Ø¨Ø³ÛŒØ§Ø± Ø®ÙˆØ¨ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾Ù… Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ø§Ø®ØªÙ„Ø§Ù Ú©Ù… Ø¨ÛŒÙ† train/val/test Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ **Ø¹Ù…ÙˆÙ…ÛŒâ€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ¨** Ø§Ø³Øª.

---

## ğŸ§© Ù†Ú©Ø§Øª Ù¾Ø§ÛŒØ§Ù†ÛŒ:

- Ø§Ú¯Ø± Ø§Ø®ØªÙ„Ø§Ù Ø²ÛŒØ§Ø¯ÛŒ Ø¨ÛŒÙ† Ø¯Ù‚Øª train Ùˆ test Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø´ÙˆØ¯ØŒ **Ø§Ø­ØªÙ…Ø§Ù„ overfitting** ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯.
- Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯:
  - Ù…ÛŒâ€ŒØªÙˆØ§Ù† dropout ÛŒØ§ weight decay Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ø§Ø¯.
  - ØªØ¹Ø¯Ø§Ø¯ epochÙ‡Ø§ Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ø±Ø¯.
- Ø¯Ø± Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ØŒ **Ûµ epoch Ú©Ø§ÙÛŒ Ø¨ÙˆØ¯** Ú†ÙˆÙ† Ù…Ø¯Ù„ Ø¨Ù‡ Ø³Ø±Ø¹Øª ÛŒØ§Ø¯ Ú¯Ø±ÙØª Ùˆ overfit Ù†Ú©Ø±Ø¯.

---

## ğŸ“Œ Ø®Ù„Ø§ØµÙ‡ ØªØµÙˆÛŒØ±ÛŒ:

| Ù…Ø±Ø­Ù„Ù‡                              | Ø¹Ù…Ù„Ú©Ø±Ø¯                                  |
| ---------------------------------- | --------------------------------------- |
| âœ… Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ú†Ø³Ø¨â€ŒØ¯Ø§Ø± | Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ú©Ø§Ù…Ù„ Ø´Ø¯                        |
| ğŸ“‰ Ú©Ø§Ù‡Ø´ loss                       | Ø¯Ø± Ù‡Ø± epoch Ø¯ÛŒØ¯Ù‡ Ø´Ø¯                     |
| ğŸ“ˆ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‚Øª                      | ØªØ§ Û¹Û·Ùª Ø±Ø³ÛŒØ¯                             |
| ğŸ”¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ                   | Ø¨Ø¯ÙˆÙ† overfittingØŒ Ù†ØªØ§ÛŒØ¬ Ø®ÙˆØ¨ Ø¯Ø± test set |

## Û¶.Û¸ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LLM Ø¨Ù‡â€ŒØ¹Ù†ÙˆØ§Ù† Ú©Ù„Ø§Ø³Ù‡â€ŒØ¨Ù†Ø¯ Ø§Ø³Ù¾Ù…

Ø¨Ø¹Ø¯ Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ù…Ø¯Ù„ Ø±Ø§ Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… (fine-tune) Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ø±Ø¯ÛŒÙ…ØŒ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ§ÛŒÙ… Ú©Ù‡ Ø§Ø² Ø¢Ù† Ø¨Ø±Ø§ÛŒ **Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯** Ø¨Ù‡ Ø§Ø³Ù¾Ù… ÛŒØ§ ØºÛŒØ± Ø§Ø³Ù¾Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ… (Ø´Ú©Ù„ Û¶.Û±Û¸).

---

### ğŸ› ï¸ ØªØ§Ø¨Ø¹ `classify_review`

Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù…ØªÙ† Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ùˆ Ù¾Ø³ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ø´Ø§Ø¨Ù‡ Ù…Ø±Ø­Ù„Ù‡â€ŒÛŒ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (tokenization Ùˆ padding)ØŒ Ø¨Ø§ Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ùˆ Ø¨Ø±Ú†Ø³Ø¨ "spam" ÛŒØ§ "not spam" Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.

```python
def classify_review(
        text, model, tokenizer, device, max_length=None,
        pad_token_id=ÛµÛ°Û²ÛµÛ¶):
    model.eval()
    input_ids = tokenizer.encode(text)          #Û±

    supported_context_length = model.pos_emb.weight.shape[Û±]

    input_ids = input_ids[:min(                   #Û²
        max_length, supported_context_length
    )]

    input_ids += [pad_token_id] * (max_length - len(input_ids))    #Û³

    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(Û°)   #Û´

    with torch.no_grad():                                #Ûµ
        logits = model(input_tensor)[:, -Û±, :]     #Û¶

    predicted_label = torch.argmax(logits, dim=-Û±).item()

    return "spam" if predicted_label == Û± else "not spam"     #Û·
```

### ØªÙˆØ¶ÛŒØ­Ø§Øª Ú©Ø¯:

- (Û±) Ù…ØªÙ† ÙˆØ±ÙˆØ¯ÛŒ Ø±Ø§ Ø¨Ù‡ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
- (Û²) Ø§Ú¯Ø± Ø·ÙˆÙ„ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø§Ø² `max_length` ÛŒØ§ Ø·ÙˆÙ„ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…Ø¯Ù„ Ø¨ÛŒØ´ØªØ± Ø¨ÙˆØ¯ØŒ Ú©ÙˆØªØ§Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
- (Û³) ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø±Ø§ ØªØ§ Ø·ÙˆÙ„ `max_length` Ø¨Ø§ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ padding Ù¾Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
- (Û´) ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ Tensor ØªØ¨Ø¯ÛŒÙ„ Ùˆ Ø¨Ù‡ Ø´Ú©Ù„ batch Ø¯Ø± Ù…ÛŒâ€ŒØ¢ÙˆØ±Ø¯.
- (Ûµ) Ù…Ø¯Ù„ Ø±Ø§ Ø¯Ø± Ø­Ø§Ù„Øª inference Ø¨Ø¯ÙˆÙ† Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù† Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
- (Û¶) Ù„Ø§Ø¬ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¢Ø®Ø±ÛŒÙ† ØªÙˆÚ©Ù† Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ (Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„).
- (Û·) Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ù„Ø§Ø¬ÛŒØªØŒ Ø¨Ø±Ú†Ø³Ø¨ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±Ø§ ØªØ¹ÛŒÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

---

### ØªØ³Øª ØªØ§Ø¨Ø¹ Ø±ÙˆÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§

```python
text_Û± = (
    "You are a winner you have been specially"
    " selected to receive $Û±Û°Û°Û° cash or a $Û²Û°Û°Û° award."
)
print(classify_review(text_Û±, model, tokenizer, device, max_length=train_dataset.max_length))
# Ø®Ø±ÙˆØ¬ÛŒ: "spam"

text_Û² = (
    "Hey, just wanted to check if we're still on"
    " for dinner tonight? Let me know!"
)
print(classify_review(text_Û², model, tokenizer, device, max_length=train_dataset.max_length))
# Ø®Ø±ÙˆØ¬ÛŒ: "not spam"
```

---

### Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø¬Ø¯Ø¯ Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¬Ø¯Ø¯:

```python
torch.save(model.state_dict(), "review_classifier.pth")
```

Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡:

```python
model_state_dict = torch.load("review_classifier.pth", map_location=device)
model.load_state_dict(model_state_dict)
```

---

## ğŸ“ Ø®Ù„Ø§ØµÙ‡ Ù†Ú©Ø§Øª Ù…Ù‡Ù…:

- **Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ** (classification fine-tuning) Ø´Ø§Ù…Ù„ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ Ø¨Ø§ Ù„Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ú©ÙˆÚ†Ú©â€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³Øª (Ù…Ø«Ù„Ø§Ù‹ Û² Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ù¾Ù…/ØºÛŒØ± Ø§Ø³Ù¾Ù…).
- Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø±Ø®Ù„Ø§Ù Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´ Ú©Ù‡ Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØªÙˆÚ©Ù† Ø¨Ø¹Ø¯ÛŒ Ø¨ÙˆØ¯ØŒ Ù…Ø¯Ù„ Ø§Ú©Ù†ÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨ Ú©Ù„Ø§Ø³ (spam/not spam) Ø±Ø§ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
- ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø¯Ù„ØŒ Ù…Ø§Ù†Ù†Ø¯ Ù‚Ø¨Ù„ØŒ Ù…ØªÙ† Ø¨Ù‡ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯Ù‡ Ø§Ø³Øª.
- Ù‚Ø¨Ù„ Ø§Ø² Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ…ØŒ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡ pretrained Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
- Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ù‚Øª Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.
- ØªØ§Ø¨Ø¹ loss Ø¯Ø± Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù‡Ù…Ø§Ù† ØªØ§Ø¨Ø¹ cross entropy Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´Ø¯.
# ÙØµÙ„ Û¶: 


> [ 
    5.Ù¾ÛŒØ´â€ŒØ¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ø¨Ø±Ú†Ø³Ø¨
     (Ù‚Ø¨Ù„ÛŒ) ](
        <05.Pretraining on unlabeled data.md>
        ) <6.Ø±ÛŒØ²ØªÙ†Ø¸ÛŒÙ… Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ>
    [
    7.ØªÙ†Ø¸ÛŒÙ… Ø¯Ù‚ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ±ÙˆÛŒ Ø§Ø² Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„â€ŒÙ‡Ø§
(Ø¨Ø¹Ø¯ÛŒ)
](<07.Fine-tuning to follow instructions.md>)