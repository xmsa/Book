<!-- language: rtl -->

# فصل ۷: تنظیم دقیق (Fine-tuning) برای پیروی از دستورالعمل‌ها

#### موضوعات این فصل

- فرآیند تنظیم دقیق مدل‌های زبان بزرگ (LLMs) برای پیروی از دستورالعمل‌ها
- آماده‌سازی داده‌ها برای تنظیم دقیق تحت نظارت
- سازماندهی داده‌های دستورالعمل در دسته‌های آموزشی
- بارگذاری یک مدل زبان بزرگ پیش‌آموزش‌دیده و تنظیم دقیق آن برای پیروی از دستورهای انسانی
- استخراج پاسخ‌های تولیدشده توسط مدل برای ارزیابی
- ارزیابی مدل تنظیم دقیق‌شده

---

## مقدمه‌ای بر تنظیم دقیق برای پیروی از دستورالعمل‌ها (7.1)

در بخش‌های قبلی، معماری مدل زبان بزرگ را پیاده‌سازی و مدل را پیش‌آموزش دادیم و وزن‌های پیش‌آموزش‌دیده را وارد کردیم. سپس مدل را برای یک کار طبقه‌بندی خاص (تشخیص پیام‌های اسپم و غیر اسپم) تنظیم دقیق کردیم. اکنون فرآیند تنظیم دقیق برای پیروی از دستورهای انسانی را پیاده‌سازی می‌کنیم، همان‌طور که در شکل 7.1 نشان داده شده است.

تنظیم دقیق دستورالعملی یکی از تکنیک‌های اصلی توسعه مدل‌های زبان بزرگ برای کاربردهایی مانند چت‌بات‌ها، دستیارهای شخصی و وظایف مکالمه‌ای است.

---

## 7.2 آماده‌سازی داده‌ها برای تنظیم دقیق تحت نظارت

داده‌های مورد استفاده شامل ۱۱۰۰ جفت دستور-پاسخ است که مشابه نمونه‌های شکل 7.2 می‌باشد. این داده‌ها برای این کتاب ایجاد شده‌اند، اما نمونه‌های عمومی دیگری نیز در پیوست B آمده است.

فرمت داده‌ها در قالب JSON است که ساختاری مشابه دیکشنری‌های پایتون دارد و هم برای انسان و هم ماشین قابل خواندن است.

#### دانلود و بارگذاری داده‌ها

```python
import json
import os
import urllib

def download_and_load_file(file_path, url):
    if not os.path.exists(file_path):
        with urllib.request.urlopen(url) as response:
            text_data = response.read().decode("utf-8")
        with open(file_path, "w", encoding="utf-8") as file:
            file.write(text_data)
    else:  # اگر فایل قبلا دانلود شده باشد، دانلود را رد می‌کند
        with open(file_path, "r", encoding="utf-8") as file:
            text_data = file.read()
    with open(file_path, "r") as file:
        data = json.load(file)
    return data

file_path = "instruction-data.json"
url = (
    "https://raw.githubusercontent.com/rasbt/LLMs-from-scratch"
    "/main/ch07/01_main-chapter-code/instruction-data.json"
)

data = download_and_load_file(file_path, url)
print("Number of entries:", len(data))
```

خروجی اجرای کد بالا:

```
Number of entries: 1100
```

#### ساختار نمونه داده‌ها

هر ورودی داده به صورت یک دیکشنری شامل سه کلید است:

- `instruction`: دستور کاری که باید انجام شود
- `input`: ورودی مربوط به دستور (ممکن است خالی باشد)
- `output`: پاسخ مطلوب

مثال:

```python
print("Example entry:\n", data[50])
```

خروجی:

```
Example entry:
 {'instruction': 'Identify the correct spelling of the following word.',
  'input': 'Ocassion',
  'output': "The correct spelling is 'Occasion.'"}
```

مثال دیگر با ورودی خالی:

```python
print("Another example entry:\n", data[999])
```

خروجی:

```
Another example entry:
 {'instruction': "What is an antonym of 'complicated'?",
  'input': '',
  'output': "An antonym of 'complicated' is 'simple'."}
```

---

#### فرمت‌بندی داده‌ها برای آموزش

در تنظیم دقیق دستورالعملی، ورودی‌ها و خروجی‌ها به صورت جفت‌های صریح آموزش داده می‌شوند. دو سبک معمول فرمت‌بندی وجود دارد که در شکل 7.4 مقایسه شده‌اند:

- سبک Alpaca: فرمت ساختاریافته با بخش‌های مشخص برای دستور، ورودی و پاسخ
- سبک Phi-3: فرمت ساده‌تر با نشانه‌گذاری‌های `<|user|>` و `<|assistant|>`

در این فصل از سبک Alpaca استفاده می‌کنیم، زیرا یکی از محبوب‌ترین و شناخته‌شده‌ترین روش‌هاست.

#### تابع فرمت‌بندی ورودی به سبک Alpaca

```python
def format_input(entry):
    instruction_text = (
        f"Below is an instruction that describes a task. "
        f"Write a response that appropriately completes the request."
        f"\n\n### Instruction:\n{entry['instruction']}"
    )
    input_text = (
        f"\n\n### Input:\n{entry['input']}" if entry["input"] else ""
    )
    return instruction_text + input_text
```

اجرای نمونه برای داده شماره ۵۰:

```python
model_input = format_input(data[50])
desired_response = f"\n\n### Response:\n{data[50]['output']}"
print(model_input + desired_response)
```

خروجی:

```
Below is an instruction that describes a task. Write a response that
appropriately completes the request.
### Instruction:
Identify the correct spelling of the following word.
### Input:
Ocassion
### Response:
The correct spelling is 'Occasion.'
```

برای ورودی خالی، بخش `### Input:` حذف می‌شود:

```python
model_input = format_input(data[999])
desired_response = f"\n\n### Response:\n{data[999]['output']}"
print(model_input + desired_response)
```

خروجی:

```
Below is an instruction that describes a task. Write a response that
appropriately completes the request.
### Instruction:
What is an antonym of 'complicated'?
### Response:
An antonym of 'complicated' is 'simple'.
```

---

#### تقسیم‌بندی داده‌ها به مجموعه‌های آموزشی، اعتبارسنجی و آزمون

مطابق با فصل قبل، داده‌ها را به سه بخش تقسیم می‌کنیم:

```python
train_portion = int(len(data) * 0.85)    # ۸۵٪ برای آموزش
test_portion = int(len(data) * 0.1)      # ۱۰٪ برای آزمون
val_portion = len(data) - train_portion - test_portion  # باقی‌مانده برای اعتبارسنجی

train_data = data[:train_portion]
test_data = data[train_portion:train_portion + test_portion]
val_data = data[train_portion + test_portion:]

print("Training set length:", len(train_data))
print("Validation set length:", len(val_data))
print("Test set length:", len(test_data))
```

خروجی:

```
Training set length: 935
Validation set length: 55
Test set length: 110
```

## 7.3 سازماندهی داده‌ها به صورت دسته‌های آموزشی (Training Batches)

در ادامه مراحل اجرای فرآیند فاین‌تیونینگ دستوری، گام بعدی که در شکل ۷.۵ نشان داده شده، تمرکز بر ساخت مؤثر دسته‌های آموزشی است. این مرحله شامل تعریف روشی است که تضمین کند مدل ما داده‌های آموزشی فرمت‌شده را در طول فاین‌تیونینگ دریافت می‌کند.

**شکل ۷.۵**  
فرآیند سه مرحله‌ای فاین‌تیونینگ دستوری یک مدل زبان بزرگ (LLM). در اینجا به گام دوم از مرحله اول یعنی جمع‌آوری دسته‌های آموزشی می‌پردازیم.

در فصل قبل، دسته‌های آموزشی به صورت خودکار توسط کلاس `DataLoader` در PyTorch ساخته می‌شدند که از یک تابع `collate` پیش‌فرض برای ترکیب نمونه‌های داده به دسته‌ها استفاده می‌کند. وظیفه تابع `collate` این است که لیستی از نمونه‌های داده را گرفته و آن‌ها را به یک دسته واحد تبدیل کند تا مدل بتواند در آموزش به شکل بهینه آن‌ها را پردازش کند.

اما در فاین‌تیونینگ دستوری، فرآیند دسته‌بندی کمی پیچیده‌تر است و نیازمند ایجاد یک تابع `collate` سفارشی هستیم که بعداً آن را به `DataLoader` متصل می‌کنیم. این تابع سفارشی برای پاسخگویی به نیازها و فرمت خاص داده‌های فاین‌تیونینگ طراحی می‌شود.

---

#### پیاده‌سازی مرحله‌بندی دسته‌ها به چند گام (شکل ۷.۶)

برای پیاده‌سازی گام‌های ۲.۱ و ۲.۲، ابتدا کلاسی به نام `InstructionDataset` می‌نویسیم که تابع `format_input` را روی همه ورودی‌ها اعمال کرده و پیش‌توکنایز می‌کند، مشابه کاری که در فصل ۶ با `SpamDataset` انجام دادیم. این فرایند دو مرحله‌ای در متد سازنده `__init__` کلاس پیاده‌سازی شده است (شکل ۷.۷).

---

**شکل ۷.۶**

پنج زیرمرحله در پیاده‌سازی فرآیند دسته‌بندی:

- ۲.۱ اعمال قالب (prompt template)
- ۲.۲ استفاده از توکنایزینگ
- ۲.۳ افزودن توکن‌های padding
- ۲.۴ ایجاد IDهای توکن هدف (target)
- ۲.۵ جایگزینی توکن‌های نگهدارنده `-100` برای ماسک کردن padding در محاسبه loss

---

**شکل ۷.۷**

دو مرحله اول پیاده‌سازی فرآیند دسته‌بندی. ابتدا ورودی‌ها با قالب خاص فرمت شده (۲.۱) و سپس توکنایز می‌شوند (۲.۲) تا رشته‌ای از IDهای توکن ایجاد شود که مدل بتواند پردازش کند.

---

**فهرست ۷.۴**

پیاده‌سازی کلاس InstructionDataset

```python
import torch
from torch.utils.data import Dataset

class InstructionDataset(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data
        self.encoded_texts = []
        for entry in data:         #1
            instruction_plus_input = format_input(entry)
            response_text = f"\n\\n### Response:\\n{entry['output']}"
            full_text = instruction_plus_input + response_text
            self.encoded_texts.append(
                tokenizer.encode(full_text)
            )
    def __getitem__(self, index):
        return self.encoded_texts[index]
    def __len__(self):
        return len(self.data)
```

> #1 پیش‌توکنایز کردن متون

---

مانند فاین‌تیونینگ برای دسته‌بندی (classification)، برای افزایش سرعت آموزش، نمونه‌های آموزشی متعددی را در یک دسته (batch) جمع‌آوری می‌کنیم که نیازمند پدینگ (padding) همه ورودی‌ها به طول مشابه است. توکن `<|endoftext|>` به عنوان توکن پدینگ استفاده می‌شود.

به جای افزودن توکن‌های `<|endoftext|>` به متن، می‌توانیم شناسه توکن مربوط به `<|endoftext|>` را مستقیماً به ورودی‌های پیش‌توکنایز شده اضافه کنیم. برای یافتن شناسه این توکن از متد `.encode` توکنایزر استفاده می‌کنیم:

```python
import tiktoken
tokenizer = tiktoken.get_encoding("gpt2")
print(tokenizer.encode("<|endoftext|>", allowed_special={"<|endoftext|>"}))
```

نتیجه این کد شناسه توکن `50256` است.

---

**مرحله ۲.۳: ایجاد تابع collate سفارشی (شکل ۷.۶)**

ما تابع collate سفارشی می‌نویسیم که طول تمام نمونه‌های یک دسته را به اندازه طول طولانی‌ترین نمونه آن دسته پد می‌کند. این کار باعث کاهش پدینگ اضافی نسبت به پدینگ کل داده‌ها می‌شود.

---

**شکل ۷.۸**

پدینگ نمونه‌های آموزشی در هر دسته با شناسه توکن ۵۰۲۵۶ برای یکسان‌سازی طول درون دسته. هر دسته می‌تواند طول متفاوتی داشته باشد.

---

**نمونه کد تابع collate اولیه**

```python
def custom_collate_draft_1(
    batch,
    pad_token_id=50256,
    device="cpu"
):
    batch_max_length = max(len(item)+1 for item in batch)   #1
    inputs_lst = []
    for item in batch:     #2
        new_item = item.copy()
        new_item += [pad_token_id]
        padded = (
            new_item + [pad_token_id] *
            (batch_max_length - len(new_item))
        )
        inputs = torch.tensor(padded[:-1])    #3
        inputs_lst.append(inputs)
    inputs_tensor = torch.stack(inputs_lst).to(device)     #4
    return inputs_tensor
```

> #1 پیدا کردن طول بیشترین رشته در دسته  
> #2 پدینگ و آماده‌سازی ورودی‌ها  
> #3 حذف توکن اضافی پدینگ در انتها  
> #4 تبدیل لیست ورودی‌ها به تنسور و انتقال به دستگاه هدف

---

**آزمایش تابع collate با سه ورودی متفاوت**

```python
inputs_1 = [0, 1, 2, 3, 4]
inputs_2 = [5, 6]
inputs_3 = [7, 8, 9]
batch = (inputs_1, inputs_2, inputs_3)
print(custom_collate_draft_1(batch))
```

خروجی:

```
tensor([[    0,     1,     2,     3,     4],
        [    5,     6, 50256, 50256, 50256],
        [    7,     8,     9, 50256, 50256]])
```

همه ورودی‌ها به طول طولانی‌ترین ورودی (inputs_1) پد شده‌اند.

---

**مرحله ۲.۴: ایجاد توکن‌های هدف (Target Token IDs)**

ما باید دسته‌های هدف را هم همراه ورودی‌ها بسازیم. توکن‌های هدف مشابه توکن‌های ورودی هستند ولی به اندازه یک موقعیت به راست شیفت داده شده‌اند. این روش به مدل کمک می‌کند توکن بعدی را پیش‌بینی کند.

---

**شکل ۷.۱۰**

تنظیم ورودی و هدف با شیفت یک توکن به راست برای پیش‌بینی توکن بعدی توسط مدل.

---

**نسخه به‌روزشده تابع collate**

```python
def custom_collate_draft_2(
    batch,
    pad_token_id=50256,
    device="cpu"
):
    batch_max_length = max(len(item)+1 for item in batch)
    inputs_lst, targets_lst = [], []
    for item in batch:
        new_item = item.copy()
        new_item += [pad_token_id]
        padded = (
            new_item + [pad_token_id] *
            (batch_max_length - len(new_item))
        )
        inputs = torch.tensor(padded[:-1])     #1
        targets = torch.tensor(padded[1:])    #2
        inputs_lst.append(inputs)
        targets_lst.append(targets)
    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)
    return inputs_tensor, targets_tensor
```

> #1 حذف آخرین توکن برای ورودی‌ها  
> #2 شیفت به راست برای توکن‌های هدف

---

**آزمایش تابع**

```python
inputs, targets = custom_collate_draft_2(batch)
print(inputs)
print(targets)
```

خروجی:

```
tensor([[    0,     1,     2,     3,     4],
        [    5,     6, 50256, 50256, 50256],
        [    7,     8,     9, 50256, 50256]])
tensor([[    1,     2,     3,     4, 50256],
        [    6, 50256, 50256, 50256, 50256],
        [    8,     9, 50256, 50256, 50256]])
```

---

#### مرحله ۲.۵: جایگزینی توکن‌های padding در توکن‌های هدف با مقدار -100 (ماسک کردن)

مقدار `-100` برای توکن‌های padding در دسته هدف قرار داده می‌شود تا این توکن‌ها در محاسبه loss آموزش لحاظ نشوند و فقط داده‌های معنی‌دار بر یادگیری مدل تأثیر بگذارند.

---

**شکل ۷.۱۱**

پس از ساخت توالی هدف، توکن‌های padding به مقدار -100 جایگزین می‌شوند.

---

توجه کنید که یک توکن `<|endoftext|>` (شناسه ۵۰۲۵۶) را حفظ می‌کنیم تا مدل بیاموزد چه زمانی باید توکن پایان متن تولید کند.

---

**شکل ۷.۱۲**

جایگزینی همه توکن‌های padding به جز اولین توکن پایان متن با مقدار -100.

---

#### نسخه کامل تابع collate با ماسک کردن padding

```python
def custom_collate_fn(
    batch,
    pad_token_id=50256,
    ignore_index=-100,
    allowed_max_length=None,
    device="cpu"
):
    batch_max_length = max(len(item)+1 for item in batch)
    inputs_lst, targets_lst = [], []
    for item in batch:
        new_item = item.copy()
        new_item += [pad_token_id]
        padded = (
            new_item + [pad_token_id] *
            (batch_max_length - len(new_item))
        )
        inputs = torch.tensor(padded[:-1])
        targets = torch.tensor(padded[1:])
        mask = targets == pad_token_id
        indices = torch.nonzero(mask).squeeze()
        if indices.numel() > 1:
            targets[indices[1:]] = ignore_index
        if allowed_max_length is not None:
            inputs = inputs[:allowed_max_length]
            targets = targets[:allowed_max_length]
        inputs_lst.append(inputs)
        targets_lst.append(targets)
    inputs_tensor = torch.stack(inputs_lst).to(device)
    targets_tensor = torch.stack(targets_lst).to(device)
    return inputs_tensor, targets_tensor
```

> #1 پدینگ تا حداکثر طول دسته
> #2 حذف آخرین توکن برای ورودی‌ها  
> #3 شیفت راست برای توکن‌های هدف  
> #4 جایگزینی همه توکن‌های padding به جز اولین با ignore_index
> #5 برش توکن‌ها به طول مجاز در صورت تعیین

---

#### تست تابع با دسته نمونه

```python
inputs, targets = custom_collate_fn(batch)
print(inputs)
print(targets)
```

خروجی:

```
tensor([[    0,     1,     2,     3,     4],
        [    5,     6, 50256, 50256, 50256],
        [    7,     8,     9, 50256, 50256]])
tensor([[    1,     2,     3,     4, 50256],
        [    6, 50256,  -100,  -100,  -100],
        [    8,     9, 50256,  -100,  -100]])
```

---

#### دلیل استفاده از -100 در محاسبه loss

در PyTorch، تابع `cross_entropy` به صورت پیش‌فرض مقدار `ignore_index=-100` دارد؛ یعنی تمام هدف‌هایی که برابر با -100 باشند در محاسبه loss نادیده گرفته می‌شوند. ما از این قابلیت برای نادیده گرفتن توکن‌های padding استفاده می‌کنیم. با این کار، توکن‌های padding تأثیری در آموزش مدل ندارند اما یک توکن پایان متن حفظ می‌شود تا مدل بیاموزد چه زمانی باید پاسخ را پایان دهد.

---

#### مثال ساده محاسبه loss با cross entropy

```python
logits_1 = torch.tensor(
    [[-1.0, 1.0],
     [-0.5, 1.5]]
)
targets_1 = torch.tensor([0, 1])
loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)
print(loss_1)
```

خروجی:

```
tensor(1.1269)
```

---

افزودن یک توکن بیشتر:

```python
logits_2 = torch.tensor(
    [[-1.0, 1.0],
     [-0.5, 1.5],
     [-0.5, 1.5]]
)
targets_2 = torch.tensor([0, 1, 1])
loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)
print(loss_2)
```

خروجی:

```
tensor(0.7936)
```

---

جایگزینی توکن سوم با -100:

```python
targets_3 = torch.tensor([0, 1, -100])
loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)
print(loss_3)
print("loss_1 == loss_3:", loss_1 == loss_3)
```

خروجی:

```
tensor(1.1269)
loss_1 == loss_3: tensor(True)
```

نتیجه: توکن‌های هدف با مقدار -100 در محاسبه loss نادیده گرفته می‌شوند.

---

#### ماسک کردن توکن‌های instruction در دسته هدف (اختیاری)

می‌توان بخش دستورالعمل (instruction) در ورودی‌ها را با مقدار -100 ماسک کرد تا فقط پاسخ تولید شده مورد آموزش قرار گیرد. این کار از بیش‌برازش (overfitting) مدل روی دستورالعمل‌ها جلوگیری می‌کند (شکل ۷.۱۳).

---

**شکل ۷.۱۳**

**سمت چپ:** متن ورودی قالب‌بندی شده که آن را توکنیزه کرده و در طول آموزش به مدل زبان بزرگ (LLM) می‌دهیم.  
**سمت راست:** متن هدفی که برای مدل آماده می‌کنیم و در آن می‌توان به‌صورت اختیاری بخش دستورالعمل را ماسک زد؛ یعنی شناسه‌های توکن مربوط به دستورالعمل را با مقدار -100 که در تابع loss به عنوان `ignore_index` تعریف شده است، جایگزین کرد.

تا زمان نگارش این مطلب، پژوهشگران در مورد مفید بودن ماسک کردن دستورالعمل‌ها در فرآیند آموزش دقیق مدل اختلاف نظر دارند. برای مثال، مقاله‌ای از Shi و همکاران در سال ۲۰۲۴ با عنوان «Instruction Tuning With Loss Over Instructions» (https://arxiv.org/abs/2405.14394) نشان داده است که عدم ماسک کردن دستورالعمل‌ها می‌تواند عملکرد مدل را بهبود بخشد (جزئیات بیشتر در پیوست B موجود است).

در اینجا ما ماسک کردن را اعمال نمی‌کنیم و این کار را به عنوان یک تمرین اختیاری برای خوانندگان علاقه‌مند باقی می‌گذاریم.

---

**تمرین ۷.۲: ماسک کردن دستورالعمل و ورودی‌ها**

پس از اتمام این فصل و انجام آموزش دقیق مدل با استفاده از `InstructionDataset`، شناسه‌های توکن مربوط به دستورالعمل و ورودی‌ها را با مقدار -100 جایگزین کنید تا روش ماسک کردن دستورالعمل که در شکل ۷.۱۳ نشان داده شده است، پیاده‌سازی شود. سپس بررسی کنید که آیا این روش تأثیر مثبتی بر عملکرد مدل دارد یا خیر.

## ۷.۴ ساخت داده‌بارکن‌ها (Data Loaders) برای مجموعه داده دستوری (Instruction Dataset)

ما مراحل مختلفی را برای پیاده‌سازی کلاس `InstructionDataset` و تابع سفارشی `custom_collate_fn` برای مجموعه داده دستوری به پایان رسانده‌ایم. همانطور که در شکل ۷.۱۴ نشان داده شده، اکنون آماده هستیم تا با اتصال این دو به داده‌بارکن‌های PyTorch، از تلاش خود بهره‌مند شویم. این داده‌بارکن‌ها به طور خودکار داده‌ها را به صورت تصادفی مرتب و دسته‌بندی می‌کنند تا فرایند آموزش دقیق مدل زبان بزرگ (LLM) با دستورالعمل‌ها انجام شود.

---

**شکل ۷.۱۴**  
فرآیند سه مرحله‌ای آموزش دقیق LLM با دستورالعمل‌ها: تاکنون داده‌ها آماده و تابع collate سفارشی پیاده‌سازی شده است. اکنون می‌توانیم داده‌بارکن‌ها را برای مجموعه‌های آموزش، اعتبارسنجی و تست بسازیم و برای آموزش و ارزیابی مدل استفاده کنیم.

---

قبل از پیاده‌سازی گام ساخت داده‌بارکن‌ها، باید کمی درباره تنظیم دستگاه (device) در تابع `custom_collate_fn` صحبت کنیم. این تابع شامل کدی است که تنسورهای ورودی و هدف (مثلاً `torch.stack(inputs_lst).to(device)`) را به دستگاه مشخصی منتقل می‌کند؛ که می‌تواند `"cpu"`، `"cuda"` (برای کارت‌های گرافیک NVIDIA) یا به صورت اختیاری `"mps"` (برای مک‌های مجهز به تراشه Apple Silicon) باشد.

> **تذکر:** استفاده از دستگاه `"mps"` ممکن است باعث تفاوت‌های عددی نسبت به محتوای این فصل شود، زیرا پشتیبانی PyTorch از Apple Silicon هنوز در مرحله آزمایشی است.

قبلاً انتقال داده‌ها به دستگاه هدف (مثلاً حافظه GPU در صورت `"cuda"`) در حلقه اصلی آموزش انجام می‌شد. اضافه کردن این فرآیند به تابع collate مزیت این را دارد که انتقال داده‌ها به صورت فرایند پس‌زمینه انجام شده و باعث بلوکه شدن GPU در طول آموزش نمی‌شود.

---

کد زیر متغیر `device` را مقداردهی می‌کند:

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# if torch.backends.mps.is_available():   #1
#     device = torch.device("mps")
print("Device:", device)
```

> #1: با برداشتن کامنت این دو خط می‌توانید از GPU روی تراشه Apple Silicon استفاده کنید.

خروجی این کد ممکن است `Device: cpu` یا `Device: cuda` باشد، بسته به سخت‌افزار شما.

---

برای استفاده مجدد از تنظیم دستگاه در `custom_collate_fn` زمانی که آن را در کلاس `DataLoader` می‌گذاریم، از تابع `partial` از کتابخانه استاندارد `functools` استفاده می‌کنیم تا نسخه‌ای از تابع بسازیم که آرگومان `device` از پیش مقداردهی شده باشد. همچنین، `allowed_max_length` را روی ۱۰۲۴ تنظیم می‌کنیم تا داده‌ها به طول زمینه‌ی قابل پشتیبانی توسط مدل GPT-2 (که بعداً آن را فاین‌تیون می‌کنیم) برش داده شوند:

```python
from functools import partial
customized_collate_fn = partial(
    custom_collate_fn,
    device=device,
    allowed_max_length=1024
)
```

---

سپس می‌توانیم داده‌بارکن‌ها را مانند قبل تنظیم کنیم، با این تفاوت که این بار از تابع collate سفارشی برای دسته‌بندی استفاده می‌کنیم.

---

**فهرست ۷.۶: مقداردهی داده‌بارکن‌ها**

```python
from torch.utils.data import DataLoader

num_workers = 0      #1
batch_size = 8

torch.manual_seed(123)

train_dataset = InstructionDataset(train_data, tokenizer)
train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=True,
    drop_last=True,
    num_workers=num_workers
)

val_dataset = InstructionDataset(val_data, tokenizer)
val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=False,
    drop_last=False,
    num_workers=num_workers
)

test_dataset = InstructionDataset(test_data, tokenizer)
test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    collate_fn=customized_collate_fn,
    shuffle=False,
    drop_last=False,
    num_workers=num_workers
)
```

> #1: در صورت پشتیبانی سیستم عامل از فرایندهای موازی پایتون، می‌توانید این عدد را افزایش دهید.

---

بیایید ابعاد دسته‌های ورودی و هدف تولید شده توسط `train_loader` را بررسی کنیم:

```python
print("Train loader:")
for inputs, targets in train_loader:
    print(inputs.shape, targets.shape)
```

خروجی نمونه (برای صرفه‌جویی در فضا کوتاه شده):

```
Train loader:
torch.Size([8, 61]) torch.Size([8, 61])
torch.Size([8, 76]) torch.Size([8, 76])
torch.Size([8, 73]) torch.Size([8, 73])
...
torch.Size([8, 74]) torch.Size([8, 74])
torch.Size([8, 69]) torch.Size([8, 69])
```

این خروجی نشان می‌دهد که اولین دسته ورودی و هدف دارای ابعاد ۸ × ۶۱ است، که عدد ۸ اندازه دسته (batch size) و ۶۱ تعداد توکن‌های هر نمونه آموزشی در آن دسته است. دسته‌های بعدی تعداد توکن متفاوتی دارند (مثلاً ۷۶). به لطف تابع collate سفارشی، داده‌بارکن می‌تواند دسته‌هایی با طول‌های متفاوت ایجاد کند.

---

## ۷.۵ بارگذاری مدل زبان بزرگ پیش‌آموزش دیده

بخش قابل توجهی از زمان را صرف آماده‌سازی داده‌ها برای آموزش دقیق بر اساس دستورالعمل‌ها کردیم، که بخشی کلیدی از فرایند آموزش نظارت شده است. بسیاری از جنبه‌های دیگر مشابه پیش‌آموزش هستند که اجازه می‌دهد بخش بزرگی از کدهای فصل‌های قبلی را مجدداً استفاده کنیم.

قبل از شروع آموزش دقیق با دستورالعمل‌ها، باید ابتدا مدل GPT پیش‌آموزش دیده‌ای که قصد فاین‌تیون آن را داریم بارگذاری کنیم (شکل ۷.۱۵). این کار قبلاً انجام شده است، اما به جای مدل کوچک ۱۲۴ میلیون پارامتری، این بار مدل متوسط با ۳۵۵ میلیون پارامتر را بارگذاری می‌کنیم. دلیل این انتخاب این است که مدل ۱۲۴ میلیون پارامتری ظرفیت کافی برای کسب نتایج رضایت‌بخش در آموزش دقیق دستورالعمل‌ها ندارد. به طور مشخص، مدل‌های کوچک‌تر توانایی یادگیری و حفظ الگوها و رفتارهای پیچیده مورد نیاز برای انجام وظایف دقیق دستورالعملی را ندارند.

---

**شکل ۷.۱۵**
فرآیند سه مرحله‌ای آموزش دقیق مدل زبان بزرگ: پس از آماده‌سازی داده‌ها، آموزش دقیق با بارگذاری مدل پیش‌آموزش دیده آغاز می‌شود که پایه‌ای برای آموزش‌های بعدی است.

---

برای بارگذاری مدل، از همان کدهای استفاده شده در فصل ۵ (پیش‌آموزش) و فصل ۶ (آموزش طبقه‌بندی) استفاده می‌کنیم، با این تفاوت که این بار `"gpt2-medium (355M)"` را به جای `"gpt2-small (124M)"` انتخاب می‌کنیم.

> **تذکر:** اجرای این کد باعث دانلود مدل GPT متوسط می‌شود که حدود ۱.۴۲ گیگابایت فضا نیاز دارد و تقریباً سه برابر مدل کوچک حجم دارد.

---

**فهرست ۷.۷: بارگذاری مدل پیش‌آموزش دیده**

```python
from gpt_download import download_and_load_gpt2
from chapter04 import GPTModel
from chapter05 import load_weights_into_gpt

BASE_CONFIG = {
    "vocab_size": 50257,     # اندازه واژگان
    "context_length": 1024,  # طول زمینه
    "drop_rate": 0.0,        # نرخ dropout
    "qkv_bias": True         # بایاس برای query-key-value
}

model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}

CHOOSE_MODEL = "gpt2-medium (355M)"
BASE_CONFIG.update(model_configs[CHOOSE_MODEL])

model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")

settings, params = download_and_load_gpt2(
    model_size=model_size,
    models_dir="gpt2"
)

model = GPTModel(BASE_CONFIG)
load_weights_into_gpt(model, params)
model.eval()
```

---

پس از اجرای این کد، چندین فایل دانلود خواهند شد:

```
checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 156kiB/s]
encoder.json: 100%|██████████| 1.04M/1.04M [00:02<00:00, 467kiB/s]
hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 198kiB/s]
model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G
[05:50<00:00, 4.05MiB/s]
model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 18.1MiB/s]
model.ckpt.meta: 100%|██████████| 927k/927k [00:02<00:00, 454kiB/s]
vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 283kiB/s]
```

---

حال بیایید عملکرد مدل پیش‌آموزش دیده را روی یکی از وظایف اعتبارسنجی با مقایسه خروجی مدل با پاسخ مورد انتظار بسنجیم. این کار یک خط مبنا برای عملکرد مدل قبل از آموزش دقیق ایجاد می‌کند و کمک می‌کند تأثیر آموزش دقیق را بهتر درک کنیم. برای این کار از اولین نمونه در مجموعه اعتبارسنجی استفاده می‌کنیم:

```python
torch.manual_seed(123)
input_text = format_input(val_data[0])
print(input_text)
```

محتوای دستورالعمل به شرح زیر است:

```
Below is an instruction that describes a task. Write a response that
appropriately completes the request.
### Instruction:
Convert the active sentence to passive: 'The chef cooks the meal every day.'
```

---

سپس پاسخ مدل را با استفاده از همان تابع `generate` که در فصل ۵ برای پیش‌آموزش استفاده کردیم، تولید می‌کنیم:

```python
from chapter05 import generate, text_to_token_ids, token_ids_to_text

token_ids = generate(
    model=model,
    idx=text_to_token_ids(input_text, tokenizer),
    max_new_tokens=35,
    context_size=BASE_CONFIG["context_length"],
    eos_id=50256,
)

generated_text = token_ids_to_text(token_ids, tokenizer)
```

تابع `generate` متن ترکیبی ورودی و خروجی را برمی‌گرداند. این رفتار برای مدل‌های پیش‌آموزش دیده که عمدتاً برای تکمیل متن طراحی شده‌اند، مناسب است، زیرا ورودی و خروجی به هم متصل شده و متن یکپارچه و قابل فهم تولید می‌شود. اما برای ارزیابی عملکرد مدل در یک وظیفه خاص، معمولاً می‌خواهیم تنها پاسخ تولید شده توسط مدل را بررسی کنیم.

---

برای جداسازی پاسخ مدل، طول متن ورودی را از ابتدای `generated_text` کم می‌کنیم:

```python
response_text = generated_text[len(input_text):].strip()
print(response_text)
```

این کد متن ورودی را از ابتدای متن تولید شده حذف می‌کند و فقط پاسخ تولید شده توسط مدل باقی می‌ماند. تابع `strip()` نیز فاصله‌های اضافی ابتدا و انتهای متن را حذف می‌کند.

---

خروجی به شکل زیر خواهد بود:

```
### Response:
The chef cooks the meal every day.
### Instruction:
Convert the active sentence to passive: 'The chef cooks the
```

این خروجی نشان می‌دهد که مدل پیش‌آموزش دیده هنوز قادر نیست دستورالعمل داده شده را به درستی دنبال کند. اگرچه بخشی با عنوان پاسخ تولید کرده، اما صرفاً جمله اصلی و بخشی از دستور را تکرار کرده و نتوانسته جمله فعال را به مجهول تبدیل کند.

---

## 7.6 آموزش دقیق (Fine-tuning) مدل زبانی بزرگ (LLM) با داده‌های دستوری

اکنون زمان آن رسیده که مدل زبانی بزرگ (LLM) را برای اجرای دستورات دقیق‌تر آموزش دهیم (شکل 7.16). در این مرحله، مدل پیش‌آموزش‌دیده‌ای که در بخش قبلی بارگذاری کردیم را بر اساس مجموعه داده‌های دستوری که در ابتدای این فصل آماده کرده‌ایم، آموزش می‌دهیم. بخش سخت کار یعنی پردازش داده‌های دستوری قبلاً انجام شده است. برای فرآیند fine-tuning می‌توانیم از توابع محاسبه خطا و آموزش که در فصل 5 پیاده‌سازی شده‌اند، مجدداً استفاده کنیم:

```python
from chapter05 import (
    calc_loss_loader,
    train_model_simple
)
```

قبل از شروع آموزش، ابتدا مقدار اولیه خطا (loss) را برای مجموعه‌های آموزش و اعتبارسنجی محاسبه می‌کنیم:

```python
model.to(device)
torch.manual_seed(123)
with torch.no_grad():
    train_loss = calc_loss_loader(
        train_loader, model, device, num_batches=5
    )
    val_loss = calc_loss_loader(
        val_loader, model, device, num_batches=5
    )
print("Training loss:", train_loss)
print("Validation loss:", val_loss)
```

مقادیر اولیه خطا به شرح زیر است؛ هدف ما همانند قبل کاهش این مقادیر است:

```
Training loss: 3.825908660888672
Validation loss: 3.7619335651397705
```

---

#### مقابله با محدودیت‌های سخت‌افزاری

استفاده و آموزش مدل‌های بزرگ‌تر مانند GPT-2 medium (با ۳۵۵ میلیون پارامتر) نسبت به مدل‌های کوچک‌تر مانند GPT-2 small (با ۱۲۴ میلیون پارامتر) نیازمند منابع محاسباتی بیشتری است. اگر با محدودیت سخت‌افزاری مواجه شدید، می‌توانید مدل را به نسخه کوچکتر تغییر دهید:

```python
CHOOSE_MODEL = "gpt2-small (124M)"
```

(به بخش 7.5 مراجعه کنید.)
همچنین برای تسریع آموزش، استفاده از GPU توصیه می‌شود. در مخزن کد این کتاب، بخش تکمیلی روش‌های مختلف استفاده از GPUهای ابری ذکر شده است:
[https://mng.bz/EOEq](https://mng.bz/EOEq)

---

جدول زیر زمان‌های تقریبی آموزش مدل‌ها بر روی دستگاه‌های مختلف (CPU و GPU) را نشان می‌دهد. اجرای این کد روی GPU سازگار نیاز به تغییر کد ندارد و آموزش را به‌طور قابل توجهی سریع‌تر می‌کند. برای نتایج ارائه شده در این فصل، از مدل GPT-2 medium و کارت گرافیک A100 استفاده شده است.

| مدل                | دستگاه               | زمان آموزش دو اپوک |
| ------------------ | -------------------- | ------------------ |
| gpt2-medium (355M) | CPU (M3 MacBook Air) | 15.78 دقیقه        |
| gpt2-medium (355M) | GPU (NVIDIA L4)      | 1.83 دقیقه         |
| gpt2-medium (355M) | GPU (NVIDIA A100)    | 0.86 دقیقه         |
| gpt2-small (124M)  | CPU (M3 MacBook Air) | 5.74 دقیقه         |
| gpt2-small (124M)  | GPU (NVIDIA L4)      | 0.69 دقیقه         |
| gpt2-small (124M)  | GPU (NVIDIA A100)    | 0.39 دقیقه         |

---

#### آموزش مدل

پس از آماده شدن مدل و بارگذارهای داده، اکنون می‌توانیم فرآیند آموزش را آغاز کنیم. کد زیر (لیست 7.8) فرایند آموزش را با مقداردهی اولیه به بهینه‌ساز (optimizer)، تعیین تعداد اپوک‌ها و تعریف فرکانس ارزیابی و متن شروع برای بررسی پاسخ‌های مدل در حین آموزش، تنظیم می‌کند. متن شروع بر اساس اولین نمونه از مجموعه اعتبارسنجی (val_data[0]) است که در بخش 7.5 بررسی کردیم.

```python
import time
start_time = time.time()
torch.manual_seed(123)
optimizer = torch.optim.AdamW(
    model.parameters(), lr=0.00005, weight_decay=0.1
)
num_epochs = 2
train_losses, val_losses, tokens_seen = train_model_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs=num_epochs, eval_freq=5, eval_iter=5,
    start_context=format_input(val_data[0]), tokenizer=tokenizer
)
end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")
```

---

#### خروجی آموزش و تحلیل آن

خروجی آموزش طی دو اپوک نشان می‌دهد که مقدار خطا به طور پیوسته کاهش یافته است، که بیانگر بهبود مدل در پیروی از دستورات و تولید پاسخ‌های مناسب است:

```
Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626
Ep 1 (Step 000005): Train loss 1.174, Val loss 1.103
Ep 1 (Step 000010): Train loss 0.872, Val loss 0.944
Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906
...
Ep 1 (Step 000115): Train loss 0.520, Val loss 0.665
```

نمونه‌ای از دستور و پاسخ تولید شده:

```
Below is an instruction that describes a task. Write a response that
appropriately completes the request.
### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'
### Response: The meal is prepared every day by the chef.<|endoftext|>
```

ادامه روند آموزش در اپوک دوم:

```
Ep 2 (Step 000120): Train loss 0.438, Val loss 0.670
Ep 2 (Step 000125): Train loss 0.453, Val loss 0.685
Ep 2 (Step 000130): Train loss 0.448, Val loss 0.681
Ep 2 (Step 000135): Train loss 0.408, Val loss 0.677
...
Ep 2 (Step 000230): Train loss 0.300, Val loss 0.657
```

نمونه‌ای از پاسخ:

```
Below is an instruction that describes a task. Write a response
that appropriately completes the request.
### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'
### Response: The meal is cooked every day by the chef.<|endoftext|>
```

---

#### تحلیل نتایج آموزش

مدل به خوبی در حال یادگیری است، چرا که خطاهای آموزش و اعتبارسنجی به صورت مداوم در دو اپوک کاهش یافته‌اند. این موضوع نشان می‌دهد مدل به تدریج در درک و اجرای دقیق دستورات بهبود یافته است. از آنجا که مدل در دو اپوک عملکرد مناسبی نشان داده، آموزش بیشتر احتمالاً ضروری نیست و ممکن است منجر به بیش‌برازش (overfitting) شود.

پاسخ‌های تولید شده در پایان هر اپوک نیز امکان بررسی پیشرفت مدل در اجرای صحیح دستورات را فراهم می‌کند. برای مثال، مدل جمله فعال «The chef cooks the meal every day.» را به صورت مجهول «The meal is cooked every day by the chef.» تبدیل کرده است.

---

#### رسم نمودار کاهش خطا

برای بررسی دقیق‌تر روند یادگیری، نمودارهای کاهش خطای آموزش و اعتبارسنجی را رسم می‌کنیم. برای این کار از تابع `plot_losses` که قبلاً در پیش‌آموزش استفاده شده بود، بهره می‌بریم:

```python
from chapter05 import plot_losses
epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)
```

نمودار (شکل 7.17) نشان می‌دهد که عملکرد مدل در هر دو مجموعه داده آموزش و اعتبارسنجی در طول آموزش به طور قابل توجهی بهبود یافته است. کاهش سریع خطا در ابتدای آموزش بیانگر یادگیری سریع الگوها و نمایش‌های معنادار از داده‌ها است. سپس در اپوک دوم، کاهش خطا ادامه دارد اما با سرعت کمتری که نشان‌دهنده تنظیم دقیق‌تر مدل و همگرایی به یک راه‌حل پایدار است.

---

**تمرین 7.3: آموزش دقیق مدل با مجموعه داده Alpaca اصلی**

مجموعه داده Alpaca که توسط پژوهشگران دانشگاه استنفورد تهیه شده، یکی از اولین و محبوب‌ترین مجموعه‌های داده دستوری است و شامل ۵۲,۰۰۲ نمونه می‌باشد. به جای استفاده از فایل `instruction-data.json` که در این فصل استفاده کردیم، می‌توانید مدل زبانی خود را با استفاده از این مجموعه داده آموزش دهید. این مجموعه داده از این لینک قابل دریافت است:
[https://mng.bz/NBnE](https://mng.bz/NBnE)

این مجموعه داده حدود ۵۰ برابر بیشتر از داده‌های مورد استفاده در این فصل ورودی دارد و نمونه‌های آن معمولاً طولانی‌تر هستند. بنابراین، پیشنهاد می‌شود برای آموزش از GPU استفاده کنید تا سرعت فرایند افزایش یابد. در صورت بروز خطاهای کمبود حافظه (OOM)، می‌توانید اندازه دسته (batch_size) را از ۸ به ۴، ۲ یا حتی ۱ کاهش دهید. همچنین کاهش مقدار `allowed_max_length` از ۱۰۲۴ به ۵۱۲ یا ۲۵۶ می‌تواند در مدیریت حافظه کمک‌کننده باشد.

## 7.7 استخراج و ذخیره پاسخ‌ها

پس از انجام **فاین‌تیونینگ** مدل زبانی بزرگ (LLM) روی بخش آموزش داده‌های دستوری، اکنون آماده‌ایم تا عملکرد آن را روی مجموعه آزمایشی نگه‌داشته‌شده (test set) ارزیابی کنیم. ابتدا پاسخ‌های تولیدشده توسط مدل برای هر ورودی در داده‌های آزمایشی استخراج و جمع‌آوری می‌شوند تا تحلیل دستی انجام شود، سپس مدل برای کمی‌سازی کیفیت پاسخ‌ها ارزیابی می‌شود (شکل 7.18).

---

**شکل 7.18**  
فرآیند سه‌مرحله‌ای برای فاین‌تیونینگ دستورالعملی LLM. در دو گام اول مرحله سوم، پاسخ‌های مدل روی مجموعه آزمایشی استخراج و جمع‌آوری شده و سپس مدل برای کمی‌سازی عملکرد ارزیابی می‌شود.

---

برای تکمیل مرحله پاسخ‌دهی به دستورات، از تابع `generate` استفاده می‌کنیم. سپس پاسخ‌های مدل را در کنار پاسخ‌های مورد انتظار برای سه نمونه اول مجموعه آزمایشی چاپ می‌کنیم تا امکان مقایسه فراهم شود:

```python
torch.manual_seed(123)
for entry in test_data[:3]:      #1
    input_text = format_input(entry)
    token_ids = generate(               #2
        model=model,
        idx=text_to_token_ids(input_text, tokenizer).to(device),
        max_new_tokens=256,
        context_size=BASE_CONFIG["context_length"],
        eos_id=50256
    )
    generated_text = token_ids_to_text(token_ids, tokenizer)
    response_text = (
        generated_text[len(input_text):]
        .replace("### Response:", "")
        .strip()
    )
    print(input_text)
    print(f"\nCorrect response:\\n>> {entry['output']}")
    print(f"\nModel response:\\n>> {response_text.strip()}")
    print("-------------------------------------")
```

> #1 حلقه روی سه نمونه اول مجموعه آزمایشی  
> #2 استفاده از تابع generate که در بخش 7.5 وارد شده است

تابع `generate` متن ترکیبی ورودی و خروجی را بازمی‌گرداند؛ بنابراین برای استخراج پاسخ مدل، از برش رشته و تابع `.replace()` روی متن تولیدشده استفاده می‌کنیم. دستورالعمل‌ها، پاسخ‌های داده‌شده و پاسخ‌های مدل به ترتیب نمایش داده می‌شوند.

بر اساس دستورات، پاسخ‌های داده‌شده و پاسخ‌های مدل، عملکرد مدل نسبتاً خوب است. پاسخ‌های دستور اول و آخر کاملاً صحیح هستند، اما پاسخ دستور دوم نزدیک به درست است ولی دقیق نیست. مدل به جای "cumulonimbus" پاسخ "cumulus cloud" داده است که البته لازم به ذکر است ابرهای cumulus می‌توانند به cumulonimbus تبدیل شوند که مسئول ایجاد رعد و برق هستند.

---

#### ارزیابی مدل

ارزیابی مدل به سادگی محاسبه درصد دقت در دسته‌بندی‌های اسپم/غیر اسپم نیست. در عمل، مدل‌های LLM فاین‌تیون‌شده بر اساس دستورالعمل‌ها مانند چت‌بات‌ها، با روش‌های مختلف ارزیابی می‌شوند:

- آزمون‌های پاسخ کوتاه و چندگزینه‌ای مانند MMLU (Measuring Massive Multitask Language Understanding) که دانش عمومی مدل را می‌سنجند.
- مقایسه ترجیح انسانی نسبت به مدل‌های دیگر، مانند LMSYS chatbot arena.
- معیارهای خودکار مکالمه‌ای که در آن مدل‌های دیگر (مانند GPT-4) پاسخ‌ها را ارزیابی می‌کنند، مانند AlpacaEval.

در عمل، ترکیبی از این سه روش — آزمون‌های چندگزینه‌ای، ارزیابی انسانی و معیارهای خودکار مکالمه‌ای — می‌تواند مفید باشد. اما از آنجا که هدف اصلی ما ارزیابی عملکرد مکالمه‌ای مدل است نه فقط پاسخ به سوالات چندگزینه‌ای، ارزیابی انسانی و معیارهای خودکار اهمیت بیشتری دارند.

---

#### عملکرد مکالمه‌ای

عملکرد مکالمه‌ای به توانایی مدل در برقراری ارتباط انسانی مانند اشاره دارد که شامل درک زمینه، ظرافت‌ها و نیت است. این مهارت‌ها شامل ارائه پاسخ‌های مرتبط و منسجم، حفظ ثبات و تطبیق با موضوعات و سبک‌های مختلف گفتگو می‌شود.

ارزیابی انسانی، هرچند ارزشمند است، اما زمان‌بر و پرزحمت است، خصوصاً با تعداد زیاد پاسخ‌ها. مثلاً خواندن و رتبه‌بندی ۱۱۰۰ پاسخ کار بسیار سنگینی است.

با توجه به حجم کاری، ما روشی مشابه معیارهای خودکار مکالمه‌ای اجرا خواهیم کرد که پاسخ‌ها را به صورت خودکار توسط یک مدل زبانی دیگر ارزیابی می‌کند. این روش به ما امکان می‌دهد کیفیت پاسخ‌های تولیدشده را به‌طور کارآمد و بدون دخالت انسانی گسترده بسنجیم و در عین حال شاخص‌های عملکرد معنی‌دار به دست آوریم.

---

#### ارزیابی با مدل دیگر و ذخیره پاسخ‌ها

روشی مشابه AlpacaEval را به کار می‌بریم، اما به جای استفاده از مجموعه داده معیار عمومی، از مجموعه آزمایشی سفارشی خودمان استفاده می‌کنیم. این کار امکان ارزیابی دقیق‌تر و مرتبط‌تر با کاربردهای هدف ما را فراهم می‌کند.

برای آماده‌سازی پاسخ‌ها جهت ارزیابی، پاسخ‌های تولیدشده مدل را به دیکشنری `test_set` اضافه می‌کنیم و داده‌های به‌روزشده را در فایل "instruction-data-with-response.json" ذخیره می‌کنیم تا قابلیت بارگذاری و تحلیل در جلسات مختلف فراهم شود.

کد زیر، مشابه قبل، از تابع `generate` استفاده می‌کند؛ اما روی کل `test_set` حلقه زده و به جای چاپ، پاسخ‌ها را به داده‌ها اضافه می‌کند:

```python
from tqdm import tqdm
for i, entry in tqdm(enumerate(test_data), total=len(test_data)):
    input_text = format_input(entry)
    token_ids = generate(
        model=model,
        idx=text_to_token_ids(input_text, tokenizer).to(device),
        max_new_tokens=256,
        context_size=BASE_CONFIG["context_length"],
        eos_id=50256
    )
    generated_text = token_ids_to_text(token_ids, tokenizer)
    response_text = (
        generated_text[len(input_text):]
        .replace("### Response:", "")
        .strip()
    )
    test_data[i]["model_response"] = response_text
with open("instruction-data-with-response.json", "w") as file:
    json.dump(test_data, file, indent=4)         #1
```

> #1 برای فرمت‌بندی خواناتر JSON

پردازش این داده‌ها حدود ۱ دقیقه روی GPU مدل A100 و حدود ۶ دقیقه روی لپ‌تاپ M3 MacBook Air طول می‌کشد:

```
100%|██████████| 110/110 [01:05<00:00,  1.68it/s]
```

برای اطمینان از اضافه شدن پاسخ‌ها، یکی از نمونه‌ها را بررسی می‌کنیم:

```python
print(test_data[0])
```

خروجی نمونه‌ای به شکل زیر خواهد بود که نشان می‌دهد `model_response` به درستی افزوده شده است:

```python
{'instruction': 'Rewrite the sentence using a simile.',
 'input': 'The car is very fast.',
 'output': 'The car is as fast as lightning.',
 'model_response': 'The car is as fast as a bullet.'}
```

---

#### ذخیره مدل

در نهایت، مدل را با نام `gpt2-medium355M-sft.pth` ذخیره می‌کنیم تا بتوان در پروژه‌های آینده از آن استفاده کرد:

```python
import re
file_name = f"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth"      #1
torch.save(model.state_dict(), file_name)
print(f"Model saved as {file_name}")
```

> #1 حذف فاصله و پرانتزها از نام فایل

مدل ذخیره‌شده را می‌توان با دستور زیر بارگذاری کرد:

```python
model.load_state_dict(torch.load("gpt2-medium355M-sft.pth"))
```

## 7.8 ارزیابی مدل زبان بزرگ (LLM) با تنظیم دقیق

قبلاً عملکرد مدل آموزش‌دیده روی دستورالعمل را با بررسی پاسخ‌های آن در سه نمونه از مجموعه داده تست ارزیابی کردیم. این روش دید کلی از کیفیت مدل می‌دهد، اما برای تعداد زیاد پاسخ‌ها مناسب نیست. بنابراین، روشی برای خودکارسازی ارزیابی پاسخ‌های مدل آموزش‌دیده با استفاده از یک مدل بزرگ‌تر و قوی‌تر پیاده‌سازی می‌کنیم که در شکل ۷.۱۹ نشان داده شده است.

**شکل ۷.۱۹**  
فرآیند سه‌مرحله‌ای آموزش دقیق مدل زبان بزرگ (LLM). در آخرین مرحله، روشی برای سنجش عملکرد مدل آموزش‌دیده با نمره‌دهی به پاسخ‌های تولیدشده روی مجموعه تست پیاده‌سازی می‌شود.

---

#### ارزیابی خودکار پاسخ‌های مجموعه تست

برای این منظور از مدل Llama 3 با ۸ میلیارد پارامتر که توسط Meta AI آموزش دقیق شده و قابل اجرا به صورت محلی با برنامه متن‌باز Ollama (https://ollama.com) است، استفاده می‌کنیم.

> **توجه:**  
> Ollama برنامه‌ای کارآمد برای اجرای LLMها روی لپ‌تاپ است که به‌عنوان رابطی روی کتابخانه متن‌باز llama.cpp (https://github.com/ggerganov/llama.cpp) عمل می‌کند. این کتابخانه مدل‌ها را به زبان C/C++ پیاده‌سازی کرده تا بهینه‌ترین عملکرد را داشته باشد. اما Ollama فقط برای تولید متن (Inference) است و امکان آموزش یا تنظیم دقیق مدل‌ها را ندارد.

---

#### استفاده از مدل‌های بزرگ‌تر از طریق API وب

مدل Llama 3 هشت میلیارد پارامتری مدل قوی‌ای است که به صورت محلی اجرا می‌شود، اما در مقایسه با مدل‌های اختصاصی بزرگ‌تر مانند GPT-4 از OpenAI توانایی کمتری دارد. اگر علاقه‌مند به استفاده از GPT-4 از طریق API OpenAI برای ارزیابی پاسخ‌های تولیدشده هستید، دفترچه کدی اختیاری در مواد مکمل کتاب به آدرس https://mng.bz/BgEv موجود است.

---

#### نصب و راه‌اندازی Ollama

برای اجرای کدهای بعدی، Ollama را از https://ollama.com دانلود و نصب کنید:

- **کاربران macOS و ویندوز:** برنامه Ollama را اجرا کنید و در صورت درخواست نصب قابلیت استفاده از خط فرمان، گزینه Yes را انتخاب کنید.
- **کاربران لینوکس:** دستورالعمل نصب در وب‌سایت Ollama موجود است.

---

#### دانلود مدل Llama 3 و اطمینان از عملکرد Ollama

برای استفاده از Ollama در خط فرمان، باید یا برنامه Ollama را اجرا کنید یا در ترمینال جداگانه دستور `ollama serve` را اجرا کنید (شکل ۷.۲۰).

**شکل ۷.۲۰**  
دو گزینه برای اجرای Ollama:

- سمت چپ: اجرای `ollama serve` در ترمینال
- سمت راست: اجرای برنامه Ollama در پس‌زمینه در macOS

---

#### اجرای مدل در خط فرمان

پس از اجرای Ollama، دستور زیر را در خط فرمان (نه در محیط پایتون) اجرا کنید:

```bash
ollama run llama3
```

در اولین اجرا، مدل به حجم ۴.۷ گیگابایت به صورت خودکار دانلود می‌شود و خروجی مشابه زیر نمایش داده می‌شود:

```
pulling manifest
pulling 6a0746a1ec1a... 100% |████████████████| 4.7 GB
...
success
```

---

#### مدل‌های جایگزین Ollama

مدل `llama3` در دستور بالا به مدل Llama 3 با ۸ میلیارد پارامتر و آموزش دقیق روی دستورالعمل‌ها اشاره دارد که حدود ۱۶ گیگابایت حافظه RAM نیاز دارد. اگر حافظه کافی ندارید، می‌توانید از مدل کوچکتر `phi3` با ۳.۸ میلیارد پارامتر استفاده کنید که نیاز به حدود ۸ گیگابایت RAM دارد:

```bash
ollama run phi3
```

برای سیستم‌های قدرتمندتر، مدل بزرگ‌تر ۷۰ میلیارد پارامتری Llama 3 با نام `llama3:70b` نیز موجود است که منابع محاسباتی بسیار بیشتری نیاز دارد.

---

#### نمونه تعامل با مدل

پس از آماده شدن مدل، می‌توانید پرسشی مانند زیر را مطرح کنید:

```
>>> What do llamas eat?
```

پاسخ نمونه:

> لاماها حیوانات نشخوارکننده با معده چهارحجمی هستند و گیاهان با فیبر بالا می‌خورند. در طبیعت معمولاً از چمن‌ها، گندم، جو و جوی دوسر تغذیه می‌کنند.

برای پایان دادن به جلسه `ollama run llama3` می‌توانید دستور `/bye` را وارد کنید، اما توجه کنید که Ollama یا `ollama serve` باید برای ادامه کار در این فصل در حال اجرا باشند.

---

#### بررسی وضعیت اجرای Ollama در پایتون

کد زیر بررسی می‌کند که Ollama در سیستم فعال باشد:

```python
import psutil

def check_if_running(process_name):
    running = False
    for proc in psutil.process_iter(["name"]):
        if process_name in proc.info["name"]:
            running = True
            break
    return running

ollama_running = check_if_running("ollama")
if not ollama_running:
    raise RuntimeError("Ollama not running. Launch ollama before proceeding.")

print("Ollama running:", check_if_running("ollama"))
```

خروجی باید به شکل زیر باشد:

```
Ollama running: True
```

---

#### بارگذاری داده‌های تست و تعریف قالب ورودی

اگر جلسه پایتون خود را بسته‌اید یا می‌خواهید در جلسه جدید کد را اجرا کنید، ابتدا داده‌های تست را بارگذاری و تابع `format_input` را تعریف کنید:

```python
import json
from tqdm import tqdm

file_path = "instruction-data-with-response.json"
with open(file_path, "r") as file:
    test_data = json.load(file)

def format_input(entry):
    instruction_text = (
        f"Below is an instruction that describes a task. "
        f"Write a response that appropriately completes the request."
        f"\n\\n### Instruction:\\n{entry['instruction']}"
    )
    input_text = (
        f"\n\\n### Input:\\n{entry['input']}" if entry["input"] else ""
    )
    return instruction_text + input_text
```

---

#### استفاده از REST API Ollama با پایتون

به جای اجرای دستور `ollama run`، می‌توان با استفاده از API محلی Ollama از مدل سؤال پرسید. کد زیر تابعی برای این کار نشان می‌دهد:

```python
import urllib.request
import json

def query_model(
    prompt,
    model="llama3",
    url="http://localhost:11434/api/chat"
):
    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "options": {
            "seed": 123,
            "temperature": 0,
            "num_ctx": 2048
        }
    }
    payload = json.dumps(data).encode("utf-8")
    request = urllib.request.Request(
        url,
        data=payload,
        method="POST"
    )
    request.add_header("Content-Type", "application/json")
    response_data = ""
    with urllib.request.urlopen(request) as response:
        while True:
            line = response.readline().decode("utf-8")
            if not line:
                break
            response_json = json.loads(line)
            response_data += response_json["message"]["content"]
    return response_data
```

---

#### مثال استفاده از `query_model`

```python
model = "llama3"
result = query_model("What do Llamas eat?", model)
print(result)
```

نمونه پاسخ:

> لاماها حیوانات نشخوارکننده با معده چهارحجمی هستند که قادر به هضم غذاهای گیاهی‌اند. رژیم غذایی آن‌ها معمولاً شامل چمن‌ها، علف‌ها و حتی علف‌های هرز است.

---

#### ارزیابی پاسخ‌های مدل آموزش‌دیده با مدل بزرگ‌تر

می‌توانیم پاسخ‌های مدل تنظیم دقیق شده خود را از نظر کیفیت به وسیله مدل Llama 3 ارزیابی کنیم. به این صورت که مدل Llama 3 نمره‌ای بین ۰ تا ۱۰۰ بر اساس مقایسه با پاسخ مرجع اختصاص می‌دهد.

برای نمونه، سه پاسخ اول مجموعه تست را ارزیابی می‌کنیم:

```python
for entry in test_data[:3]:
    prompt = (
        f"Given the input `{format_input(entry)}` "
        f"and correct output `{entry['output']}`, "
        f"score the model response `{entry['model_response']}`"
        f" on a scale from 0 to 100, where 100 is the best score. "
    )
    print("\nDataset response:")
    print(">>", entry['output'])
    print("\nModel response:")
    print(">>", entry["model_response"])
    print("\nScore:")
    print(">>", query_model(prompt))
    print("\n-------------------------")
```

نتایج نشان می‌دهد که مدل Llama 3 توانایی اختصاص نمره منطقی دارد و حتی پاسخ‌های نیمه‌درست را تا حدی تشخیص می‌دهد.

---

#### تولید نمرات عددی و محاسبه میانگین

برای دریافت فقط نمره عددی صحیح، پرامپت را به گونه‌ای تغییر می‌دهیم که فقط عدد صحیح را بازگرداند. تابع زیر این کار را انجام می‌دهد:

```python
def generate_model_scores(json_data, json_key, model="llama3"):
    scores = []
    for entry in tqdm(json_data, desc="Scoring entries"):
        prompt = (
            f"Given the input `{format_input(entry)}` "
            f"and correct output `{entry['output']}`, "
            f"score the model response `{entry[json_key]}`"
            f" on a scale from 0 to 100, where 100 is the best score. "
            f"Respond with the integer number only."
        )
        score = query_model(prompt, model)
        try:
            scores.append(int(score))
        except ValueError:
            print(f"Could not convert score: {score}")
            continue
    return scores
```

سپس این تابع را روی کل مجموعه تست اجرا می‌کنیم:

```python
scores = generate_model_scores(test_data, "model_response")
print(f"Number of scores: {len(scores)} of {len(test_data)}")
print(f"Average score: {sum(scores)/len(scores):.2f}\n")
```

خروجی نمونه:

```
Scoring entries: 100%|████████████████████████| 110/110
Number of scores: 110 of 110
Average score: 50.32
```

این میانگین نشان‌دهنده عملکرد مدل تنظیم دقیق شده ما است که معیاری برای مقایسه یا بهبود مدل فراهم می‌کند.

---

#### نکات مهم

- Ollama در حال حاضر کاملاً قطعی (deterministic) نیست و ممکن است نمرات اندکی متفاوت باشند. برای نتایج قوی‌تر، ارزیابی را چند بار تکرار و میانگین بگیرید.
- برای بهبود عملکرد مدل می‌توان:

  - ابرپارامترهای تنظیم دقیق مانند نرخ یادگیری، اندازه بچ، یا تعداد اپوک‌ها را تغییر داد.
  - حجم یا تنوع داده‌های آموزشی را افزایش داد.
  - قالب پرامپت‌ها و دستورالعمل‌ها را برای هدایت بهتر مدل اصلاح کرد.
  - از مدل‌های پیش‌آموزش بزرگ‌تر استفاده کرد تا الگوهای پیچیده‌تر را بهتر یاد بگیرد.

---

#### یادداشت

- مدل پایه Llama 3 با ۸ میلیارد پارامتر بدون تنظیم دقیق، نمره متوسط ۵۸.۵۱ در مجموعه تست کسب می‌کند.
- مدل Llama 3 8B instruct که روی مجموعه داده دستورالعمل عمومی آموزش دقیق شده، نمره متوسط قابل توجه ۸۲.۶ دارد.

---

#### تمرین ۷.۴: تنظیم دقیق کارآمد با LoRA

برای افزایش بهره‌وری تنظیم دقیق مدل، کد این فصل را طوری تغییر دهید که از روش Low-Rank Adaptation (LoRA) که در پیوست E توضیح داده شده استفاده کند. زمان اجرای آموزش و عملکرد مدل را قبل و بعد از این تغییر مقایسه کنید.

---

## 7.9 نتیجه‌گیری

این فصل پایان مسیر ما در چرخه توسعه مدل‌های زبانی بزرگ (LLM) است. در این مسیر، تمامی مراحل ضروری شامل پیاده‌سازی معماری LLM، پیش‌ آموزش مدل و سپس ریزتنظیم (Fine-tuning) آن برای وظایف خاص را پوشش دادیم، که در شکل ۷.۲۱ خلاصه شده است. در ادامه، به برخی پیشنهادها برای گام‌های بعدی می‌پردازیم.

### 7.9.1 مراحل بعدی چیست؟

اگرچه مراحل اصلی را پوشش دادیم، یک مرحله اختیاری پس از ریزتنظیم مبتنی بر دستورالعمل وجود دارد که به آن ریزتنظیم ترجیحی (Preference Fine-tuning) گفته می‌شود. این مرحله برای شخصی‌سازی مدل و تطابق بهتر آن با ترجیحات خاص کاربران بسیار مفید است. اگر علاقه‌مند به مطالعه بیشتر هستید، پوشه 04_preference-tuning-with-dpo در مخزن گیت‌هاب مکمل این کتاب به آدرس https://mng.bz/dZwD را مشاهده کنید.

علاوه بر محتوای اصلی کتاب، مخزن گیت‌هاب شامل مجموعه‌ای گسترده از مطالب تکمیلی است که ممکن است برای شما مفید باشد. برای آشنایی بیشتر با این منابع، به بخش Bonus Material در صفحه README مخزن به آدرس https://mng.bz/r12g مراجعه کنید.

### 7.9.2 به‌روز ماندن در حوزه‌ای پویا

زمینه‌های هوش مصنوعی و پژوهش در حوزه LLM با سرعت بالایی در حال پیشرفت هستند (که بسته به دیدگاه، هیجان‌انگیز نیز هست). یکی از راه‌های به‌روز ماندن، مطالعه مقالات جدید در آرشیو arXiv به آدرس https://arxiv.org/list/cs.LG/recent است. همچنین، بسیاری از پژوهشگران و فعالان این حوزه در شبکه‌های اجتماعی مانند X (که پیش‌تر توییتر نام داشت) و Reddit به اشتراک‌گذاری و بحث درباره تازه‌ترین پیشرفت‌ها می‌پردازند. به‌ویژه subreddit با نام r/LocalLLaMA منبع مناسبی برای ارتباط با جامعه و آگاهی از ابزارها و روندهای جدید است. من نیز به طور منظم در وبلاگ‌های خود به اشتراک‌گذاری دیدگاه‌ها و تحلیل‌های جدید درباره پژوهش‌های LLM می‌پردازم که در https://magazine.sebastianraschka.com و https://sebastianraschka.com/blog/ قابل دسترسی است.

### 7.9.3 سخن پایانی

امیدوارم این سفر پیاده‌سازی یک مدل زبانی بزرگ از پایه، همراه با کدنویسی مراحل پیش‌ آموزش و ریزتنظیم از صفر برای شما جذاب بوده باشد. به نظر من، ساخت یک LLM از ابتدا بهترین روش برای کسب درک عمیق از نحوه عملکرد این مدل‌ها است. امیدوارم این رویکرد عملی، بینش‌های ارزشمندی به شما داده و پایه‌ای محکم در توسعه LLM فراهم کرده باشد.

هدف اصلی این کتاب آموزشی است، اما اگر به استفاده از مدل‌های بزرگ‌تر و قدرتمندتر در کاربردهای عملی علاقه‌مند هستید، توصیه می‌کنم ابزارهای محبوبی مانند Axolotl (https://github.com/OpenAccess-AI-Collective/axolotl) یا LitGPT (https://github.com/Lightning-AI/litgpt) را بررسی کنید که من نیز در توسعه آنها مشارکت دارم.

از همراهی شما در این مسیر یادگیری سپاسگزارم و برایتان موفقیت در آینده در حوزه هیجان‌انگیز LLM و هوش مصنوعی آرزو دارم!

---

## خلاصه مطالب

- فرآیند ریزتنظیم مبتنی بر دستورالعمل، یک مدل زبانی پیش‌آموزش‌دیده را برای پیروی بهتر از دستورات انسانی و تولید پاسخ‌های دلخواه تطبیق می‌دهد.
- آماده‌سازی داده‌ها شامل دانلود مجموعه داده دستور-پاسخ، قالب‌بندی ورودی‌ها و تقسیم آن‌ها به مجموعه‌های آموزش، اعتبارسنجی و آزمون است.
- بچ‌های آموزشی با استفاده از تابع collate سفارشی ساخته می‌شوند که دنباله‌ها را پدگذاری کرده، شناسه‌های توکن هدف را ایجاد و توکن‌های پد را ماسک می‌کند.
- یک مدل GPT-2 متوسط پیش‌آموزش‌دیده با ۳۵۵ میلیون پارامتر به عنوان نقطه شروع برای ریزتنظیم دستورالعمل بارگذاری می‌شود.
- مدل پیش‌آموزش‌دیده با استفاده از حلقه آموزشی مشابه پیش‌ آموزش روی مجموعه داده دستورالعمل ریزتنظیم می‌شود.
- ارزیابی شامل استخراج پاسخ‌های مدل روی مجموعه آزمون و امتیازدهی به آنها (برای مثال با استفاده از یک مدل زبانی دیگر) است.
- اپلیکیشن Ollama همراه با مدل Llama با ۸ میلیارد پارامتر می‌تواند برای امتیازدهی خودکار پاسخ‌های مدل ریزتنظیم‌شده در مجموعه آزمون استفاده شود و امتیاز متوسطی برای کمی‌سازی عملکرد ارائه دهد.



> [ 
    6.تنظیم دقیق  برای دسته‌بندی 
     (قبلی) ](
        <06.Fine-tuning for classification.md>
        ) <7.تنظیم دقیق برای پیروی از دستورالعمل‌ها>
    [
    پیوست A
(بعدی)
](<A.appendix.md>)