<!-- language: rtl -->

# پیاده‌سازی مدل GPT از ابتدا برای تولید متن

شما پیش‌تر مکانیزم **multi-head attention** را یاد گرفته و کد زده‌اید که یکی از اجزای اصلی مدل‌های زبانی بزرگ (LLM) است. اکنون قصد داریم سایر بلوک‌های سازنده‌ی یک LLM را پیاده‌سازی کرده و آن‌ها را در قالب مدلی شبیه به GPT کنار هم قرار دهیم تا در فصل بعد بتوانیم آن را برای تولید متن انسانی آموزش دهیم.

### معماری LLM

معماری LLM که در شکل ۴.۱ نشان داده شده، شامل چند بلوک ساختاری است. ابتدا نمای کلی مدل را از بالا به پایین مرور می‌کنیم و سپس به جزئیات هر بخش می‌پردازیم.

شکل ۴.۱: سه مرحله اصلی کدنویسی یک LLM. تمرکز این فصل بر گام سوم از مرحله اول یعنی پیاده‌سازی معماری LLM است.

### کدنویسی معماری LLM

مدل‌های زبانی بزرگ مانند GPT (Generative Pretrained Transformer) شبکه‌های عصبی عمیق بزرگی هستند که متن جدید را به صورت کلمه به کلمه (یا توکن به توکن) تولید می‌کنند. اگرچه این مدل‌ها بزرگ هستند، ساختار آن‌ها به پیچیدگی‌ای که فکر می‌کنید نیست؛ زیرا بسیاری از اجزا تکرار می‌شوند. در شکل ۴.۲ نمای کلی یک LLM شبیه به GPT به همراه اجزای اصلی آن نشان داده شده است.

تا اینجا بخش‌هایی از معماری LLM مانند توکنیزه کردن ورودی، embedding و ماژول masked multi-head attention را آموختیم. حالا ساختار اصلی مدل GPT شامل بلوک‌های ترنسفورمر را پیاده‌سازی می‌کنیم که در ادامه برای تولید متن انسانی آموزش داده خواهد شد.

در ابتدا برای ساده‌سازی از ابعاد embedding کوچک استفاده می‌کردیم تا مفاهیم راحت‌تر جا بیفتند، اما اکنون به ابعاد مدل کوچک GPT-2 با ۱۲۴ میلیون پارامتر می‌پردازیم، که نسخه‌ای از مدل GPT-2 است (مشخصات آن در مقاله Radford et al. ذکر شده است).

> توجه: تعداد پارامترها در گزارش اولیه ۱۱۷ میلیون ذکر شده بود که بعداً اصلاح شد.

در فصل ۶ آموزش، به بارگذاری وزن‌های pretrained و مدل‌های بزرگ‌تر GPT-2 با ۳۴۵، ۷۶۲ و ۱۵۴۲ میلیون پارامتر می‌پردازیم.

**پارامترها** در یادگیری عمیق به وزن‌های قابل آموزش مدل گفته می‌شود؛ متغیرهای داخلی که در طول آموزش برای کمینه کردن تابع خطا بهینه می‌شوند.

شکل ۴.۲: مدل GPT با اجزای embedding و یک یا چند بلوک ترنسفورمر شامل ماژول masked multi-head attention.

برای مثال، در یک لایه شبکه عصبی که با ماتریس وزن ۲۰۴۸×۲۰۴۸ تعریف می‌شود، هر عنصر این ماتریس یک پارامتر است؛ مجموع پارامترها برابر با ۲۰۴۸×۲۰۴۸=۴،۱۹۴،۳۰۴ است.

---

### تفاوت GPT-2 و GPT-3

ما روی GPT-2 تمرکز می‌کنیم زیرا وزن‌های pretrained آن به صورت عمومی در دسترس است و در فصل ۶ آن‌ها را بارگذاری می‌کنیم. GPT-3 از نظر معماری مشابه است، اما از ۱.۵ میلیارد پارامتر GPT-2 به ۱۷۵ میلیارد پارامتر افزایش یافته و روی داده‌های بیشتری آموزش دیده است.

تا زمان نگارش این متن، وزن‌های GPT-3 به صورت عمومی منتشر نشده‌اند. علاوه بر این، GPT-2 برای یادگیری پیاده‌سازی LLM مناسب‌تر است زیرا روی یک لپ‌تاپ معمولی قابل اجراست، در حالی که GPT-3 به کلاستر GPU نیاز دارد.

بر اساس گزارش Lambda Labs، آموزش GPT-3 روی یک GPU دیتاسنتر V100 به تنهایی ۳۵۵ سال و روی GPU مصرفی RTX 8000 حدود ۶۶۵ سال طول می‌کشد.

---

### پیکربندی مدل کوچک GPT-2

پیکربندی مدل کوچک GPT-2 به صورت دیکشنری پایتون زیر تعریف شده است که در مثال‌های کد استفاده می‌شود:

```python
GPT_CONFIG_124M = {
    "vocab_size": 50257,      # اندازه دیکشنری واژگان
    "context_length": 1024,   # طول حداکثر ورودی (تعداد توکن‌ها)
    "emb_dim": 768,           # ابعاد embedding
    "n_heads": 12,            # تعداد سرهای attention
    "n_layers": 12,           # تعداد لایه‌های ترنسفورمر
    "drop_rate": 0.1,         # نرخ dropout
    "qkv_bias": False         # استفاده از بایاس در Linear های query-key-value
}
```

- **vocab_size**: اندازه دیکشنری با ۵۰۲۵۷ توکن (مربوط به توکنیزر BPE در فصل ۲)
- **context_length**: بیشینه تعداد توکن‌های ورودی که مدل با positional embedding پوشش می‌دهد
- **emb_dim**: اندازه بردار embedding هر توکن (۷۶۸)
- **n_heads**: تعداد سرهای attention در مکانیزم multi-head attention (فصل ۳)
- **n_layers**: تعداد بلوک‌های ترنسفورمر (در ادامه تشریح خواهد شد)
- **drop_rate**: شدت dropout برای جلوگیری از overfitting (۱۰٪)
- **qkv_bias**: تعیین فعال بودن یا نبودن بایاس در Linear های query, key, value (ابتدا غیرفعال)

---

### معماری کلی مدل GPT (شکل ۴.۳)

با استفاده از این پیکربندی، ابتدا یک معماری ساده و نمایشی به نام `DummyGPTModel` می‌سازیم تا دید کلی از اجزا و نحوه اتصال آن‌ها به دست آوریم. شکل ۴.۳ ترتیب بخش‌هایی را نشان می‌دهد که باید برای ساخت نهایی مدل کد شوند.

---

### کد نمونه کلاس `DummyGPTModel`

```python
import torch
import torch.nn as nn

class DummyGPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])
        self.trf_blocks = nn.Sequential(
            *[DummyTransformerBlock(cfg) for _ in range(cfg["n_layers"])]
        )
        self.final_norm = DummyLayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits

class DummyTransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()

    def forward(self, x):
        return x

class DummyLayerNorm(nn.Module):
    def __init__(self, normalized_shape, eps=1e-5):
        super().__init__()

    def forward(self, x):
        return x
```

کلاس `DummyGPTModel` نسخه‌ای ساده‌شده از مدل GPT است که با PyTorch پیاده‌سازی شده است. این مدل شامل embedding توکن‌ها و موقعیت‌ها، dropout، چند بلوک ترنسفورمر (که در حال حاضر به صورت placeholder تعریف شده‌اند)، لایه نرمال‌سازی و لایه خطی خروجی است.

متد `forward` جریان داده را به ترتیب زیر پیاده‌سازی می‌کند:

- محاسبه embedding توکن‌ها و موقعیت‌ها
- اعمال dropout
- عبور از بلوک‌های ترنسفورمر
- اعمال نرمال‌سازی نهایی
- تولید **logits** با لایه خطی خروجی

توجه کنید که `DummyTransformerBlock` و `DummyLayerNorm` فعلاً فقط placeholders هستند و بعداً با نسخه‌های واقعی جایگزین می‌شوند.

---

### آماده‌سازی ورودی و اجرای مدل نمونه

برای استفاده از مدل، ابتدا ورودی‌های متنی را توکنیزه می‌کنیم. با استفاده از توکنیزر `tiktoken` (فصل ۲):

```python
import tiktoken
tokenizer = tiktoken.get_encoding("gpt2")

batch = []
txt1 = "Every effort moves you"
txt2 = "Every day holds a"
batch.append(torch.tensor(tokenizer.encode(txt1)))
batch.append(torch.tensor(tokenizer.encode(txt2)))
batch = torch.stack(batch, dim=0)

print(batch)
```

خروجی توکن‌ها:

```python
tensor([[6109, 3626, 6100, 345],
        [6109, 1110, 6622, 257]])
```

سپس مدل `DummyGPTModel` را مقداردهی اولیه کرده و روی ورودی اجرا می‌کنیم:

```python
torch.manual_seed(123)
model = DummyGPTModel(GPT_CONFIG_124M)
logits = model(batch)

print("Output shape:", logits.shape)
print(logits)
```

خروجی مدل (logits):

```python
Output shape: torch.Size([2, 4, 50257])
tensor([[[-1.2034, 0.3201, -0.7130, ..., -1.5548, -0.2390, -0.4667],
         [-0.1192, 0.4539, -0.4432, ..., 0.2392, 1.3469, 1.2430],
         [ 0.5307, 1.6720, -0.4695, ..., 1.1966, 0.0111, 0.5835],
         [ 0.0139, 1.6755, -0.3388, ..., 1.1586, -0.0435, -1.0400]],

        [[-1.0908, 0.1798, -0.9484, ..., -1.6047, 0.2439, -0.4530],
         [-0.7860, 0.5581, -0.0610, ..., 0.4835, -0.0077, 1.6621],
         [ 0.3567, 1.2698, -0.6398, ..., -0.0162, -0.1296, 0.3717],
         [-0.2407, -0.7349, -0.5102, ..., 2.0057, -0.3694, 0.1814]]],
       grad_fn=<UnsafeViewBackward0>)
```

خروجی شامل دو نمونه متنی است، هر نمونه ۴ توکن دارد و هر توکن یک بردار ۵۰۲۵۷ بعدی (مطابق دیکشنری توکن‌ها) است.

این بردارها بعداً به شناسه توکن تبدیل شده و به کلمات بازگردانده می‌شوند.

---

### نتیجه‌گیری

اکنون که نمای کلی معماری GPT و نحوه ورودی و خروجی آن را دیدیم، در ادامه کلاس‌های واقعی جایگزین placeholders مانند `LayerNorm` را پیاده‌سازی خواهیم کرد.

---

## ۴.۲ نرمال‌سازی فعال‌سازی‌ها با لایه‌نرمالیزیشن (Layer Normalization)

آموزش شبکه‌های عصبی عمیق با تعداد زیادی لایه، گاهی با مشکلاتی مانند **کاهش گرادیان** یا **افزایش گرادیان** مواجه می‌شود. این مشکلات باعث می‌شوند روند آموزش ناپایدار شود و شبکه نتواند به خوبی وزن‌های خود را تنظیم کند. در نتیجه، فرایند یادگیری به سختی می‌تواند پارامترهایی را پیدا کند که تابع خطا را کمینه کند. به بیان ساده‌تر، شبکه در شناسایی الگوهای داده و پیش‌بینی‌های دقیق دچار مشکل می‌شود.

> **توجه:** اگر با مفاهیم گرادیان و آموزش شبکه‌های عصبی آشنا نیستید، می‌توانید به بخش A.4 در پیوست A مراجعه کنید. اما دانش عمیق ریاضی در این زمینه برای ادامه کتاب ضروری نیست.

### ایده اصلی لایه‌نرمالیزیشن

هدف لایه‌نرمالیزیشن، بهبود پایداری و سرعت آموزش شبکه است. این کار با تنظیم مقادیر فعال‌سازی (خروجی‌های یک لایه) به گونه‌ای انجام می‌شود که میانگین آن‌ها برابر با ۰ و واریانس آن‌ها برابر با ۱ باشد (واریانس واحد). این تنظیم باعث می‌شود آموزش سریع‌تر به همگرایی برسد و پایدارتر باشد.

در معماری‌های مدرن مانند GPT-2، معمولاً لایه‌نرمالیزیشن قبل و بعد از ماژول چندسر توجه (multi-head attention) و همچنین قبل از لایه خروجی نهایی اعمال می‌شود (مطابق آنچه در مثال DummyLayerNorm دیده‌ایم).

---

### بررسی یک مثال ساده

کدی که در ادامه آمده، یک لایه شبکه عصبی با ۵ ورودی و ۶ خروجی ایجاد می‌کند و دو نمونه ورودی را به آن می‌دهد:

```python
torch.manual_seed(123)
batch_example = torch.randn(2, 5)
layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())
out = layer(batch_example)
print(out)
```

خروجی این کد به شکل زیر است:

```
tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],
        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],
       grad_fn=<ReluBackward0>)
```

در اینجا، لایه شبکه شامل یک لایه خطی (Linear) و تابع فعال‌سازی **ReLU** است که مقادیر منفی را به صفر تبدیل می‌کند. به همین دلیل خروجی منفی وجود ندارد.

---

### محاسبه میانگین و واریانس خروجی‌ها

میانگین و واریانس هر نمونه ورودی به صورت زیر محاسبه می‌شود:

```python
mean = out.mean(dim=-1, keepdim=True)
var = out.var(dim=-1, keepdim=True)
print("Mean:\n", mean)
print("Variance:\n", var)
```

نتایج:

```
Mean:
tensor([[0.1324],
        [0.2170]], grad_fn=<MeanBackward1>)
Variance:
tensor([[0.0231],
        [0.0398]], grad_fn=<VarBackward0>)
```

- `dim=-1` به این معناست که عملیات روی آخرین بعد (ستون‌ها) انجام می‌شود.
- `keepdim=True` باعث حفظ ابعاد اولیه تنسور می‌شود (در اینجا خروجی ۲×۱ خواهد بود نه ۲).

---

### اجرای نرمال‌سازی لایه‌ای روی خروجی‌ها

فرآیند لایه‌نرمالیزیشن به صورت زیر است: مقدار میانگین از هر مقدار کم می‌شود و سپس بر ریشه دوم واریانس تقسیم می‌شود.

```python
out_norm = (out - mean) / torch.sqrt(var)
mean = out_norm.mean(dim=-1, keepdim=True)
var = out_norm.var(dim=-1, keepdim=True)
print("Normalized layer outputs:\n", out_norm)
print("Mean:\n", mean)
print("Variance:\n", var)
```

خروجی:

```
Normalized layer outputs:
tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],
        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],
       grad_fn=<DivBackward0>)
Mean:
tensor([[-5.9605e-08],
        [ 1.9868e-08]], grad_fn=<MeanBackward1>)
Variance:
tensor([[1.],
        [1.]], grad_fn=<VarBackward0>)
```

اعداد بسیار نزدیک به صفر در میانگین، ناشی از خطاهای عددی محدود کامپیوتر است.

---

### جلوگیری از نمایش اعداد به صورت علمی

برای خوانایی بهتر، می‌توان نمایش علمی اعداد را غیر فعال کرد:

```python
torch.set_printoptions(sci_mode=False)
print("Mean:\n", mean)
print("Variance:\n", var)
```

خروجی:

```
Mean:
tensor([[0.0000],
        [0.0000]], grad_fn=<MeanBackward1>)
Variance:
tensor([[1.],
        [1.]], grad_fn=<VarBackward0>)
```

---

### پیاده‌سازی کلاس LayerNorm در PyTorch

کد زیر یک کلاس برای لایه‌نرمالیزیشن ارائه می‌دهد که در ادامه در مدل GPT استفاده خواهد شد:

```python
class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))
    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift
```

توضیح متغیرها:

- `eps` عدد کوچکی برای جلوگیری از تقسیم بر صفر است.
- `scale` و `shift` پارامترهای قابل آموزش هستند که مدل می‌تواند به‌طور خودکار تنظیم کند تا عملکرد بهتری داشته باشد.
- `unbiased=False` باعث استفاده از واریانس با برآورد بایاس‌دار می‌شود که در ابعاد بزرگ تفاوت ناچیزی دارد و برای سازگاری با مدل GPT-2 اصلی انتخاب شده است.

---

### آزمایش LayerNorm روی داده ورودی

```python
ln = LayerNorm(emb_dim=5)
out_ln = ln(batch_example)
mean = out_ln.mean(dim=-1, keepdim=True)
var = out_ln.var(dim=-1, unbiased=False, keepdim=True)
print("Mean:\n", mean)
print("Variance:\n", var)
```

خروجی:

```
Mean:
tensor([[-0.0000],
        [ 0.0000]], grad_fn=<MeanBackward1>)
Variance:
tensor([[1.0000],
        [1.0000]], grad_fn=<VarBackward0>)
```

همان‌طور که مشاهده می‌کنید، مقادیر نرمال‌شده دارای میانگین صفر و واریانس یک هستند.

---

تا اینجا، دو بلوک اصلی معماری GPT یعنی ساختار پایه و لایه‌نرمالیزیشن را بررسی کردیم (شکل ۴.۷). در ادامه، به تابع فعال‌سازی GELU و شبکه‌های تغذیه رو به جلو (Feed Forward) می‌پردازیم.

---

### تفاوت لایه‌نرمالیزیشن و باتچ‌نرمالیزیشن

اگر با باتچ‌نرمالیزیشن آشنا هستید، باید بدانید که تفاوت اصلی آن با لایه‌نرمالیزیشن در نحوه نرمال کردن داده‌هاست:

- **باتچ‌نرمالیزیشن** بر روی بعد دسته (batch) انجام می‌شود.
- **لایه‌نرمالیزیشن** بر روی بعد ویژگی‌ها (feature) انجام می‌شود.

چون اندازه دسته‌ها در آموزش و استنتاج مدل‌های بزرگ می‌تواند متفاوت باشد یا حتی به ۱ برسد، لایه‌نرمالیزیشن پایداری و انعطاف‌پذیری بیشتری دارد و برای آموزش توزیع‌شده یا محیط‌های با منابع محدود مناسب‌تر است.

---

## ۴.۳ پیاده‌سازی شبکه تغذیه رو به جلو با فعال‌سازی GELU

در این بخش، قصد داریم یک زیرماژول کوچک شبکه عصبی را که بخشی از بلوک ترنسفورمر در مدل‌های زبان بزرگ (LLM) است، پیاده‌سازی کنیم. ابتدا تابع فعال‌سازی **GELU** را معرفی و پیاده‌سازی می‌کنیم، چرا که نقش مهمی در این زیرماژول دارد.

> **توجه:** برای اطلاعات بیشتر درباره پیاده‌سازی شبکه‌های عصبی در PyTorch به بخش A.5 در پیوست A مراجعه کنید.

---

### فعال‌سازی GELU در مقابل ReLU

تاریخچه استفاده از تابع فعال‌سازی **ReLU** (تابع خطی اصلاح‌شده) به دلیل سادگی و عملکرد خوب آن در انواع معماری‌های شبکه عصبی عمیق بسیار رایج بوده است. اما در مدل‌های زبان بزرگ (LLM) توابع فعال‌سازی پیچیده‌تر و نرم‌تری مانند **GELU** (Gaussian Error Linear Unit) و **SwiGLU** (Swish-Gated Linear Unit) استفاده می‌شوند.

- **GELU** تابعی نرم و غیرخطی است که توزیع گاوسی و خاصیت خطی را ترکیب می‌کند.
- **SwiGLU** ترکیبی از واحد خطی و دروازه‌ای مبتنی بر سیگموید است.

این توابع نسبت به ReLU عملکرد بهتری در مدل‌های عمیق ارائه می‌دهند.

---

### تعریف تابع فعال‌سازی GELU

تعریف دقیق GELU به صورت زیر است:

\[
GELU(x) = x \cdot \Phi(x)
\]

که در آن \(\Phi(x)\) تابع توزیع تجمعی نرمال استاندارد است. اما در عمل معمولاً از تقریب زیر استفاده می‌شود که هزینه محاسباتی کمتری دارد و در مدل اصلی GPT-2 نیز به کار رفته است:

\[
GELU(x) \approx 0.5 \cdot x \cdot \left(1 + \tanh \left[\sqrt{\frac{2}{\pi}} \cdot \left(x + 0.044715 \cdot x^3\right)\right] \right)
\]

---

### پیاده‌سازی تابع GELU در PyTorch

```python
class GELU(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))
```

---

### مقایسه بصری GELU و ReLU

برای درک بهتر رفتار این دو تابع، نمودار آن‌ها را در بازه ([-3, 3]) رسم می‌کنیم:

```python
import matplotlib.pyplot as plt
gelu, relu = GELU(), nn.ReLU()
x = torch.linspace(-3, 3, 100)
y_gelu, y_relu = gelu(x), relu(x)

plt.figure(figsize=(8, 3))
for i, (y, label) in enumerate(zip([y_gelu, y_relu], ["GELU", "ReLU"]), 1):
    plt.subplot(1, 2, i)
    plt.plot(x, y)
    plt.title(f"{label} activation function")
    plt.xlabel("x")
    plt.ylabel(f"{label}(x)")
    plt.grid(True)
plt.tight_layout()
plt.show()
```

---

### توضیح نمودارها

- **ReLU** (سمت راست) یک تابع خطی تکه‌ای است که برای ورودی‌های مثبت مقدار ورودی را بازمی‌گرداند و برای ورودی‌های منفی صفر می‌دهد.
- **GELU** (سمت چپ) یک تابع نرم و غیرخطی است که تقریباً رفتار ReLU را دارد، اما گرادیان غیرصفر برای مقادیر منفی تقریباً در تمام نقاط به جز حدود (x \approx -0.75) دارد.

این نرمی در GELU به بهبود روند بهینه‌سازی کمک می‌کند، زیرا امکان تنظیم دقیق‌تر پارامترهای مدل فراهم می‌شود. در مقابل، ReLU در صفر گوشه تیزی دارد که ممکن است به سختی در بهینه‌سازی منجر شود، خصوصاً در شبکه‌های عمیق یا پیچیده.

علاوه بر این، GELU برخلاف ReLU که مقادیر منفی را صفر می‌کند، مقادیر منفی کوچک ولی غیرصفر بازمی‌گرداند، که باعث می‌شود نورون‌ها با ورودی منفی هم در فرایند یادگیری مشارکت کنند، هرچند با شدت کمتر.

---

### پیاده‌سازی ماژول FeedForward با فعال‌سازی GELU

اکنون از تابع GELU استفاده می‌کنیم تا ماژول کوچک شبکه عصبی **FeedForward** را پیاده‌سازی کنیم که در بلوک ترنسفورمر مدل LLM کاربرد دارد.

```python
class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )
    def forward(self, x):
        return self.layers(x)
```

---

### توضیح ساختار FeedForward

- این ماژول شامل دو لایه خطی (Linear) و یک تابع فعال‌سازی GELU در میان آن‌ها است.
- در مدل GPT با ۱۲۴ میلیون پارامتر، ابعاد embedding هر توکن برابر با ۷۶۸ است که در دیکشنری `GPT_CONFIG_124M` به صورت `GPT_CONFIG_124M["emb_dim"] = 768` تعریف شده است.
- شکل زیر (شکل ۴.۹) نشان می‌دهد که چگونه اندازه embedding داخل این شبکه کوچک تغییر می‌کند.

---

### نمونه استفاده و بررسی خروجی FeedForward

یک نمونه از ورودی با دو نمونه (batch size = 2) و هر نمونه شامل سه توکن با embedding 768 بعدی را به ماژول می‌دهیم:

```python
ffn = FeedForward(GPT_CONFIG_124M)
x = torch.rand(2, 3, 768)
out = ffn(x)
print(out.shape)
```

خروجی:

```
torch.Size([2, 3, 768])
```

همان‌طور که مشاهده می‌شود، ابعاد خروجی مشابه ورودی باقی می‌ماند.

---

### نکات مهم در طراحی FeedForward

- درونی، بعد embedding ابتدا توسط لایه اول به فضای بزرگ‌تر (4 \times) (یعنی از ۷۶۸ به ۳۰۷۲) گسترش می‌یابد.
- سپس توسط GELU فعال‌سازی می‌شود.
- در نهایت، لایه دوم ابعاد را به مقدار اصلی بازمی‌گرداند.
- این طراحی امکان بررسی نمایش‌های غنی‌تر و غیرخطی‌تر داده‌ها را فراهم می‌کند.
- همچنین حفظ ابعاد ورودی و خروجی باعث می‌شود که بتوان چندین لایه FeedForward را به راحتی روی هم انباشته کرد و مدل را مقیاس‌پذیرتر نمود.

---

اکنون اکثر بلوک‌های ساختاری لازم برای معماری GPT را پیاده‌سازی کرده‌ایم (شکل ۴.۱۱). مرحله بعدی بررسی اتصالات میان‌بر (Shortcut Connections) است که برای بهبود عملکرد آموزش در شبکه‌های عصبی عمیق کاربرد دارند.

## 4.4 افزودن اتصالات کوتاه‌بر (Shortcut Connections)

در این بخش مفهوم اتصالات کوتاه‌بر که با نام‌های دیگری مانند اتصالات پرش (skip connections) یا اتصالات باقیمانده (residual connections) نیز شناخته می‌شوند، مورد بررسی قرار می‌گیرد. در ابتدا، این نوع اتصالات برای شبکه‌های عمیق در حوزه بینایی ماشین (به ویژه در شبکه‌های باقیمانده یا Residual Networks) معرفی شدند تا مشکل **نابودی گرادیان‌ها** (vanishing gradients) را کاهش دهند. این مشکل به وضعیتی اشاره دارد که در آن گرادیان‌ها (که جهت به‌روزرسانی وزن‌ها در طول آموزش استفاده می‌شوند) در طی فرآیند بازگشت به عقب به‌تدریج کوچک و کمتر می‌شوند و در نتیجه آموزش لایه‌های ابتدایی به سختی انجام می‌گیرد.

شکل ۴.۱۲ نشان می‌دهد که اتصال کوتاه‌بر مسیر جایگزین و کوتاه‌تری برای عبور گرادیان‌ها در شبکه ایجاد می‌کند، به طوری که یک یا چند لایه را دور می‌زند. این کار با افزودن خروجی یک لایه به خروجی لایه‌ای بعدی انجام می‌شود. به همین دلیل به این اتصالات، اتصال پرش نیز گفته می‌شود. این اتصالات نقش حیاتی در حفظ جریان گرادیان‌ها هنگام عبور بازگشتی (backward pass) در آموزش دارند.

در ادامه، شبکه عصبی شکل ۴.۱۲ را پیاده‌سازی می‌کنیم تا نشان دهیم چگونه می‌توان اتصالات کوتاه‌بر را در متد forward اضافه کرد.

---

**شکل ۴.۱۲** مقایسه شبکه عصبی عمیقی شامل پنج لایه بدون (سمت چپ) و با اتصالات کوتاه‌بر (سمت راست). این اتصالات ورودی‌های یک لایه را به خروجی آن اضافه می‌کنند و به این ترتیب مسیر جایگزینی ایجاد می‌کنند که بعضی لایه‌ها را دور می‌زند. مقادیر گرادیان‌ها در شکل، میانگین قدر مطلق گرادیان هر لایه هستند که در لیستینگ ۴.۵ محاسبه شده‌اند.

---

### لیستینگ ۴.۵: پیاده‌سازی شبکه عصبی برای نشان دادن اتصالات کوتاه‌بر

```python
class ExampleDeepNeuralNetwork(nn.Module):
    def __init__(self, layer_sizes, use_shortcut):
        super().__init__()
        self.use_shortcut = use_shortcut
        self.layers = nn.ModuleList([
            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),
            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),
            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),
            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),
            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())
        ])

    def forward(self, x):
        for layer in self.layers:
            layer_output = layer(x)
            if self.use_shortcut and x.shape == layer_output.shape:
                x = x + layer_output
            else:
                x = layer_output
        return x
```

این کد یک شبکه عصبی عمیق با پنج لایه را پیاده‌سازی می‌کند که هر لایه شامل یک لایه Linear و تابع فعال‌سازی GELU است. در عبور رو به جلو (forward pass)، ورودی به‌صورت متوالی از لایه‌ها عبور می‌کند و در صورت فعال بودن گزینه‌ی `use_shortcut`، اتصال کوتاه‌بر با جمع کردن ورودی و خروجی لایه اعمال می‌شود.

---

### نمونه مقداردهی اولیه بدون اتصال کوتاه‌بر

هر لایه به گونه‌ای مقداردهی می‌شود که ورودی سه‌بعدی دریافت و خروجی سه‌بعدی برمی‌گرداند، به جز آخرین لایه که خروجی تک‌بعدی دارد:

```python
layer_sizes = [3, 3, 3, 3, 3, 1]
sample_input = torch.tensor([[1., 0., -1.]])
torch.manual_seed(123)
model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)
```

---

### تابع محاسبه و چاپ گرادیان‌ها در عبور بازگشتی

```python
def print_gradients(model, x):
    output = model(x)
    target = torch.tensor([[0.]])
    loss = nn.MSELoss()
    loss = loss(output, target)
    loss.backward()
    for name, param in model.named_parameters():
        if 'weight' in name:
            print(f"{name} has gradient mean of {param.grad.abs().mean().item()}")
```

این کد تابع خطایی را تعریف می‌کند که فاصله بین خروجی مدل و هدف مشخص‌شده (در اینجا مقدار صفر برای سادگی) را اندازه می‌گیرد. سپس با فراخوانی `loss.backward()`، گرادیان‌های خطا برای هر لایه محاسبه می‌شوند. در نهایت، میانگین قدر مطلق گرادیان وزن‌های هر لایه چاپ می‌شود تا مقایسه‌ی آسان‌تری از میزان گرادیان‌ها بین لایه‌ها داشته باشیم.

---

### اجرای تابع گرادیان برای مدل بدون اتصالات کوتاه‌بر

```python
print_gradients(model_without_shortcut, sample_input)
```

خروجی:

```
layers.0.0.weight has gradient mean of 0.00020173587836325169
layers.1.0.weight has gradient mean of 0.0001201116101583466
layers.2.0.weight has gradient mean of 0.0007152041653171182
layers.3.0.weight has gradient mean of 0.001398873864673078
layers.4.0.weight has gradient mean of 0.005049646366387606
```

همانطور که دیده می‌شود، گرادیان‌ها در لایه‌های اولیه به شدت کوچک‌تر می‌شوند که نمونه‌ای از مشکل **نابودی گرادیان** است.

---

### اجرای مدل با اتصالات کوتاه‌بر و مقایسه گرادیان‌ها

```python
torch.manual_seed(123)
model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)
print_gradients(model_with_shortcut, sample_input)
```

خروجی:

```
layers.0.0.weight has gradient mean of 0.22169792652130127
layers.1.0.weight has gradient mean of 0.20694105327129364
layers.2.0.weight has gradient mean of 0.32896995544433594
layers.3.0.weight has gradient mean of 0.2665732502937317
layers.4.0.weight has gradient mean of 1.3258541822433472
```

گرادیان لایه آخر همچنان بزرگ‌ترین مقدار را دارد، اما در مقایسه با مدل بدون اتصال کوتاه‌بر، گرادیان‌ها در لایه‌های ابتدایی پایدارتر هستند و به مقادیر بسیار کوچک نزدیک نمی‌شوند.

---

اتصالات کوتاه‌بر نقش مهمی در حل مشکل نابودی گرادیان در شبکه‌های عصبی عمیق ایفا می‌کنند. این نوع اتصالات، ساختار اصلی مدل‌های بسیار بزرگ مانند مدل‌های زبان بزرگ (LLMs) هستند و با تضمین جریان مداوم گرادیان‌ها در سراسر لایه‌ها، آموزش مؤثرتر مدل‌هایی مانند GPT را امکان‌پذیر می‌سازند.

---

## 4.5 اتصال لایه‌های attention و linear در بلوک ترنسفورمر

اکنون می‌خواهیم بلوک ترنسفورمر را پیاده‌سازی کنیم؛ این بلوک یکی از اجزای اصلی معماری GPT و سایر مدل‌های زبان بزرگ (LLM) است. این بلوک که در نسخه ۱۲۴ میلیون پارامتری GPT-2 حدود دوازده بار تکرار شده، ترکیبی از چندین مفهوم است که قبلاً مطرح کردیم: توجه چندسر (multi-head attention)، نرمال‌سازی لایه (layer normalization)، دراپ‌اوت (dropout)، لایه‌های feed forward و تابع فعال‌سازی GELU. در ادامه، این بلوک ترنسفورمر را به سایر اجزای معماری GPT متصل خواهیم کرد.

---

**شکل ۴.۱۳** بلوک ترنسفورمر را نشان می‌دهد که اجزای مختلفی مانند ماژول توجه چندسر ماسک‌دار (مطابق فصل ۳) و ماژول FeedForward (مطابق بخش ۴.۳) را ترکیب می‌کند. وقتی بلوک ترنسفورمر دنباله‌ای از ورودی‌ها را پردازش می‌کند، هر عنصر در دنباله (مثلاً یک کلمه یا توکن زیرکلمه) توسط یک بردار با طول ثابت (در اینجا ۷۶۸ بعد) نمایش داده می‌شود. عملیات داخل بلوک ترنسفورمر، شامل توجه چندسر و لایه‌های feed forward، به گونه‌ای طراحی شده‌اند که ابعاد بردارها حفظ شود.

مکانیزم توجه خود (self-attention) در بلوک توجه چندسر، روابط بین عناصر دنباله ورودی را شناسایی و تحلیل می‌کند، در حالی که شبکه feed forward داده‌ها را به‌صورت جداگانه در هر موقعیت تغییر می‌دهد. این ترکیب باعث می‌شود مدل بتواند درک دقیق‌تر و پردازش بهتری روی ورودی‌ها داشته باشد و ظرفیت بیشتری برای یادگیری الگوهای پیچیده داده‌ها پیدا کند.

---

**شکل ۴.۱۳** نمایی از بلوک ترنسفورمر. توکن‌های ورودی به بردارهای ۷۶۸ بعدی تعبیه شده‌اند. هر ردیف نمایانگر بردار یک توکن است. خروجی‌های بلوک ترنسفورمر بردارهایی با همان ابعاد ورودی هستند که می‌توانند به لایه‌های بعدی در یک LLM داده شوند.

---

### پیاده‌سازی کد بلوک ترنسفورمر

کد زیر ترنسفورمر در GPT را نمایش می‌دهد:

```python
class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"]
        )
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_shortcut = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)
        x = self.drop_shortcut(x)
        x = x + shortcut

        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_shortcut(x)
        x = x + shortcut

        return x
```

در این کد، کلاس `TransformerBlock` در PyTorch تعریف شده است که شامل مکانیزم توجه چندسر (`MultiHeadAttention`) و شبکه feed forward (`FeedForward`) می‌باشد. این اجزا بر اساس دیکشنری تنظیمات `cfg` پیکربندی می‌شوند (مانند `GPT_CONFIG_124M`).

نرمال‌سازی لایه (LayerNorm) قبل از هر یک از این دو بخش اعمال می‌شود و دراپ‌اوت بعد از آن‌ها برای جلوگیری از بیش‌برازش (overfitting) استفاده می‌شود. این روش به عنوان Pre-LayerNorm شناخته می‌شود. معماری‌های قدیمی‌تر مانند ترنسفورمر اصلی، نرمال‌سازی لایه را بعد از مکانیزم توجه و شبکه feed forward اعمال می‌کردند که به Post-LayerNorm معروف است و معمولاً منجر به دینامیک آموزش ضعیف‌تر می‌شود.

علاوه بر این، در متد forward هر بخش با یک اتصال کوتاه‌بر (shortcut connection) دنبال می‌شود که ورودی بلوک را به خروجی اضافه می‌کند. این ویژگی حیاتی به جریان یافتن گرادیان‌ها در حین آموزش کمک کرده و یادگیری مدل‌های عمیق را بهبود می‌بخشد (مطابق بخش ۴.۴).

---

### نمونه‌سازی و تست بلوک ترنسفورمر با داده نمونه

با استفاده از دیکشنری تنظیمات `GPT_CONFIG_124M` که پیش‌تر تعریف کردیم، یک بلوک ترنسفورمر را مقداردهی اولیه کرده و به آن داده نمونه می‌دهیم:

```python
torch.manual_seed(123)
x = torch.rand(2, 4, 768)
block = TransformerBlock(GPT_CONFIG_124M)
output = block(x)
print("Input shape:", x.shape)
print("Output shape:", output.shape)
```

خروجی:

```
Input shape: torch.Size([2, 4, 768])
Output shape: torch.Size([2, 4, 768])
```

همانطور که مشاهده می‌کنیم، بلوک ترنسفورمر ابعاد ورودی را در خروجی حفظ می‌کند، که نشان‌دهنده پردازش دنباله‌های داده بدون تغییر شکل آنها در سراسر شبکه است.

---

حفظ ابعاد در طول معماری بلوک ترنسفورمر یک ویژگی کلیدی و هدفمند است. این طراحی امکان کاربرد مؤثر آن در انواع وظایف دنباله به دنباله (sequence-to-sequence) را فراهم می‌کند، جایی که هر بردار خروجی مستقیماً متناظر با یک بردار ورودی است و رابطه یک‌به‌یک حفظ می‌شود. با این حال، بردار خروجی یک بردار زمینه‌ای (context vector) است که اطلاعات کل دنباله ورودی را در خود جای داده است (مطابق فصل ۳). به عبارت دیگر، در حالی که ابعاد فیزیکی دنباله (طول و اندازه ویژگی‌ها) در گذر از بلوک ترنسفورمر تغییر نمی‌کنند، محتوای هر بردار خروجی به گونه‌ای بازکدگذاری می‌شود که اطلاعات زمینه‌ای کل دنباله ورودی را ادغام کند.

---

با پیاده‌سازی بلوک ترنسفورمر، اکنون تمامی بلوک‌های سازنده لازم برای پیاده‌سازی معماری GPT را در اختیار داریم. همانطور که در شکل ۴.۱۴ مشاهده می‌شود، بلوک ترنسفورمر شامل نرمال‌سازی لایه، شبکه feed forward، فعال‌سازی GELU و اتصالات کوتاه‌بر است. این بلوک، بخش اصلی معماری GPT را تشکیل می‌دهد.

---

**شکل ۴.۱۴** بلوک‌های سازنده مورد نیاز برای ساخت معماری GPT را نشان می‌دهد. علامت‌های تیک سیاه نشان‌دهنده بلوک‌هایی هستند که تا کنون پیاده‌سازی شده‌اند.

## بخش 4.6: کدنویسی مدل GPT

در ابتدای این فصل، یک نمای کلی از معماری GPT ارائه دادیم که آن را DummyGPTModel نامیدیم. در این پیاده‌سازی، ورودی‌ها و خروجی‌های مدل GPT را نشان دادیم، اما اجزای سازنده آن به صورت یک جعبه سیاه با استفاده از کلاس‌های DummyTransformerBlock و DummyLayerNorm به عنوان جایگزین بودند.

اکنون قصد داریم این جایگزین‌ها را با کلاس‌های واقعی TransformerBlock و LayerNorm که قبلاً کد کرده‌ایم جایگزین کنیم تا نسخه کاملاً عملی و اصلی مدل GPT-2 با 124 میلیون پارامتر را بسازیم. در فصل 5 مدل GPT-2 را پیش‌آموزش خواهیم داد و در فصل 6 وزن‌های پیش‌آموزش دیده‌شده توسط OpenAI را بارگذاری می‌کنیم.

قبل از ساخت مدل GPT-2 در کد، به ساختار کلی آن نگاهی می‌اندازیم (شکل 4.15). همان‌طور که مشاهده می‌کنیم، بلاک ترنسفورمر در معماری GPT چندین بار تکرار می‌شود. برای مدل 124 میلیون پارامتری GPT-2، این بلاک 12 بار تکرار می‌شود که از طریق مقدار `n_layers` در دیکشنری `GPT_CONFIG_124M` تعیین می‌شود. در بزرگترین مدل GPT-2 با 1.54 میلیارد پارامتر، این بلاک 48 بار تکرار می‌شود.

خروجی از آخرین بلاک ترنسفورمر، ابتدا از یک مرحله نرمال‌سازی نهایی عبور کرده و سپس به لایه خروجی خطی می‌رود. این لایه خروجی، بردارهای خروجی ترنسفورمر را به فضای با ابعاد بالا (در اینجا 50,257 بعد که برابر با اندازه واژگان مدل است) نگاشت می‌کند تا توکن بعدی در دنباله پیش‌بینی شود.

اکنون به کدنویسی معماری شکل 4.15 می‌پردازیم.

---

### شکل 4.15: نمای کلی معماری مدل GPT

متن توکنیزه شده ابتدا به بردارهای توکن (Token Embeddings) تبدیل می‌شود، سپس بردارهای موقعیتی (Positional Embeddings) به آن اضافه می‌شود. این داده ترکیبی به صورت یک تانسور به چندین بلاک ترنسفورمر متوالی (هر کدام شامل مکانیزم چندسر توجه Multi-head Attention و شبکه عصبی پیش‌خور همراه با دراپ‌اوت و نرمال‌سازی لایه) ارسال می‌شود که در مجموع 12 بار تکرار شده‌اند.

---

### فهرست کد 4.7: پیاده‌سازی معماری مدل GPT

```python
class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])
        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])]
        )
        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits
```

با استفاده از کلاس TransformerBlock، کلاس GPTModel بسیار کوچک و جمع‌وجور باقی مانده است.

در سازنده `__init__` این کلاس، لایه‌های توکن و موقعیت با استفاده از پیکربندی موجود در دیکشنری `cfg` مقداردهی اولیه می‌شوند. این لایه‌ها مسئول تبدیل شاخص‌های توکن ورودی به بردارهای چگال و افزودن اطلاعات موقعیتی هستند (مراجعه به فصل 2).

سپس، تعدادی بلاک TransformerBlock به صورت متوالی ایجاد می‌شوند، تعداد آن‌ها برابر با مقدار `n_layers` در `cfg` است. پس از این بلاک‌ها، لایه LayerNorm اعمال می‌شود تا خروجی‌ها استاندارد شده و فرآیند یادگیری پایدارتر شود. در نهایت، یک لایه خطی بدون بایاس تعریف شده که خروجی ترنسفورمر را به فضای واژگان نگاشت می‌کند و برای هر توکن در واژگان، لاجیت‌ها را تولید می‌کند.

متد `forward` یک دسته از شاخص‌های توکن ورودی را گرفته، بردارهای توکن و موقعیت را محاسبه می‌کند، داده‌ها را از بلاک‌های ترنسفورمر عبور داده، خروجی را نرمال‌سازی می‌کند و در نهایت لاجیت‌ها را تولید می‌کند که احتمال‌های بدون نرمال‌سازی توکن بعدی هستند.

---

### نمونه‌ای از مقداردهی اولیه مدل 124 میلیون پارامتری GPT

```python
torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
out = model(batch)
print("Input batch:\n", batch)
print("\nOutput shape:", out.shape)
print(out)
```

خروجی این کد:

```
Input batch:
tensor([[6109, 3626, 6100, 345],
        [6109, 1110, 6622, 257]])
Output shape: torch.Size([2, 4, 50257])
tensor([[[ 0.3613, 0.4222, -0.0711, ..., 0.3483, 0.4661, -0.2838],
         [-0.1792, -0.5660, -0.9485, ..., 0.0477, 0.5181, -0.3168],
         [ 0.7120, 0.0332, 0.1085, ..., 0.1018, -0.4327, -0.2553],
         [-1.0076, 0.3418, -0.1190, ..., 0.7195, 0.4023, 0.0532]],

        [[-0.2564, 0.0900, 0.0335, ..., 0.2659, 0.4454, -0.6806],
         [ 0.1230, 0.3653, -0.2074, ..., 0.7705, 0.2710, 0.2246],
         [ 1.0558, 1.0318, -0.2800, ..., 0.6936, 0.3205, -0.3178],
         [-0.1565, 0.3926, 0.3288, ..., 1.2630, -0.1858, 0.0388]]],
       grad_fn=<UnsafeViewBackward0>)
```

همانطور که مشاهده می‌شود، تانسور خروجی دارای شکل `[2, 4, 50257]` است، زیرا دو متن ورودی با 4 توکن به مدل داده شده است. بعد آخر (50257) برابر با اندازه دیکشنری واژگان است. بعداً خواهیم دید که چگونه این بردارهای خروجی با ابعاد 50,257 را به توکن‌های متنی تبدیل کنیم.

---

### محاسبه تعداد پارامترهای مدل

برای محاسبه تعداد کل پارامترهای مدل از متد `numel()` استفاده می‌کنیم:

```python
total_params = sum(p.numel() for p in model.parameters())
print(f"Total number of parameters: {total_params:,}")
```

خروجی:

```
Total number of parameters: 163,009,536
```

ممکن است تعجب کنید که چرا مدل ما 163 میلیون پارامتر دارد در حالی که گفتیم مدل 124 میلیون پارامتری است؟

دلیل این اختلاف، مفهوم **وزن اشتراکی (Weight Tying)** است که در معماری اصلی GPT-2 استفاده شده است. به این معنی که وزن‌های لایه توکن ایمبدینگ (token embedding) در لایه خروجی نیز مجدداً استفاده می‌شوند.

بیایید ابعاد لایه توکن ایمبدینگ و لایه خروجی را بررسی کنیم:

```python
print("Token embedding layer shape:", model.tok_emb.weight.shape)
print("Output layer shape:", model.out_head.weight.shape)
```

خروجی:

```
Token embedding layer shape: torch.Size([50257, 768])
Output layer shape: torch.Size([50257, 768])
```

وزن‌های این دو لایه کاملاً هم‌شکل هستند و از تعداد زیادی سطر (به اندازه واژگان مدل) تشکیل شده‌اند.

اگر پارامترهای لایه خروجی را از کل پارامترهای مدل کم کنیم (با توجه به اشتراک وزن‌ها):

```python
total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())
print(f"Number of trainable parameters considering weight tying: {total_params_gpt2:,}")
```

خروجی:

```
Number of trainable parameters considering weight tying: 124,412,160
```

در این حالت، تعداد پارامترها برابر با 124 میلیون می‌شود که مطابق با اندازه اصلی مدل GPT-2 است.

اشتراک وزن‌ها باعث کاهش حافظه مورد نیاز و پیچیدگی محاسباتی می‌شود. اما در تجربه من، استفاده از لایه‌های جداگانه برای توکن ایمبدینگ و خروجی باعث بهبود آموزش و عملکرد مدل می‌شود؛ بنابراین در پیاده‌سازی خودمان از لایه‌های جدا استفاده می‌کنیم. این موضوع برای مدل‌های بزرگ زبان مدرن نیز صادق است. البته در فصل 6 دوباره به پیاده‌سازی وزن اشتراکی خواهیم پرداخت، زمانی که وزن‌های پیش‌آموزش دیده شده OpenAI را بارگذاری می‌کنیم.

---

### تمرین 4.1: تعداد پارامترها در ماژول‌های Feed Forward و Attention

تعداد پارامترهای موجود در ماژول Feed Forward و ماژول Multi-head Attention را محاسبه و مقایسه کنید.

---

### محاسبه حافظه مورد نیاز مدل

با فرض اینکه هر پارامتر یک عدد float32 (4 بایت) است، حافظه لازم برای مدل را محاسبه می‌کنیم:

```python
total_size_bytes = total_params * 4
total_size_mb = total_size_bytes / (1024 * 1024)
print(f"Total size of the model: {total_size_mb:.2f} MB")
```

خروجی:

```
Total size of the model: 621.83 MB
```

بنابراین، مدل 163 میلیون پارامتری ما حدود 622 مگابایت حافظه نیاز دارد که نشان‌دهنده حجم نسبتا بالای ذخیره‌سازی برای مدل‌های زبان کوچک است.

---

### تمرین 4.2: مقداردهی اولیه مدل‌های بزرگ‌تر GPT

ما مدل GPT با 124 میلیون پارامتر (GPT-2 small) را مقداردهی اولیه کردیم. بدون تغییر کد به جز بروزرسانی فایل پیکربندی، با استفاده از کلاس GPTModel مدل‌های زیر را پیاده‌سازی کنید:

- GPT-2 medium (بردارهای 1024 بعدی، 24 بلاک ترنسفورمر، 16 سر توجه)
- GPT-2 large (بردارهای 1280 بعدی، 36 بلاک ترنسفورمر، 20 سر توجه)
- GPT-2 XL (بردارهای 1600 بعدی، 48 بلاک ترنسفورمر، 25 سر توجه)

همچنین، تعداد کل پارامترها در هر مدل را محاسبه کنید.

---

## ۴.۷ تولید متن

در این بخش، کدی را پیاده‌سازی می‌کنیم که خروجی تانسوری مدل GPT را به متن قابل خواندن برای انسان تبدیل می‌کند.  
پیش از شروع، نگاهی گذرا به این موضوع خواهیم داشت که یک مدل زبانی تولیدی (مانند LLM) چگونه متن را به صورت مرحله‌ای و توکن به توکن تولید می‌کند.

---

### تولید مرحله‌ای متن

شکل 4.16 فرآیند تولید متن را به صورت گام‌به‌گام نشان می‌دهد. فرض کنید ورودی اولیه مدل عبارتی مانند «Hello, I am» باشد. مدل در هر مرحله یک توکن جدید را پیش‌بینی کرده و به ورودی قبلی اضافه می‌کند تا زمینه (context) گسترده‌تری برای پیش‌بینی‌های بعدی داشته باشد.

تا مرحله ششم، مدل جمله‌ای کامل مانند «Hello, I am a model ready to help» را می‌سازد.  
در پیاده‌سازی فعلی ما، کلاس `GPTModel` خروجی‌هایی با شکل `[batch_size, num_tokens, vocab_size]` تولید می‌کند. حال سوال این است که چگونه از این تانسور به متن نهایی می‌رسیم؟

---

### فرآیند تبدیل لاجیت‌ها به متن

شکل 4.17 فرآیند تولید توکن بعدی را در هر مرحله نشان می‌دهد. خلاصه مراحل به شکل زیر است:

1. مدل، توکن‌های ورودی را دریافت کرده و خروجی لاجیت‌ها را تولید می‌کند.
2. بردار خروجی مربوط به آخرین توکن از طریق تابع softmax به توزیع احتمال تبدیل می‌شود.
3. توکنی با بیشترین احتمال (بیشترین مقدار در بردار) انتخاب می‌شود.
4. شناسه توکن به متن تبدیل شده و به ورودی اضافه می‌شود.
5. این فرایند تا رسیدن به تعداد مشخصی از توکن‌های جدید تکرار می‌شود.

---

### کد پیاده‌سازی تولید متن

### فهرست کد 4.8: تابع تولید متن ساده

```python
def generate_text_simple(model, idx, max_new_tokens, context_size):
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
            logits = logits[:, -1, :]
            probas = torch.softmax(logits, dim=-1)
            idx_next = torch.argmax(probas, dim=-1, keepdim=True)
            idx = torch.cat((idx, idx_next), dim=1)
    return idx
```

این تابع یک حلقه ساده تولید متن را پیاده‌سازی می‌کند. در هر مرحله:

- زمینه فعلی به اندازه حداکثر طول مجاز بریده می‌شود.
- مدل لاجیت‌ها را برای آن زمینه تولید می‌کند.
- توکن بعدی با بیشترین احتمال (greedy decoding) انتخاب شده و به ورودی افزوده می‌شود.

> نکته: استفاده از softmax در اینجا برای اهداف آموزشی است و کاربرد عملی ندارد، زیرا ترتیب مقادیر در بردار logits حفظ می‌شود و `argmax` همان خروجی را می‌دهد. اما این مرحله به درک بهتر تبدیل لاجیت‌ها به احتمال کمک می‌کند.

در فصل بعد، تکنیک‌های پیشرفته‌تری برای نمونه‌گیری پیاده‌سازی خواهیم کرد تا متن‌های متنوع‌تر و خلاقانه‌تری تولید شود.

---

### مثال عملی

ابتدا ورودی «Hello, I am» را به توکن‌ها تبدیل می‌کنیم:

```python
start_context = "Hello, I am"
encoded = tokenizer.encode(start_context)
print("encoded:", encoded)
encoded_tensor = torch.tensor(encoded).unsqueeze(0)
print("encoded_tensor.shape:", encoded_tensor.shape)
```

خروجی:

```
encoded: [15496, 11, 314, 716]
encoded_tensor.shape: torch.Size([1, 4])
```

مدل را به حالت ارزیابی (`eval`) می‌بریم:

```python
model.eval()
out = generate_text_simple(
    model=model,
    idx=encoded_tensor,
    max_new_tokens=6,
    context_size=GPT_CONFIG_124M["context_length"]
)
print("Output:", out)
print("Output length:", len(out[0]))
```

خروجی توکن‌ها:

```
Output: tensor([[15496, 11, 314, 716, 27018, 24086, 47843,
                 30961, 42348, 7267]])
Output length: 10
```

اکنون توکن‌ها را به متن تبدیل می‌کنیم:

```python
decoded_text = tokenizer.decode(out.squeeze(0).tolist())
print(decoded_text)
```

خروجی:

```
Hello, I am Featureiman Byeswickattribute argue
```

همان‌طور که می‌بینیم، خروجی بی‌معنی و نامنسجم است. علت این است که مدل هنوز آموزش ندیده و وزن‌های آن به صورت تصادفی مقداردهی شده‌اند. فرآیند آموزش مدل، موضوع فصل بعد خواهد بود.

---

### تمرین ۴.۳: استفاده از نرخ‌های دراپ‌اوت مجزا

در ابتدای این فصل، ما از یک نرخ کلی دراپ‌اوت (`drop_rate`) در دیکشنری `GPT_CONFIG_124M` برای تمام لایه‌های دراپ‌اوت استفاده کردیم.

اکنون کد را تغییر دهید تا نرخ‌های دراپ‌اوت جداگانه برای بخش‌های مختلف مدل در نظر بگیرد:

1. **لایه‌های جاسازی (Embedding)**
2. **لایه‌های اتصال میان‌بری (Shortcut Connections)**
3. **ماژول Attention چندسر**

> 💡 راهنما: برای این کار، کافی است پارامترهای مجزایی مانند `emb_dropout`, `attn_dropout`, و `resid_dropout` در پیکربندی قرار دهید و آن‌ها را در بخش‌های مربوطه از کد استفاده کنید.

---

## خلاصه فصل

- **نرمال‌سازی لایه (Layer Normalization)** به پایداری فرایند آموزش کمک می‌کند، زیرا خروجی هر لایه را با میانگین و واریانس یکنواخت تنظیم می‌کند.

- **اتصالات میان‌بری (Shortcut Connections)**، با عبور دادن خروجی یک لایه به لایه‌های عمیق‌تر، از بروز مشکل *ناپدید شدن گرادیان* در شبکه‌های عمیق مانند مدل‌های زبانی بزرگ (LLM) جلوگیری می‌کنند.

- **بلوک‌های ترنسفورمر (Transformer Blocks)**، اجزای اصلی در معماری مدل‌های GPT هستند و از دو بخش تشکیل می‌شوند:
  - ماژول‌های *attention* چندسر با ماسک
  - شبکه‌های کاملاً متصل feed-forward با تابع فعال‌سازی **GELU**

- **مدل‌های GPT** از تکرار چندین بلوک ترنسفورمر ساخته شده‌اند و تعداد پارامترهای آن‌ها از میلیون تا میلیارد متغیر است.

- مدل‌های GPT در اندازه‌های مختلفی ارائه شده‌اند، از جمله مدل‌های با **۱۲۴، ۳۴۵، ۷۶۲ و ۱۵۴۲ میلیون پارامتر** که همگی را می‌توان با کلاس پایتونی `GPTModel` پیاده‌سازی کرد.

- **قابلیت تولید متن** در مدل‌های GPT با پیش‌بینی توکن‌ها به صورت ترتیبی و بر اساس متن ورودی (context) انجام می‌شود. خروجی‌های تانسوری مدل به کمک توکنایزر به متن قابل فهم برای انسان تبدیل می‌شوند.

- بدون آموزش، خروجی مدل GPT بی‌معنی و نامنسجم است، که نشان‌دهنده‌ی اهمیت **آموزش مدل** برای تولید متن منسجم و معنادار است.


> [ 
        3.کدنویسی مکانیزم‌های توجه
     (قبلی) ](
        <03.Coding attention mechanisms.md>
        ) <
    4.پیاده‌سازی مدل GPT از ابتدا برای تولید متن>
[
    5.پیش‌آموزش روی داده‌های بدون برچسب
(بعدی)
](<05.Pretraining on unlabeled data.md>)