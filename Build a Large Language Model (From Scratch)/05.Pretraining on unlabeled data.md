<!-- language: rtl -->

# 5.پیش‌آموزش روی داده‌های بدون برچسب

# **پیش‌پردازش با داده‌های بدون برچسب**

تا اینجا، ما مراحل نمونه‌برداری از داده و مکانیزم توجه (Attention) را پیاده‌سازی کرده و معماری مدل زبانی بزرگ (LLM) را کدنویسی کرده‌ایم. اکنون زمان آن رسیده است که یک تابع آموزشی تعریف کرده و مدل LLM را پیش‌پردازش (Pretrain) کنیم. در این مرحله، با روش‌های پایه‌ای ارزیابی مدل آشنا خواهیم شد تا کیفیت متنی را که مدل تولید می‌کند بسنجیم — این کار برای بهینه‌سازی مدل در حین آموزش ضروری است.

علاوه‌ بر این، بررسی خواهیم کرد که چگونه می‌توان وزن‌های از پیش آموزش‌دیده (Pretrained Weights) را بارگذاری کرد تا مدل ما نقطه شروع قوی‌تری برای تنظیم دقیق (Fine-tuning) داشته باشد.

**شکل 5.1** نمای کلی برنامه‌ریزی ما را نشان می‌دهد و مرحله‌ای را که در این فصل به آن خواهیم پرداخت (مرحله ۲: پیش‌پردازش مدل LLM) برجسته می‌کند. این فصل شامل پیاده‌سازی کد آموزشی (مرحله ۵)، ارزیابی عملکرد (مرحله ۶) و ذخیره‌سازی/بارگذاری وزن‌های مدل (مرحله ۷) است.

---

### **پارامترهای وزن (Weight Parameters)**

در زمینه مدل‌های زبانی بزرگ و دیگر مدل‌های یادگیری عمیق، **وزن‌ها** به پارامترهای قابل‌آموزش اشاره دارند که طی فرایند یادگیری، به‌روزرسانی می‌شوند. این وزن‌ها معمولاً با عنوان **پارامترهای وزن** یا به اختصار **پارامترها** شناخته می‌شوند.

در فریم‌ورک‌هایی مانند PyTorch، این وزن‌ها در لایه‌های خطی (Linear Layers) ذخیره می‌شوند. ما قبلاً از این لایه‌ها برای پیاده‌سازی ماژول توجه چندسری (Multi-head Attention) در فصل ۳ و کلاس `GPTModel` در فصل ۴ استفاده کرده‌ایم.

پس از مقداردهی اولیه به یک لایه مانند:

```python
new_layer = torch.nn.Linear(...)
```

می‌توان به وزن‌های آن از طریق صفت `.weight` مانند `new_layer.weight` دسترسی داشت. همچنین، PyTorch به ما امکان می‌دهد تا به‌طور مستقیم به همه پارامترهای قابل‌آموزش یک مدل (از جمله وزن‌ها و بایاس‌ها) با استفاده از متد `model.parameters()` دسترسی پیدا کنیم؛ از این روش در ادامه برای پیاده‌سازی آموزش مدل استفاده خواهیم کرد.

---

## **5.1 ارزیابی مدل‌های مولد متن**

پیش از ارزیابی، ابتدا مروری کوتاه بر فرایند تولید متن خواهیم داشت که در فصل ۴ پیاده‌سازی کردیم. سپس مدل LLM را برای تولید متن آماده می‌کنیم و در ادامه، روش‌های پایه برای سنجش کیفیت متن تولیدشده را بررسی خواهیم کرد. در نهایت، مقدار **خطای آموزش** و **خطای اعتبارسنجی** (Training and Validation Loss) را محاسبه خواهیم کرد.

**شکل 5.2** موضوعات تحت پوشش این فصل را نمایش می‌دهد که در حال حاضر روی سه مرحله اول متمرکز هستیم.

---

### **5.1.1 استفاده از GPT برای تولید متن**

بیایید مدل LLM را مقداردهی اولیه کرده و مروری کوتاه بر فرایند تولید متنی داشته باشیم که در فصل ۴ پیاده‌سازی کرده‌ایم. برای این کار، از کلاس `GPTModel` و پیکربندی `GPT_CONFIG_124M` استفاده خواهیم کرد:

```python
import torch
from chapter04 import GPTModel

GPT_CONFIG_124M = {
    "vocab_size": 50257,
    "context_length": 256,
    "emb_dim": 768,
    "n_heads": 12,
    "n_layers": 12,
    "drop_rate": 0.1,
    "qkv_bias": False
}

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval()
```

در این پیکربندی، تنها تفاوت با فصل گذشته، کاهش طول کانتکست (**context_length**) به ۲۵۶ توکن است. این تغییر باعث می‌شود نیاز محاسباتی مدل کاهش یافته و بتوان آن را روی یک لپ‌تاپ معمولی آموزش داد.

در نسخه اصلی GPT-2 با ۱۲۴ میلیون پارامتر، طول کانتکست تا ۱۰۲۴ توکن بود. پس از اتمام آموزش، این مقدار را مجدداً افزایش داده و وزن‌های آموزش‌دیده را بارگذاری خواهیم کرد تا از کانتکست ۱۰۲۴ توکن بهره بگیریم.

برای تولید متن با استفاده از نمونه‌ی مدل `GPTModel`، از تابع `generate_text_simple` (که در فصل ۴ معرفی شد) بهره می‌گیریم. در کنار آن، دو تابع مفید تعریف می‌کنیم:

- `text_to_token_ids`: برای تبدیل متن به شناسه‌های توکن
- `token_ids_to_text`: برای بازگرداندن شناسه‌های توکن به متن

این توابع در سراسر این فصل برای تبدیل بین متن و نمایشی عددی آن استفاده خواهند شد.

---

#### **فرایند تولید متن با GPT**

**شکل 5.3** فرایند سه‌مرحله‌ای تولید متن را با استفاده از مدل GPT نشان می‌دهد:

1. **توکن‌سازی**: ابتدا متن ورودی به شناسه‌های توکن تبدیل می‌شود (فصل ۲).
2. **پیش‌بینی**: سپس این شناسه‌ها به مدل داده می‌شوند تا بردارهایی به نام **logits** تولید شود؛ این بردارها توزیع احتمال هر توکن در واژگان را نشان می‌دهند (فصل ۴).
3. **بازتوکن‌سازی**: نهایتاً، logitsها به توکن‌های خروجی تبدیل شده و توسط توکنایزر به متن قابل‌خواندن تبدیل می‌شوند.

---

#### **لیست 5.1 – توابع کمکی برای تبدیل متن به شناسه توکن**

```python
import tiktoken
from chapter04 import generate_text_simple

def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)
    return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)
    return tokenizer.decode(flat.tolist())

start_context = "Every effort moves you"
tokenizer = tiktoken.get_encoding("gpt2")

token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(start_context, tokenizer),
    max_new_tokens=10,
    context_size=GPT_CONFIG_124M["context_length"]
)

print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```

#### **خروجی احتمالی:**

```
Output text:
Every effort moves you rentingetic wasn? refres RexMeCHicular stren
```

---

#### **تحلیل خروجی**

همان‌طور که مشاهده می‌کنید، مدل هنوز متنی منطقی تولید نمی‌کند. دلیل آن ساده است: مدل هنوز آموزش ندیده است. برای اینکه بتوانیم تعیین کنیم یک متن «منطقی» یا «با کیفیت» است، باید از روشی عددی برای ارزیابی استفاده کنیم. این کار به ما امکان می‌دهد که عملکرد مدل را در طول آموزش رصد کرده و بهبود دهیم.

در ادامه، یک **معیار خطا (Loss Function)** را برای خروجی‌های تولیدشده محاسبه خواهیم کرد. این معیار به عنوان نشانگر پیشرفت و موفقیت آموزش عمل می‌کند. همچنین در فصل‌های آینده، هنگام تنظیم دقیق مدل، با روش‌های بیشتری برای ارزیابی کیفیت آشنا خواهیم شد.

---

اگر آماده هستید، ادامه متن را بفرستید تا با همین رویکرد ادامه بدهم.

در ادامه، ترجمه و توضیح علمی، دقیق و روان بخش 5.1.2 را مطابق با اصول مورد نظر شما ارائه می‌دهم. کدها عیناً و بدون تغییر در قالب Markdown درج شده‌اند.

---

### **5.1.2 محاسبه‌ی خطای تولید متن (Text Generation Loss)**

در این بخش، با روش‌های عددی برای ارزیابی کیفیت متنی که مدل در حین آموزش تولید می‌کند آشنا می‌شویم. این کار را با **محاسبه‌ی یک معیار خطا (loss)** انجام خواهیم داد. برای درک بهتر این موضوع، آن را مرحله‌به‌مرحله همراه با یک مثال عملی بررسی می‌کنیم؛ از بارگذاری داده‌ها گرفته تا تولید متن توسط تابع `generate_text_simple`.

---

#### **مروری بر فرایند تولید متن**

**شکل 5.4** روند کلی تولید متن را در پنج مرحله نشان می‌دهد — از ورودی متنی تا خروجی تولیدشده توسط مدل LLM. این روند در واقع کاری است که تابع `generate_text_simple` به‌صورت داخلی انجام می‌دهد. برای اینکه بتوانیم مقدار خطا را محاسبه کنیم، باید دقیقاً همین مراحل ابتدایی را اجرا کنیم.

نکته: برای ساده‌سازی، در **شکل 5.4** فقط از یک واژگان ۷‌تایی استفاده شده، اما در واقعیت، مدل GPT ما با واژگانی به اندازه‌ی **۵۰٬۲۵۷ توکن** کار می‌کند. در نتیجه، شناسه‌های توکن تولید شده در کدهای زیر در بازه‌ی ۰ تا ۵۰٬۲۵۶ قرار دارند.

---

#### **مقداردهی اولیه داده‌های ورودی و هدف**

در ادامه، از دو مثال متنی استفاده می‌کنیم:

- `"every effort moves"`
- `"I really like"`

شناسه‌های توکن مربوط به این جملات به‌صورت زیر تعریف می‌شوند:

```python
inputs = torch.tensor([
    [16833, 3626, 6100],    # "every effort moves"
    [40, 1107, 588]         # "I really like"
])
```

و شناسه‌های توکن هدف (targets)، که یک موقعیت جلوتر از ورودی‌ها هستند، به‌صورت زیر تعریف شده‌اند:

```python
targets = torch.tensor([
    [3626, 6100, 345],      # "effort moves you"
    [1107, 588, 11311]      # "really like chocolate"
])
```

> ⬅️ این تکنیک **جابجایی یک موقعیتی (shifted targets)** در فصل ۲ نیز توضیح داده شد و برای آموزش مدل‌های زبانی ضروری است، چرا که مدل را وادار می‌کند «توکن بعدی» را یاد بگیرد.

---

#### **تبدیل logits به احتمالات (Softmax)**

ورودی‌ها را به مدل می‌دهیم و از خروجی `logits`، مقادیر احتمال (probabilities) را با تابع Softmax محاسبه می‌کنیم:

```python
with torch.no_grad():
    logits = model(inputs)
    probas = torch.softmax(logits, dim=-1)
    print(probas.shape)
```

خروجی:

```
torch.Size([2, 3, 50257])
```

> 🔍 این ابعاد به‌ترتیب به موارد زیر اشاره دارند:
>
> - ۲: تعداد نمونه‌ها (batch size)
> - ۳: تعداد توکن‌ها در هر نمونه
> - ۵۰٬۲۵۷: اندازه‌ی واژگان (vocab size)

---

#### **مرحله‌ی بعدی – انتخاب توکن خروجی با بیشترین احتمال (Argmax)**

```python
token_ids = torch.argmax(probas, dim=-1, keepdim=True)
print("Token IDs:\n", token_ids)
```

خروجی:

```
Token IDs:
tensor([[[16657],
         [  339],
         [42826]],
        [[49906],
         [29669],
         [41751]]])
```

---

#### **مقایسه خروجی مدل با هدف (Target)**

```python
print(f"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}")
print(f"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}")
```

خروجی:

```
Targets batch 1: effort moves you
Outputs batch 1: Armed heNetflix
```

> ❌ همان‌طور که مشخص است، خروجی مدل کاملاً تصادفی است و ارتباطی با جمله‌ی هدف ندارد؛ دلیل این موضوع، آموزش ندیده بودن مدل است.

---

#### **محاسبه‌ی عددی خطا (Loss) برای ارزیابی عملکرد**

در ادامه، می‌خواهیم **مقدار خطا را به‌صورت عددی محاسبه کنیم**. این مقدار هم به ما در ارزیابی کیفیت خروجی کمک می‌کند و هم سنگ‌بنای تعریف تابع آموزش مدل است.

هدف نهایی آموزش مدل این است که **احتمال (Softmax probability)** اختصاص‌یافته به توکن‌های صحیح (مقابل هدف‌ها) را **افزایش دهد**.

---

#### **نمایش احتمال اولیه توکن هدف در خروجی مدل**

```python
text_idx = 0
target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]
print("Text 1:", target_probas_1)

text_idx = 1
target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]
print("Text 2:", target_probas_2)
```

خروجی:

```
Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])
Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])
```

> 📌 این مقادیر بسیار کوچک هستند، چون مدل هنوز تصادفی عمل می‌کند. (در واژگان ۵۰٬۲۵۷‌تایی، احتمال اولیه هر توکن تقریباً 0.00002 است)

---

#### **Backpropagation و تابع خطا**

برای به‌روزرسانی وزن‌های مدل به‌طوری‌که احتمال توکن صحیح افزایش یابد، از **روش پس‌انتشار (Backpropagation)** استفاده می‌کنیم. این روش نیاز به یک **تابع خطا (Loss Function)** دارد که تفاوت بین خروجی پیش‌بینی‌شده و خروجی واقعی را اندازه‌گیری کند.

---

#### **محاسبه log احتمالات هدف**

```python
log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))
print(log_probas)
```

خروجی:

```
tensor([ -9.5042, -10.3796, -11.3677, -11.4798, -9.7764, -12.2561])
```

> ✍️ کار با لگاریتم احتمال‌ها در بهینه‌سازی عددی ساده‌تر از کار با خود احتمال‌هاست.

---

#### **میانگین‌گیری از log احتمالات**

```python
avg_log_probas = torch.mean(log_probas)
print(avg_log_probas)
```

خروجی:

```
tensor(-10.7940)
```

---

#### **محاسبه مقدار نهایی خطا (Cross-Entropy Loss)**

در یادگیری عمیق، معمول است که **میانگین log احتمال‌ها را منفی کنیم** تا مقدار loss نهایی به‌دست آید:

```python
neg_avg_log_probas = avg_log_probas * -1
print(neg_avg_log_probas)
```

خروجی:

```
tensor(10.7940)
```

> 🔺 این مقدار همان **Cross Entropy Loss** است؛ هرچه به صفر نزدیک‌تر باشد، خروجی مدل به هدف نزدیک‌تر است.

📦 خوشبختانه، در PyTorch، تمام این مراحل در یک تابع آماده به‌نام `torch.nn.functional.cross_entropy` پیاده‌سازی شده‌اند و نیازی به انجام دستی مراحل بالا در عمل نیست.

---

#### ✅ جمع‌بندی این بخش:

- مدل فعلاً عملکرد ضعیفی دارد چون آموزش ندیده است.
- با استفاده از **احتمال Softmax** و **لگاریتم آن‌ها**، می‌توان عملکرد مدل را عددی سنجید.
- این مقدار **مبنای آموزش مدل** خواهد بود، زیرا با کمینه کردن این loss، وزن‌های مدل طوری به‌روزرسانی می‌شوند که خروجی تولیدشده به هدف نزدیک‌تر شود.
- در ادامه، از همین مکانیزم برای نوشتن تابع آموزش مدل استفاده خواهیم کرد.

---

#### **Cross Entropy Loss (خطای آنتروپی متقاطع)**

خطای آنتروپی متقاطع، یکی از معیارهای بسیار محبوب در یادگیری ماشین و یادگیری عمیق است که اختلاف بین دو توزیع احتمال را اندازه می‌گیرد:

- توزیع واقعی (مانند توکن‌های هدف در داده‌ها)
- توزیع پیش‌بینی شده توسط مدل (مانند احتمال‌های تولیدشده برای توکن‌ها توسط مدل زبانی بزرگ)

در چارچوب‌هایی مانند PyTorch، تابع `cross_entropy` این اختلاف را برای حالت‌های گسسته محاسبه می‌کند. این مقدار به معنای منفی میانگین لگاریتم احتمال توکن‌های هدف است که مدل پیش‌بینی کرده است. به همین دلیل، اصطلاحات **cross entropy** و **negative average log probability** معمولاً به جای هم استفاده می‌شوند.

---

#### یادآوری ابعاد `logits` و `targets`

```python
print("Logits shape:", logits.shape)
print("Targets shape:", targets.shape)
```

خروجی:

```
Logits shape: torch.Size([2, 3, 50257])
Targets shape: torch.Size([2, 3])
```

- `logits` یک تنسور سه‌بعدی است که ابعاد آن به‌ترتیب:
  (تعداد نمونه‌ها، تعداد توکن‌ها، اندازه واژگان)
- `targets` یک تنسور دو‌بعدی است با ابعاد:
  (تعداد نمونه‌ها، تعداد توکن‌ها)

---

#### تبدیل به حالت مسطح (Flattening)

برای استفاده از تابع `cross_entropy` در PyTorch، باید این دو تنسور را روی ابعاد batch و token با هم ترکیب کنیم (flatten کنیم):

```python
logits_flat = logits.flatten(0, 1)
targets_flat = targets.flatten()
print("Flattened logits:", logits_flat.shape)
print("Flattened targets:", targets_flat.shape)
```

خروجی:

```
Flattened logits: torch.Size([6, 50257])
Flattened targets: torch.Size([6])
```

---

#### محاسبه loss با استفاده از `cross_entropy`

تابع `cross_entropy` در PyTorch تمامی مراحل زیر را به صورت خودکار انجام می‌دهد:

- اعمال تابع Softmax روی `logits`
- انتخاب احتمال‌های مربوط به توکن‌های هدف
- محاسبه لگاریتم احتمالات
- میانگین‌گیری و منفی کردن برای گرفتن Cross Entropy

کد:

```python
loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)
print(loss)
```

خروجی:

```
tensor(10.7940)
```

> این مقدار دقیقاً همان مقداری است که قبلاً به صورت دستی و مرحله به مرحله محاسبه کردیم.

---

#### **Perplexity (تردید مدل)**

پِرپِلِکسیتی معیاری است که معمولاً همراه با cross entropy برای ارزیابی مدل‌های زبانی استفاده می‌شود و تفسیر بهتری از عدم قطعیت مدل در پیش‌بینی توکن بعدی فراهم می‌کند.

- **Perplexity** نشان می‌دهد که مدل چقدر در تطابق توزیع پیش‌بینی شده با توزیع واقعی داده‌ها موفق است.
- هرچه مقدار perplexity کمتر باشد، مدل عملکرد بهتری دارد و احتمال توکن‌های صحیح را بهتر پیش‌بینی می‌کند.

---

#### محاسبه perplexity از loss

فرمول:

```python
perplexity = torch.exp(loss)
print(perplexity)
```

خروجی:

```
tensor(48725.8203)
```

---

#### تفسیر عدد perplexity

- مقدار perplexity معادل اندازه‌ی واژگان موثری است که مدل درباره آن‌ها در هر گام پیش‌بینی نامطمئن است.
- در این مثال، مقدار تقریبی 48,725 به این معناست که مدل در انتخاب توکن بعدی در بین حدود 48,725 توکن موجود در واژگان کاملاً نامطمئن است.

---

#### **خلاصه و گام بعدی**

- ما اکنون loss مربوط به دو نمونه کوچک متنی را محاسبه کردیم.
- در ادامه، همین محاسبه loss را برای کل مجموعه‌های آموزش و اعتبارسنجی به‌کار خواهیم برد.

---

### 5.1.3 محاسبه خطا (loss) مجموعه‌های آموزش و اعتبارسنجی

ابتدا باید مجموعه داده‌های آموزش و اعتبارسنجی را آماده کنیم که برای آموزش مدل زبان بزرگ (LLM) استفاده خواهیم کرد. سپس، همانطور که در شکل 5.8 نشان داده شده، میزان آنتروپی متقاطع (cross entropy) برای هر دو مجموعه آموزش و اعتبارسنجی را محاسبه می‌کنیم که جزو بخش‌های مهم فرایند آموزش مدل است.

---

#### انتخاب مجموعه داده و علت استفاده

برای محاسبه loss روی مجموعه‌های آموزش و اعتبارسنجی، از یک مجموعه داده متنی بسیار کوچک استفاده می‌کنیم: داستان کوتاه «The Verdict» اثر Edith Wharton که قبلاً در فصل ۲ با آن کار کردیم.
انتخاب متن از **دامنه عمومی (public domain)** باعث می‌شود نگرانی‌های حقوق استفاده برطرف شود.
همچنین به دلیل کوچک بودن این مجموعه داده، اجرای مثال‌ها حتی روی لپ‌تاپ‌های معمولی و بدون نیاز به GPU پیشرفته، در عرض چند دقیقه ممکن است؛ که برای اهداف آموزشی بسیار مناسب است.

> **نکته:** خوانندگان علاقه‌مند می‌توانند از کدهای تکمیلی کتاب برای آماده‌سازی مجموعه داده‌های بزرگ‌تر (مثلاً شامل بیش از ۶۰ هزار کتاب از پروژه گوتنبرگ) استفاده کنند و LLM را روی آنها آموزش دهند (جزئیات در پیوست D).

---

#### هزینه آموزش مدل‌های LLM در مقیاس بزرگ

برای درک بزرگی پروژه، مدل Llama 2 با 7 میلیارد پارامتر را در نظر بگیرید که نیازمند ۱۸۴,۳۲۰ ساعت GPU روی کارت‌های A100 گران‌قیمت بوده است و در مجموع ۲ تریلیون توکن پردازش کرده است.
با توجه به قیمت حدودی ۳۰ دلار در ساعت برای ۸ کارت A100 در سرویس ابری AWS، هزینه تقریبی آموزش چنین مدلی حدود **۶۹۰,۰۰۰ دلار** است.

---

#### بارگذاری مجموعه داده «The Verdict»

```python
file_path = "the-verdict.txt"
with open(file_path, "r", encoding="utf-8") as file:
    text_data = file.read()
```

---

#### شمارش تعداد کاراکترها و توکن‌ها

```python
total_characters = len(text_data)
total_tokens = len(tokenizer.encode(text_data))
print("Characters:", total_characters)
print("Tokens:", total_tokens)
```

خروجی:

```
Characters: 20479
Tokens: 5145
```

---

با ۵۱۴۵ توکن، این مجموعه داده ممکن است برای آموزش کامل یک LLM بزرگ بسیار کوچک به نظر برسد، اما هدف آموزشی است تا کد در چند دقیقه اجرا شود نه چند هفته.
همچنین در ادامه از وزن‌های پیش‌آموزش‌دیده (pretrained) مدل OpenAI در کد GPTModel استفاده خواهیم کرد.

---

#### تقسیم داده به مجموعه آموزش و اعتبارسنجی

مجموعه داده را به دو بخش تقسیم می‌کنیم: ۹۰٪ برای آموزش و ۱۰٪ برای اعتبارسنجی.

```python
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]
```

---

#### ایجاد DataLoader برای آموزش و اعتبارسنجی

از تابع `create_dataloader_v1` که در فصل ۲ تعریف شده استفاده می‌کنیم:

```python
from chapter02 import create_dataloader_v1

torch.manual_seed(123)

train_loader = create_dataloader_v1(
    train_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],  # معمولاً 256
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=True,
    shuffle=True,
    num_workers=0
)

val_loader = create_dataloader_v1(
    val_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=False,
    shuffle=False,
    num_workers=0
)
```

---

#### بررسی ابعاد داده‌ها

برای اطمینان از درستی ساخت DataLoaderها، می‌توانیم ابعاد دسته‌های داده را چاپ کنیم:

```python
print("Train loader:")
for x, y in train_loader:
    print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
    print(x.shape, y.shape)
```

خروجی نمونه:

```
Train loader:
torch.Size([2, 256]) torch.Size([2, 256])
... (تکرار برای ۹ دسته)
Validation loader:
torch.Size([2, 256]) torch.Size([2, 256])
```

> در هر دسته (batch)، دو نمونه با طول ۲۵۶ توکن داریم. دسته‌های اعتبارسنجی نیز مشابه هستند اما فقط یک دسته داریم چون داده کمتر است.

---

#### محاسبه loss برای یک دسته داده

```python
def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch = input_batch.to(device)
    target_batch = target_batch.to(device)
    logits = model(input_batch)
    loss = torch.nn.functional.cross_entropy(
        logits.flatten(0, 1), target_batch.flatten()
    )
    return loss
```

---

#### محاسبه متوسط loss برای همه دسته‌ها

```python
def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))

    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches
```

- این تابع به طور پیش‌فرض روی همه دسته‌ها در DataLoader پیمایش کرده، loss هر دسته را جمع می‌کند و میانگین می‌گیرد.
- می‌توان تعداد دسته‌ها را با `num_batches` محدود کرد تا ارزیابی سریع‌تر انجام شود.

---

#### محاسبه و نمایش loss مجموعه‌های آموزش و اعتبارسنجی

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

with torch.no_grad():
    train_loss = calc_loss_loader(train_loader, model, device)
    val_loss = calc_loss_loader(val_loader, model, device)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)
```

نمونه خروجی:

```
Training loss: 10.98758347829183
Validation loss: 10.98110580444336
```

---

#### تفسیر نتایج

- مقدار loss هنوز نسبتاً بالا است، چون مدل هنوز آموزش ندیده و تصادفی عمل می‌کند.
- هرچه مدل بهتر شود و توکن‌ها را دقیق‌تر پیش‌بینی کند، مقدار loss به صفر نزدیک خواهد شد.

---

#### جمع‌بندی

ما مراحل زیر را طی کردیم:

1. بارگذاری داده‌های متنی
2. تقسیم داده به آموزش و اعتبارسنجی
3. ایجاد DataLoader با دسته‌های توکنیزه شده
4. تعریف توابع محاسبه loss برای هر دسته و کل داده
5. ارزیابی اولیه loss روی داده‌های آموزش و اعتبارسنجی

در مراحل بعدی، مدل را آموزش داده و سعی می‌کنیم این loss را کاهش دهیم تا مدل در تولید متن بهتر عمل کند.

---

## بخش ۵.۲ آموزش یک مدل زبان بزرگ (LLM)

اکنون زمان آن رسیده است که کد پیش‌آموزش مدل زبان بزرگ خود، یعنی GPTModel را پیاده‌سازی کنیم. در این بخش، روی یک حلقه آموزش ساده تمرکز می‌کنیم تا کد مختصر، قابل فهم و خوانا باشد.

> **نکته:** علاقه‌مندان می‌توانند تکنیک‌های پیشرفته‌تری مانند گرم‌کردن نرخ یادگیری (learning rate warmup)، کاهش نرخ یادگیری به صورت کسینوسی (cosine annealing) و برش گرادیان (gradient clipping) را در ضمیمه D مطالعه کنند.

---

**شکل ۵.۱۱**:
یک حلقه آموزش معمولی در PyTorch برای شبکه‌های عصبی عمیق شامل مراحل متعددی است که در هر دور (epoch) روی دسته‌های داده (batch) آموزش تکرار می‌شود. در هر تکرار، مقدار خطا (loss) هر دسته محاسبه شده و از آن برای به‌روزرسانی وزن‌های مدل استفاده می‌شود تا خطای مجموعه آموزش به حداقل برسد.

---

**روند کلی آموزش در PyTorch** (شکل ۵.۱۱) شامل ۸ مرحله است:

- تکرار روی هر epoch
- پردازش دسته‌های داده
- صفر کردن گرادیان‌ها
- محاسبه خطا و گرادیان‌ها
- به‌روزرسانی وزن‌ها
- نمایش میزان خطا و نمونه متن تولید شده

> **نکته:** اگر تازه‌کار هستید و برخی مراحل برایتان نامفهوم است، پیشنهاد می‌کنیم بخش‌های A.5 تا A.8 در ضمیمه A را مطالعه کنید.

---

#### پیاده‌سازی حلقه آموزش ساده

تابع `train_model_simple` این روند را به صورت کد پیاده‌سازی می‌کند:

```python
def train_model_simple(model, train_loader, val_loader,
                       optimizer, device, num_epochs,
                       eval_freq, eval_iter, start_context, tokenizer):
    train_losses, val_losses, track_tokens_seen = [], [], []    #1
    tokens_seen, global_step = 0, -1

    for epoch in range(num_epochs):    #2
        model.train()
        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()   #3
            loss = calc_loss_batch(
                input_batch, target_batch, model, device
            )
            loss.backward()                     #4
            optimizer.step()                    #5
            tokens_seen += input_batch.numel()
            global_step += 1

            if global_step % eval_freq == 0:    #6
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                track_tokens_seen.append(tokens_seen)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, "
                      f"Val loss {val_loss:.3f}"
                )

        generate_and_print_sample(                      #7
            model, tokenizer, device, start_context
        )
    return train_losses, val_losses, track_tokens_seen
```

- #1 مقداردهی اولیه لیست‌ها برای ثبت خطا و تعداد توکن‌های دیده شده
- #2 شروع حلقه اصلی آموزش
- #3 صفر کردن گرادیان‌ها از تکرار قبلی
- #4 محاسبه گرادیان‌ها
- #5 به‌روزرسانی وزن‌های مدل
- #6 ارزیابی مدل به صورت دوره‌ای
- #7 تولید و چاپ نمونه متنی پس از هر epoch

> توجه: دو تابع `evaluate_model` و `generate_and_print_sample` که در این تابع استفاده شده‌اند هنوز تعریف نشده‌اند.

---

#### تعریف تابع ارزیابی مدل

تابع `evaluate_model` مطابق با مرحله ۷ شکل ۵.۱۱، خطا روی داده‌های آموزش و اعتبارسنجی را محاسبه می‌کند و هنگام محاسبه، مدل را در حالت ارزیابی قرار می‌دهد تا گرادیان‌گیری و dropout غیرفعال شود:

```python
def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()  #1
    with torch.no_grad():                              #2
        train_loss = calc_loss_loader(
            train_loader, model, device, num_batches=eval_iter
        )
        val_loss = calc_loss_loader(
            val_loader, model, device, num_batches=eval_iter
        )
    model.train()
    return train_loss, val_loss
```

- #1 حالت ارزیابی مدل برای غیرفعال‌کردن dropout
- #2 غیر فعال‌کردن پیگیری گرادیان برای کاهش هزینه محاسباتی

---

#### تعریف تابع تولید و نمایش نمونه متن

این تابع برای بررسی کیفی بهبود مدل در طول آموزش استفاده می‌شود. متن شروع (start_context) را به توکن تبدیل کرده، سپس با استفاده از مدل، متن جدید تولید می‌کند و چاپ می‌کند:

```python
def generate_and_print_sample(model, tokenizer, device, start_context):
    model.eval()
    context_size = model.pos_emb.weight.shape[0]
    encoded = text_to_token_ids(start_context, tokenizer).to(device)
    with torch.no_grad():
        token_ids = generate_text_simple(
            model=model, idx=encoded,
            max_new_tokens=50, context_size=context_size
        )
    decoded_text = token_ids_to_text(token_ids, tokenizer)
    print(decoded_text.replace("\n", " "))      #1
    model.train()
```

- #1 چاپ متن تولید شده در یک خط برای وضوح بهتر

---

#### استفاده از بهینه‌ساز AdamW

بهینه‌ساز AdamW یک نسخه بهبود یافته از Adam است که روش وزن‌کاهش (weight decay) را بهبود می‌بخشد. این کار باعث کاهش بیش‌برازش (overfitting) و بهبود تعمیم‌پذیری مدل می‌شود و به همین دلیل در آموزش مدل‌های زبان بزرگ به طور گسترده استفاده می‌شود.

---

#### اجرای آموزش مدل

نمونه‌ای از اجرای آموزش ۱۰ epoch با بهینه‌ساز AdamW و تابع `train_model_simple` به شکل زیر است:

```python
torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(
     model.parameters(),           #1
    lr=0.0004, weight_decay=0.1
)
num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs=num_epochs, eval_freq=5, eval_iter=5,
    start_context="Every effort moves you", tokenizer=tokenizer
)
```

- #1 متد `.parameters()` تمامی وزن‌های قابل آموزش مدل را برمی‌گرداند.

---

#### نتایج اجرای آموزش

خروجی نمونه اجرا به شرح زیر است:

```
Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933
Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339
Every effort moves you,,,,,,,,,,,,.
Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048
Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616
Every effort moves you, and, and, and, and, and, and, and, and, and, and,
 and, and, and, and, and, and, and, and, and, and, and, and,, and, and,
[...]                                                   #1
Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393
Every effort moves you?"  "Yes--quite insensible to the irony. She wanted
him vindicated--and by me!"  He laughed again, and threw back the
window-curtains, I had the donkey. "There were days when I
Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452
Every effort moves you know," was one of the axioms he laid down across the
Sevres and silver of an exquisitely appointed luncheon-table, when, on a
later day, I had again run over from Monte Carlo; and Mrs. Gis
```

- #1 بخش‌هایی از خروجی برای صرفه‌جویی در فضا حذف شده‌اند.

همانطور که مشاهده می‌کنیم، خطای آموزش از ۹.۷۸۱ به ۰.۳۹۱ کاهش چشمگیری دارد و مدل توانایی تولید جملات صحیح دستوری را پیدا کرده است.
خطای اعتبارسنجی نیز کاهش می‌یابد اما هرگز به اندازه خطای آموزش پایین نمی‌آید که نشان‌دهنده احتمال بیش‌برازش مدل است.

---

#### رسم نمودار خطاهای آموزش و اعتبارسنجی

برای مشاهده بهتر روند آموزش، می‌توانیم خطاهای آموزش و اعتبارسنجی را در یک نمودار رسم کنیم:

```python
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
    fig, ax1 = plt.subplots(figsize=(5, 3))
    ax1.plot(epochs_seen, train_losses, label="Training loss")
    ax1.plot(
        epochs_seen, val_losses, linestyle="-.", label="Validation loss"
    )
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")
    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
    ax2 = ax1.twiny()                   #1
    ax2.plot(tokens_seen, train_losses, alpha=0)     #2
    ax2.set_xlabel("Tokens seen")
    fig.tight_layout()
    plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)
```

- #1 ایجاد محور دوم برای محور x که با محور y مشترک است
- #2 رسم نمودار نامرئی برای هماهنگ‌سازی تیک‌ها

---

#### تحلیل نمودار

همانطور که در شکل ۵.۱۲ مشاهده می‌شود، خطاهای آموزش و اعتبارسنجی در ابتدا هر دو کاهش می‌یابند، اما پس از epoch دوم از هم فاصله می‌گیرند. این موضوع همراه با بزرگ‌تر بودن خطای اعتبارسنجی نسبت به خطای آموزش، نشان‌دهنده بیش‌برازش مدل است. متن تولید شده نیز گواهی بر حفظ دقیق داده‌های آموزش توسط مدل است.

---

#### آماده‌سازی مدل برای استنتاج

پس از اتمام آموزش، مدل را از GPU به CPU منتقل کرده و در حالت ارزیابی قرار می‌دهیم تا بخش‌های تصادفی مانند dropout غیرفعال شوند:

```python
model.to("cpu")
model.eval()
```

---

#### تولید متن نمونه با مدل آموزش‌دیده

از تابع `generate_text_simple` برای تولید متن به صورت مرحله به مرحله استفاده می‌کنیم:

```python
tokenizer = tiktoken.get_encoding("gpt2")
token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids("Every effort moves you", tokenizer),
    max_new_tokens=25,
    context_size=GPT_CONFIG_124M["context_length"]
)
print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```

خروجی نمونه:

```
Output text:
Every effort moves you know," was one of the axioms he laid down across the
Sevres and silver of an exquisitely appointed lun
```

همانطور که قبلاً توضیح داده شد، در هر مرحله تولید، توکنی با بیشترین احتمال انتخاب می‌شود، بنابراین هر بار اجرای تابع خروجی یکسانی تولید می‌کند.

#### ۵.۳.۱ مقیاس‌بندی دما (Temperature Scaling)

اکنون به مقیاس‌بندی دما می‌پردازیم؛ تکنیکی که فرایند تولید توکن بعدی را به صورت احتمالاتی درمی‌آورد. پیش‌تر، در تابع `generate_text_simple`، همیشه توکنی که بالاترین احتمال را داشت به عنوان توکن بعدی با استفاده از تابع `torch.argmax` انتخاب می‌کردیم که به آن **رمزگشایی طماعانه (greedy decoding)** گفته می‌شود. برای تولید متن با تنوع بیشتر، می‌توانیم به جای `argmax` از تابعی استفاده کنیم که نمونه‌گیری را بر اساس توزیع احتمالات انجام می‌دهد (اینجا منظور توزیع احتمالاتی است که مدل زبان بزرگ برای هر کلمه در هر مرحله تولید توکن ارائه می‌دهد).

برای توضیح این نمونه‌گیری احتمالاتی، یک مثال ساده با یک دیکشنری واژگان کوچک می‌آوریم:

```python
vocab = {
    "closer": 0,
    "every": 1,
    "effort": 2,
    "forward": 3,
    "inches": 4,
    "moves": 5,
    "pizza": 6,
    "toward": 7,
    "you": 8,
}
inverse_vocab = {v: k for k, v in vocab.items()}
```

فرض کنید مدل زبان جمله شروعی «every effort moves you» را گرفته و لاجیت‌های توکن بعدی را به شکل زیر تولید کرده است:

```python
next_token_logits = torch.tensor(
    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]
)
```

همان‌طور که در فصل ۴ توضیح داده شد، در تابع `generate_text_simple` این لاجیت‌ها را به احتمال با استفاده از تابع softmax تبدیل می‌کنیم و سپس با `argmax`، شناسه توکن با بیشترین احتمال را انتخاب و با `inverse_vocab` به متن تبدیل می‌کنیم:

```python
probas = torch.softmax(next_token_logits, dim=0)
next_token_id = torch.argmax(probas).item()
print(inverse_vocab[next_token_id])
```

از آنجا که بزرگ‌ترین مقدار لاجیت (و به تبع آن بیشترین احتمال softmax) در جایگاه چهارم (اندیس ۳ در پایتون) قرار دارد، توکن تولید شده "forward" است.

---

برای نمونه‌گیری احتمالاتی، می‌توانیم `argmax` را با تابع `multinomial` در PyTorch جایگزین کنیم:

```python
torch.manual_seed(123)
next_token_id = torch.multinomial(probas, num_samples=1).item()
print(inverse_vocab[next_token_id])
```

خروجی همان "forward" است. دلیلش چیست؟ تابع `multinomial` توکن بعدی را متناسب با احتمال آن نمونه‌گیری می‌کند؛ یعنی "forward" هنوز پر احتمال‌ترین توکن است و اغلب انتخاب می‌شود، اما همیشه نه. برای نمایش بهتر، تابعی می‌نویسیم که این نمونه‌گیری را ۱۰۰۰ بار تکرار کند:

```python
def print_sampled_tokens(probas):
    torch.manual_seed(123)
    sample = [torch.multinomial(probas, num_samples=1).item()
             for i in range(1_000)]
    sampled_ids = torch.bincount(torch.tensor(sample))
    for i, freq in enumerate(sampled_ids):
        print(f"{freq} x {inverse_vocab[i]}")

print_sampled_tokens(probas)
```

خروجی نمونه‌گیری به شکل زیر است:

```
73 x closer
0 x every
0 x effort
582 x forward
2 x inches
0 x moves
0 x pizza
343 x toward
```

همان‌طور که می‌بینیم، "forward" اغلب انتخاب شده اما توکن‌هایی مانند "closer"، "inches" و "toward" نیز گاهی انتخاب شده‌اند. این یعنی اگر در تابع `generate_and_print_sample` به جای `argmax` از `multinomial` استفاده کنیم، متن‌هایی مثل «every effort moves you toward» یا «every effort moves you inches» هم ممکن است تولید شوند.

---

#### مقیاس‌بندی دما (Temperature Scaling)

با استفاده از **مقیاس‌بندی دما**، می‌توانیم توزیع احتمال توکن‌ها را کنترل کنیم. این کار با تقسیم لاجیت‌ها بر یک عدد مثبت انجام می‌شود:

```python
def softmax_with_temperature(logits, temperature):
    scaled_logits = logits / temperature
    return torch.softmax(scaled_logits, dim=0)
```

- دماهای بالاتر از ۱ توزیع احتمال را یکنواخت‌تر می‌کنند (احتمال‌ها به هم نزدیک‌تر می‌شوند).
- دماهای کمتر از ۱ توزیع را تیزتر و قطعی‌تر می‌کنند.

برای درک بهتر، توزیع احتمال اصلی و توزیع‌های مقیاس‌یافته با دماهای مختلف را رسم می‌کنیم:

```python
temperatures = [1, 0.1, 5]
scaled_probas = [softmax_with_temperature(next_token_logits, T)
                for T in temperatures]
x = torch.arange(len(vocab))
bar_width = 0.15
fig, ax = plt.subplots(figsize=(5, 3))
for i, T in enumerate(temperatures):
    rects = ax.bar(x + i * bar_width, scaled_probas[i],
                   bar_width, label=f'Temperature = {T}')
ax.set_ylabel('Probability')
ax.set_xticks(x)
ax.set_xticklabels(vocab.keys(), rotation=90)
ax.legend()
plt.tight_layout()
plt.show()
```

**توضیح شکل (شکل ۵.۱۴):**

- دمای ۱ معادل عدم تغییر در توزیع احتمال است.
- کاهش دما به ۰.۱ باعث تیز شدن توزیع و افزایش احتمال "forward" می‌شود.
- افزایش دما به ۵ توزیع را یکنواخت‌تر کرده و احتمال سایر توکن‌ها را بیشتر می‌کند.

با دمای ۱، توکن "forward" حدود ۶۰٪ انتخاب می‌شود. دمای پایین مانند ۰.۱ تقریباً رفتار `argmax` را شبیه‌سازی می‌کند (انتخاب قطعی توکن پر احتمال). دمای بالا مثل ۵ باعث انتخاب‌های متنوع‌تر، اما گاهاً متن‌های بی‌معنی می‌شود، مثلاً متن‌هایی با توکن "pizza".

---

#### تمرین ۵.۱

با استفاده از تابع `print_sampled_tokens`، توزیع نمونه‌گیری توکن‌ها را برای دماهای مختلف شکل ۵.۱۴ چاپ کنید. در هر حالت، چند بار توکن "pizza" انتخاب شده است؟ آیا می‌توانید روش سریع‌تر و دقیق‌تری برای تعیین فراوانی انتخاب "pizza" پیشنهاد دهید؟

---

#### ۵.۳.۲ نمونه‌گیری Top-k

تا اینجا، نمونه‌گیری احتمالاتی به همراه مقیاس‌بندی دما را پیاده‌سازی کردیم تا تنوع نتایج افزایش یابد. دماهای بالاتر باعث توزیع یکنواخت‌تر و خروجی‌های متنوع‌تر می‌شوند اما گاهی منجر به جملات بی‌معنی مانند «every effort moves you pizza» می‌شوند.

برای بهبود کیفیت، می‌توان از **نمونه‌گیری Top-k** استفاده کرد. در این روش، فقط از میان **k** توکن با بیشترین احتمال نمونه‌گیری می‌کنیم و بقیه توکن‌ها را از انتخاب حذف می‌کنیم (با صفر کردن احتمال آن‌ها یا جایگزینی لاجیت‌شان با منفی بی‌نهایت).

---

**شرح تصویر (شکل ۵.۱۵):**

با تنظیم ( k=3 )، فقط سه توکن با بزرگ‌ترین لاجیت‌ها نگه داشته می‌شوند و بقیه لاجیت‌ها به (-\infty) تغییر می‌کنند. سپس با اعمال softmax احتمال بقیه توکن‌ها صفر می‌شود.

---

کد نمونه‌گیری Top-k به این صورت است:

```python
top_k = 3
top_logits, top_pos = torch.topk(next_token_logits, top_k)
print("Top logits:", top_logits)
print("Top positions:", top_pos)
```

خروجی لاجیت‌ها و اندیس‌های سه توکن برتر:

```
Top logits: tensor([6.7500, 6.2800, 4.5100])
Top positions: tensor([3, 7, 0])
```

سپس با استفاده از `torch.where` لاجیت‌های کمتر از کمترین لاجیت در این سه را به منفی بی‌نهایت تغییر می‌دهیم:

```python
new_logits = torch.where(
    condition=next_token_logits < top_logits[-1],    #1
    input=torch.tensor(float('-inf')),               #2
    other=next_token_logits                          #3
)
print(new_logits)
#1 شرط شناسایی لاجیت‌های کمتر از کمترین لاجیت در top-k
#2 اختصاص مقدار –inf به آن‌ها
#3 حفظ لاجیت اصلی سایر توکن‌ها
```

لاجیت‌های جدید:

```
tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800, -inf])
```

با اعمال softmax توزیع احتمال نهایی:

```python
topk_probas = torch.softmax(new_logits, dim=0)
print(topk_probas)
```

که نتیجه می‌شود:

```
tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])
```

در نهایت، می‌توانیم با ترکیب دما و نمونه‌گیری چندجمله‌ای (multinomial) از میان این سه توکن انتخاب کنیم و توکن بعدی را تولید نماییم. این کار را در ادامه با تغییر تابع تولید متن انجام خواهیم داد.

### ۵.۳.۳ تغییر تابع تولید متن

حال، ترکیبی از نمونه‌گیری با دما و نمونه‌گیری Top-k را در تابع `generate_text_simple` که قبلاً برای تولید متن با مدل زبان بزرگ (LLM) استفاده می‌کردیم، پیاده‌سازی می‌کنیم و یک تابع جدید به نام `generate` می‌سازیم.

---

**لیست ۵.۴: تابع تولید متن اصلاح شده با تنوع بیشتر**

```python
def generate(model, idx, max_new_tokens, context_size,
             temperature=0.0, top_k=None, eos_id=None):
    for _ in range(max_new_tokens):            #1
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]
        if top_k is not None:                #2
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(
                logits < min_val,
                torch.tensor(float('-inf')).to(logits.device),
                logits
            )
        if temperature > 0.0:                  #3
            logits = logits / temperature
            probs = torch.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
        else:    #4
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)
        if idx_next == eos_id:              #5
            break
        idx = torch.cat((idx, idx_next), dim=1)
    return idx
```

**توضیح خطوط کد:**

1. حلقه `for` مشابه قبل است؛ لاجیت‌ها را دریافت می‌کند و فقط به آخرین گام زمانی توجه می‌کند.
2. اعمال نمونه‌گیری Top-k با فیلتر کردن لاجیت‌های پایین‌تر از آستانه.
3. اعمال مقیاس‌بندی دما و نمونه‌گیری احتمالاتی.
4. اگر دما صفر یا کمتر باشد، انتخاب به صورت طماعانه (greedy) انجام می‌شود.
5. اگر توکن پایان دنباله (eos_id) تولید شود، تولید زودتر متوقف می‌شود.

---

### اجرای تابع generate با تنظیمات نمونه

```python
torch.manual_seed(123)
token_ids = generate(
    model=model,
    idx=text_to_token_ids("Every effort moves you", tokenizer),
    max_new_tokens=15,
    context_size=GPT_CONFIG_124M["context_length"],
    top_k=25,
    temperature=1.4
)
print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```

خروجی تولید شده:

```
Output text:
 Every effort moves you stand to work on surprise, a one of us had gone
 with random-
```

همانطور که مشاهده می‌کنیم، متن تولیدی بسیار متفاوت از متن تولید شده با تابع ساده `generate_simple` در بخش ۵.۳ است (که جمله‌ای از داده‌های آموزشی بود).

---

### تمرین ۵.۲

با تنظیمات مختلف دما و Top-k بازی کنید. بر اساس مشاهدات خود، در چه کاربردهایی تنظیمات دما و Top-k پایین‌تر مطلوب است؟ و در چه کاربردهایی تنظیمات بالاتر ترجیح داده می‌شود؟
(پیشنهاد می‌شود این تمرین را پس از بارگذاری وزن‌های پیش‌آموزش‌دیده از OpenAI در انتهای فصل مجدداً بررسی کنید.)

---

### تمرین ۵.۳

چه ترکیب‌های مختلفی از پارامترهای تابع `generate` وجود دارد که رفتار **قطعی** (deterministic) را تضمین کند؛ یعنی نمونه‌گیری تصادفی غیرفعال شود و خروجی همیشه مشابه خروجی تابع `generate_simple` باشد؟

---

### بخش ۵.۴ بارگذاری و ذخیره وزن‌های مدل در PyTorch

تا اینجا، نحوه ارزیابی عددی پیشرفت آموزش و پیش‌آموزش یک مدل زبان بزرگ (LLM) از ابتدا را بررسی کردیم. هرچند مدل و داده‌ها نسبتاً کوچک بودند، این تمرین نشان داد که پیش‌آموزش LLM‌ها نیازمند هزینه محاسباتی بالایی است. بنابراین، مهم است که بتوانیم مدل را ذخیره کنیم تا مجبور نباشیم هر بار برای استفاده در جلسه جدید، دوباره آن را آموزش دهیم.

در این بخش، نحوه ذخیره و بارگذاری مدل پیش‌آموزش‌دیده را که در شکل ۵.۱۶ نشان داده شده، توضیح می‌دهیم. سپس یک مدل GPT پیش‌آموزش‌دیده قوی‌تر از OpenAI را در نمونه GPTModel خود بارگذاری می‌کنیم.

---

**شکل ۵.۱۶:** پس از آموزش و بررسی مدل، معمولاً مفید است که مدل را ذخیره کنیم تا بعداً بتوانیم از آن استفاده کنیم یا آموزش آن را ادامه دهیم (مرحله ۶).

خوشبختانه، ذخیره مدل در PyTorch نسبتاً ساده است. روش پیشنهادی ذخیره `state_dict` مدل است؛ دیکشنری‌ای که هر لایه را به پارامترهای آن نگاشت می‌کند. برای این کار از تابع `torch.save` استفاده می‌کنیم:

```python
torch.save(model.state_dict(), "model.pth")
```

`model.pth` نام فایلی است که `state_dict` در آن ذخیره می‌شود. پسوند `.pth` در PyTorch متداول است اما می‌توان از هر پسوند دیگری نیز استفاده کرد.

بعد از ذخیره وزن‌ها، می‌توانیم آن‌ها را در یک نمونه جدید از مدل بارگذاری کنیم:

```python
model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(torch.load("model.pth", map_location=device))
model.eval()
```

همانطور که در فصل ۴ بحث شد، در حین آموزش، دراپ‌اوت از بیش‌برازش مدل جلوگیری می‌کند؛ اما هنگام استنتاج (Inference) نمی‌خواهیم اطلاعات مدل به صورت تصادفی حذف شوند. دستور `model.eval()` مدل را به حالت ارزیابی می‌برد و دراپ‌اوت را غیرفعال می‌کند. اگر قصد ادامه پیش‌آموزش مدل را داشته باشیم، ذخیره وضعیت بهینه‌ساز نیز توصیه می‌شود.

بهینه‌سازهای تطبیقی مانند AdamW پارامترهای اضافی‌ای برای هر وزن مدل ذخیره می‌کنند که نرخ یادگیری هر پارامتر را پویا تنظیم می‌کند. بدون این اطلاعات، بهینه‌ساز ریست می‌شود و مدل ممکن است بهینه آموزش نبیند یا همگرا نشود که باعث از دست رفتن قابلیت تولید متن هم‌بسته خواهد شد. با `torch.save` می‌توانیم وضعیت مدل و بهینه‌ساز را همزمان ذخیره کنیم:

```python
torch.save({
    "model_state_dict": model.state_dict(),
    "optimizer_state_dict": optimizer.state_dict(),
    },
    "model_and_optimizer.pth"
)
```

سپس برای بازگرداندن وضعیت، ابتدا داده‌ها را بارگذاری کرده و سپس با `load_state_dict` وضعیت‌ها را بازنشانی می‌کنیم:

```python
checkpoint = torch.load("model_and_optimizer.pth", map_location=device)
model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(checkpoint["model_state_dict"])
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
model.train();
```

---

**تمرین ۵.۴:**
پس از ذخیره وزن‌ها، مدل و بهینه‌ساز را در یک جلسه جدید پایتون یا Jupyter بارگذاری کنید و با تابع `train_model_simple` یک دوره آموزش دیگر ادامه دهید.

---

## بخش ۵.۵ بارگذاری وزن‌های پیش‌آموزش‌دیده از OpenAI

قبلاً یک مدل کوچک GPT-2 را روی مجموعه داده‌ای محدود (کتاب داستان کوتاه) آموزش دادیم تا اصول پایه را بدون صرف زمان و منابع زیاد بیاموزیم.

خوشبختانه OpenAI وزن‌های مدل‌های GPT-2 خود را به صورت متن‌باز منتشر کرده است، پس نیازی به صرف هزینه‌های زیاد برای آموزش مجدد مدل روی داده‌های بزرگ نیست. حالا این وزن‌ها را در کلاس `GPTModel` بارگذاری می‌کنیم و مدل را برای تولید متن به کار می‌بریم. وزن‌ها پارامترهای ذخیره‌شده در ویژگی `.weight` لایه‌های PyTorch مانند Linear و Embedding هستند.

در فصل ۶، از این وزن‌ها برای تنظیم دقیق (Fine-tuning) مدل در یک کار دسته‌بندی متن و پیروی از دستورات مانند ChatGPT استفاده خواهیم کرد.

---

**نکته:**
OpenAI وزن‌های GPT-2 را در ابتدا با TensorFlow ذخیره کرده بود که باید آن را نصب کنیم. همچنین برای نمایش پیشرفت دانلود از کتابخانه `tqdm` استفاده می‌شود:

```bash
pip install tensorflow>=2.15.0 tqdm>=4.66
```

کد دانلود نسبتا طولانی و یکنواخت است؛ بنابراین ما فایل `gpt_download.py` را مستقیماً از مخزن آنلاین این فصل دریافت می‌کنیم:

```python
import urllib.request
url = (
    "https://raw.githubusercontent.com/rasbt/"
    "LLMs-from-scratch/main/ch05/"
    "01_main-chapter-code/gpt_download.py"
)
filename = url.split('/')[-1]
urllib.request.urlretrieve(url, filename)
```

بعد از دانلود، بهتر است محتویات فایل را بررسی کنید تا از صحت آن مطمئن شوید.

حالا تابع `download_and_load_gpt2` را وارد می‌کنیم که تنظیمات معماری (`settings`) و پارامترهای وزن (`params`) را بارگذاری می‌کند:

```python
from gpt_download import download_and_load_gpt2
settings, params = download_and_load_gpt2(
    model_size="124M", models_dir="gpt2"
)
```

این کد هفت فایل مرتبط با مدل GPT-2 با ۱۲۴ میلیون پارامتر را دانلود می‌کند.

---

**چاپ تنظیمات و کلیدهای دیکشنری وزن‌ها:**

```python
print("Settings:", settings)
print("Parameter dictionary keys:", params.keys())
```

محتویات:

```
Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}
Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])
```

- `settings`: شامل تنظیمات معماری مدل مشابه `GPT_CONFIG_124M` است.
- `params`: دیکشنری وزن‌های واقعی مدل است.

می‌توانیم وزن‌های لایه جاسازی توکن را این‌گونه بررسی کنیم:

```python
print(params["wte"])
print("Token embedding weight tensor dimensions:", params["wte"].shape)
```

ابعاد وزن لایه جاسازی: `(50257, 768)`

---

OpenAI همچنین وزن مدل‌های بزرگتر با پارامترهای ۳۵۵M، ۷۷۴M و ۱۵۵۸M را منتشر کرده است که ساختار کلی مشابهی دارند اما اندازه جاسازی و تعداد لایه‌ها و سرهای توجه متفاوت است (شکل ۵.۱۷).

---

### آماده‌سازی پیکربندی مدل برای بارگذاری وزن‌ها

یک دیکشنری برای مشخصات مدل‌های مختلف GPT-2 تعریف می‌کنیم:

```python
model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}
```

برای بارگذاری مدل کوچک ۱۲۴M:

```python
model_name = "gpt2-small (124M)"
NEW_CONFIG = GPT_CONFIG_124M.copy()
NEW_CONFIG.update(model_configs[model_name])
```

توجه داشته باشید که مدل‌های اصلی GPT-2 با طول توکن ۱۰۲۴ آموزش دیده‌اند، پس باید این مقدار را به‌روزرسانی کنیم:

```python
NEW_CONFIG.update({"context_length": 1024})
```

همچنین OpenAI از بردارهای بایاس در لایه‌های خطی توجه چندسر استفاده کرده است که ما نیز برای سازگاری آن را فعال می‌کنیم:

```python
NEW_CONFIG.update({"qkv_bias": True})
```

حالا مدل را با این پیکربندی جدید مقداردهی اولیه می‌کنیم:

```python
gpt = GPTModel(NEW_CONFIG)
gpt.eval()
```

---

### بارگذاری وزن‌ها از دیکشنری `params` به مدل

یک تابع کمکی `assign` تعریف می‌کنیم که شکل دو آرایه را بررسی کرده و وزن‌ها را به صورت پارامترهای قابل آموزش PyTorch تبدیل می‌کند:

```python
def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, Right: {right.shape}")
    return torch.nn.Parameter(torch.tensor(right))
```

سپس تابع `load_weights_into_gpt` را تعریف می‌کنیم که وزن‌ها را به مدل منتقل می‌کند:

```python
import numpy as np

def load_weights_into_gpt(gpt, params):
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])

    for b in range(len(params["blocks"])):
        q_w, k_w, v_w = np.split(params["blocks"][b]["attn"]["c_attn"]["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(params["blocks"][b]["attn"]["c_attn"]["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(gpt.trf_blocks[b].att.out_proj.weight, params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(gpt.trf_blocks[b].att.out_proj.bias, params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(gpt.trf_blocks[b].ff.layers[0].weight, params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(gpt.trf_blocks[b].ff.layers[0].bias, params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(gpt.trf_blocks[b].ff.layers[2].weight, params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(gpt.trf_blocks[b].ff.layers[2].bias, params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(gpt.trf_blocks[b].norm1.scale, params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(gpt.trf_blocks[b].norm1.shift, params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(gpt.trf_blocks[b].norm2.scale, params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(gpt.trf_blocks[b].norm2.shift, params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])
```

- نکته مهم: OpenAI وزن‌های لایه خروجی را همان وزن‌های جاسازی توکن‌ها (weight tying) استفاده کرده که باعث کاهش تعداد کل پارامترها می‌شود.

---

### استفاده از تابع بارگذاری وزن‌ها و تولید متن

```python
load_weights_into_gpt(gpt, params)
gpt.to(device)

torch.manual_seed(123)
token_ids = generate(
    model=gpt,
    idx=text_to_token_ids("Every effort moves you", tokenizer).to(device),
    max_new_tokens=25,
    context_size=NEW_CONFIG["context_length"],
    top_k=50,
    temperature=1.5
)
print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```

خروجی نمونه:

```
Output text:
 Every effort moves you toward finding an ideal new way to practice something!
What makes us want to be on top of that?
```

این خروجی نشان می‌دهد که وزن‌ها به درستی بارگذاری شده‌اند، زیرا مدل متن هم‌بسته و معنادار تولید می‌کند.

---

### تمرین ۵.۵

**محاسبه‌ی مقادیر خطای آموزش و اعتبارسنجی (training و validation losses) برای مدل GPT با استفاده از وزن‌های پیش‌آموزش دیده شده از OpenAI روی داده‌های مجموعه "The Verdict".**

- هدف: با استفاده از وزن‌های پیش‌آموزش مدل GPT که از OpenAI بارگذاری شده‌اند، مقادیر خطا را روی مجموعه داده آموزشی و اعتبارسنجی محاسبه کنید.
- این کار به شما امکان می‌دهد کیفیت عملکرد مدل را روی داده‌های واقعی بررسی کنید و میزان تطابق مدل با داده‌های دیده شده و ندیده را بسنجید.

---

### تمرین ۵.۶

**آزمایش مدل‌های GPT-2 با اندازه‌های مختلف، به‌عنوان مثال مدل بزرگ‌ترین با 1,558 میلیون پارامتر، و مقایسه‌ی متن تولیدشده با مدل 124 میلیون پارامتری.**

- هدف: مدل‌های GPT-2 با اندازه‌های متفاوت را بارگذاری و اجرا کنید.
- سپس متن تولیدشده توسط این مدل‌ها را با یکدیگر مقایسه کنید تا تأثیر اندازه مدل بر کیفیت و تنوع متن خروجی را بررسی کنید.
- این تمرین به شما درک بهتری از رابطه بین اندازه مدل و توانایی تولید متن دقیق و منسجم می‌دهد.

## خلاصه

- هنگام تولید متن توسط مدل‌های زبان بزرگ (LLMs)، خروجی به صورت گام به گام و یک توکن در هر بار تولید می‌شود.
- به طور پیش‌فرض، توکن بعدی با تبدیل خروجی مدل به امتیازهای احتمالی و انتخاب توکنی که بیشترین احتمال را دارد تولید می‌شود که به آن «رمزگشایی حریصانه» (greedy decoding) گفته می‌شود.
- با استفاده از نمونه‌گیری احتمالاتی و تنظیم دما (temperature scaling)، می‌توان تنوع و انسجام متن تولیدشده را کنترل کرد.
- مقادیر خطا (loss) در مجموعه‌های آموزش و اعتبارسنجی می‌توانند برای ارزیابی کیفیت متن تولیدشده توسط مدل در طول آموزش به کار روند.
- پیش‌آموزش یک مدل زبان بزرگ شامل به‌روزرسانی وزن‌ها به منظور کمینه کردن خطای آموزش است.
- حلقه آموزش مدل‌های زبان بزرگ، خود یک روند استاندارد در یادگیری عمیق است که از تابع خطای متقاطع (cross entropy) و بهینه‌ساز AdamW استفاده می‌کند.
- پیش‌آموزش مدل روی مجموعه داده بزرگ، زمان‌بر و پرهزینه است؛ بنابراین می‌توان به جای پیش‌آموزش از وزن‌های پیش‌آموزش دیده شده و در دسترس عمومی استفاده کرد.


> [ 
        4.پیاده‌سازی مدل GPT از ابتدا برای تولید متن
     (قبلی) ](
        <04.Implementing a GPT model from scratch to generate text.md>
        ) <
    5.پیش‌آموزش روی داده‌های بدون برچسب>[
    6.تنظیم دقیق  برای دسته‌بندی 
(بعدی)
](<06.Fine-tuning for classification.md>)