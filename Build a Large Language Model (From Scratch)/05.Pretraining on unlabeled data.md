<!-- language: rtl -->

# 5.ูพุดโุขููุฒุด ุฑู ุฏุงุฏูโูุง ุจุฏูู ุจุฑฺุณุจ

# **ูพุดโูพุฑุฏุงุฒุด ุจุง ุฏุงุฏูโูุง ุจุฏูู ุจุฑฺุณุจ**

ุชุง ุงูุฌุงุ ูุง ูุฑุงุญู ูููููโุจุฑุฏุงุฑ ุงุฒ ุฏุงุฏู ู ูฺฉุงูุฒู ุชูุฌู (Attention) ุฑุง ูพุงุฏูโุณุงุฒ ฺฉุฑุฏู ู ูุนูุงุฑ ูุฏู ุฒุจุงู ุจุฒุฑฺฏ (LLM) ุฑุง ฺฉุฏููุณ ฺฉุฑุฏูโุงู. ุงฺฉููู ุฒูุงู ุขู ุฑุณุฏู ุงุณุช ฺฉู ฺฉ ุชุงุจุน ุขููุฒุด ุชุนุฑู ฺฉุฑุฏู ู ูุฏู LLM ุฑุง ูพุดโูพุฑุฏุงุฒุด (Pretrain) ฺฉูู. ุฏุฑ ุงู ูุฑุญููุ ุจุง ุฑูุดโูุง ูพุงูโุง ุงุฑุฒุงุจ ูุฏู ุขุดูุง ุฎูุงูู ุดุฏ ุชุง ฺฉูุช ูุชู ุฑุง ฺฉู ูุฏู ุชููุฏ ูโฺฉูุฏ ุจุณูุฌู โ ุงู ฺฉุงุฑ ุจุฑุง ุจูููโุณุงุฒ ูุฏู ุฏุฑ ุญู ุขููุฒุด ุถุฑูุฑ ุงุณุช.

ุนูุงููโ ุจุฑ ุงูุ ุจุฑุฑุณ ุฎูุงูู ฺฉุฑุฏ ฺฉู ฺฺฏููู ูโุชูุงู ูุฒูโูุง ุงุฒ ูพุด ุขููุฒุดโุฏุฏู (Pretrained Weights) ุฑุง ุจุงุฑฺฏุฐุงุฑ ฺฉุฑุฏ ุชุง ูุฏู ูุง ููุทู ุดุฑูุน ููโุชุฑ ุจุฑุง ุชูุธู ุฏูู (Fine-tuning) ุฏุงุดุชู ุจุงุดุฏ.

**ุดฺฉู 5.1** ููุง ฺฉู ุจุฑูุงููโุฑุฒ ูุง ุฑุง ูุดุงู ูโุฏูุฏ ู ูุฑุญููโุง ุฑุง ฺฉู ุฏุฑ ุงู ูุตู ุจู ุขู ุฎูุงูู ูพุฑุฏุงุฎุช (ูุฑุญูู ฒ: ูพุดโูพุฑุฏุงุฒุด ูุฏู LLM) ุจุฑุฌุณุชู ูโฺฉูุฏ. ุงู ูุตู ุดุงูู ูพุงุฏูโุณุงุฒ ฺฉุฏ ุขููุฒุด (ูุฑุญูู ต)ุ ุงุฑุฒุงุจ ุนููฺฉุฑุฏ (ูุฑุญูู ถ) ู ุฐุฎุฑูโุณุงุฒ/ุจุงุฑฺฏุฐุงุฑ ูุฒูโูุง ูุฏู (ูุฑุญูู ท) ุงุณุช.

---

### **ูพุงุฑุงูุชุฑูุง ูุฒู (Weight Parameters)**

ุฏุฑ ุฒููู ูุฏูโูุง ุฒุจุงู ุจุฒุฑฺฏ ู ุฏฺฏุฑ ูุฏูโูุง ุงุฏฺฏุฑ ุนููุ **ูุฒูโูุง** ุจู ูพุงุฑุงูุชุฑูุง ูุงุจูโุขููุฒุด ุงุดุงุฑู ุฏุงุฑูุฏ ฺฉู ุท ูุฑุงูุฏ ุงุฏฺฏุฑุ ุจูโุฑูุฒุฑุณุงู ูโุดููุฏ. ุงู ูุฒูโูุง ูุนูููุงู ุจุง ุนููุงู **ูพุงุฑุงูุชุฑูุง ูุฒู** ุง ุจู ุงุฎุชุตุงุฑ **ูพุงุฑุงูุชุฑูุง** ุดูุงุฎุชู ูโุดููุฏ.

ุฏุฑ ูุฑูโูุฑฺฉโูุง ูุงููุฏ PyTorchุ ุงู ูุฒูโูุง ุฏุฑ ูุงูโูุง ุฎุท (Linear Layers) ุฐุฎุฑู ูโุดููุฏ. ูุง ูุจูุงู ุงุฒ ุงู ูุงูโูุง ุจุฑุง ูพุงุฏูโุณุงุฒ ูุงฺูู ุชูุฌู ฺูุฏุณุฑ (Multi-head Attention) ุฏุฑ ูุตู ณ ู ฺฉูุงุณ `GPTModel` ุฏุฑ ูุตู ด ุงุณุชูุงุฏู ฺฉุฑุฏูโุงู.

ูพุณ ุงุฒ ููุฏุงุฑุฏู ุงููู ุจู ฺฉ ูุงู ูุงููุฏ:

```python
new_layer = torch.nn.Linear(...)
```

ูโุชูุงู ุจู ูุฒูโูุง ุขู ุงุฒ ุทุฑู ุตูุช `.weight` ูุงููุฏ `new_layer.weight` ุฏุณุชุฑุณ ุฏุงุดุช. ููฺููุ PyTorch ุจู ูุง ุงูฺฉุงู ูโุฏูุฏ ุชุง ุจูโุทูุฑ ูุณุชูู ุจู ููู ูพุงุฑุงูุชุฑูุง ูุงุจูโุขููุฒุด ฺฉ ูุฏู (ุงุฒ ุฌููู ูุฒูโูุง ู ุจุงุงุณโูุง) ุจุง ุงุณุชูุงุฏู ุงุฒ ูุชุฏ `model.parameters()` ุฏุณุชุฑุณ ูพุฏุง ฺฉููุ ุงุฒ ุงู ุฑูุด ุฏุฑ ุงุฏุงูู ุจุฑุง ูพุงุฏูโุณุงุฒ ุขููุฒุด ูุฏู ุงุณุชูุงุฏู ุฎูุงูู ฺฉุฑุฏ.

---

## **5.1 ุงุฑุฒุงุจ ูุฏูโูุง ูููุฏ ูุชู**

ูพุด ุงุฒ ุงุฑุฒุงุจุ ุงุจุชุฏุง ูุฑูุฑ ฺฉูุชุงู ุจุฑ ูุฑุงูุฏ ุชููุฏ ูุชู ุฎูุงูู ุฏุงุดุช ฺฉู ุฏุฑ ูุตู ด ูพุงุฏูโุณุงุฒ ฺฉุฑุฏู. ุณูพุณ ูุฏู LLM ุฑุง ุจุฑุง ุชููุฏ ูุชู ุขูุงุฏู ูโฺฉูู ู ุฏุฑ ุงุฏุงููุ ุฑูุดโูุง ูพุงู ุจุฑุง ุณูุฌุด ฺฉูุช ูุชู ุชููุฏุดุฏู ุฑุง ุจุฑุฑุณ ุฎูุงูู ฺฉุฑุฏ. ุฏุฑ ููุงุชุ ููุฏุงุฑ **ุฎุทุง ุขููุฒุด** ู **ุฎุทุง ุงุนุชุจุงุฑุณูุฌ** (Training and Validation Loss) ุฑุง ูุญุงุณุจู ุฎูุงูู ฺฉุฑุฏ.

**ุดฺฉู 5.2** ููุถูุนุงุช ุชุญุช ูพูุดุด ุงู ูุตู ุฑุง ููุงุด ูโุฏูุฏ ฺฉู ุฏุฑ ุญุงู ุญุงุถุฑ ุฑู ุณู ูุฑุญูู ุงูู ูุชูุฑฺฉุฒ ูุณุชู.

---

### **5.1.1 ุงุณุชูุงุฏู ุงุฒ GPT ุจุฑุง ุชููุฏ ูุชู**

ุจุงุฏ ูุฏู LLM ุฑุง ููุฏุงุฑุฏู ุงููู ฺฉุฑุฏู ู ูุฑูุฑ ฺฉูุชุงู ุจุฑ ูุฑุงูุฏ ุชููุฏ ูุชู ุฏุงุดุชู ุจุงุดู ฺฉู ุฏุฑ ูุตู ด ูพุงุฏูโุณุงุฒ ฺฉุฑุฏูโุงู. ุจุฑุง ุงู ฺฉุงุฑุ ุงุฒ ฺฉูุงุณ `GPTModel` ู ูพฺฉุฑุจูุฏ `GPT_CONFIG_124M` ุงุณุชูุงุฏู ุฎูุงูู ฺฉุฑุฏ:

```python
import torch
from chapter04 import GPTModel

GPT_CONFIG_124M = {
    "vocab_size": 50257,
    "context_length": 256,
    "emb_dim": 768,
    "n_heads": 12,
    "n_layers": 12,
    "drop_rate": 0.1,
    "qkv_bias": False
}

torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.eval()
```

ุฏุฑ ุงู ูพฺฉุฑุจูุฏุ ุชููุง ุชูุงูุช ุจุง ูุตู ฺฏุฐุดุชูุ ฺฉุงูุด ุทูู ฺฉุงูุชฺฉุณุช (**context_length**) ุจู ฒตถ ุชูฺฉู ุงุณุช. ุงู ุชุบุฑ ุจุงุนุซ ูโุดูุฏ ูุงุฒ ูุญุงุณุจุงุช ูุฏู ฺฉุงูุด ุงูุชู ู ุจุชูุงู ุขู ุฑุง ุฑู ฺฉ ููพโุชุงูพ ูุนููู ุขููุฒุด ุฏุงุฏ.

ุฏุฑ ูุณุฎู ุงุตู GPT-2 ุจุง ฑฒด ูููู ูพุงุฑุงูุชุฑุ ุทูู ฺฉุงูุชฺฉุณุช ุชุง ฑฐฒด ุชูฺฉู ุจูุฏ. ูพุณ ุงุฒ ุงุชูุงู ุขููุฒุดุ ุงู ููุฏุงุฑ ุฑุง ูุฌุฏุฏุงู ุงูุฒุงุด ุฏุงุฏู ู ูุฒูโูุง ุขููุฒุดโุฏุฏู ุฑุง ุจุงุฑฺฏุฐุงุฑ ุฎูุงูู ฺฉุฑุฏ ุชุง ุงุฒ ฺฉุงูุชฺฉุณุช ฑฐฒด ุชูฺฉู ุจูุฑู ุจฺฏุฑู.

ุจุฑุง ุชููุฏ ูุชู ุจุง ุงุณุชูุงุฏู ุงุฒ ูููููโ ูุฏู `GPTModel`ุ ุงุฒ ุชุงุจุน `generate_text_simple` (ฺฉู ุฏุฑ ูุตู ด ูุนุฑู ุดุฏ) ุจูุฑู ูโฺฏุฑู. ุฏุฑ ฺฉูุงุฑ ุขูุ ุฏู ุชุงุจุน ููุฏ ุชุนุฑู ูโฺฉูู:

- `text_to_token_ids`: ุจุฑุง ุชุจุฏู ูุชู ุจู ุดูุงุณูโูุง ุชูฺฉู
- `token_ids_to_text`: ุจุฑุง ุจุงุฒฺฏุฑุฏุงูุฏู ุดูุงุณูโูุง ุชูฺฉู ุจู ูุชู

ุงู ุชูุงุจุน ุฏุฑ ุณุฑุงุณุฑ ุงู ูุตู ุจุฑุง ุชุจุฏู ุจู ูุชู ู ููุงุด ุนุฏุฏ ุขู ุงุณุชูุงุฏู ุฎูุงููุฏ ุดุฏ.

---

#### **ูุฑุงูุฏ ุชููุฏ ูุชู ุจุง GPT**

**ุดฺฉู 5.3** ูุฑุงูุฏ ุณูโูุฑุญููโุง ุชููุฏ ูุชู ุฑุง ุจุง ุงุณุชูุงุฏู ุงุฒ ูุฏู GPT ูุดุงู ูโุฏูุฏ:

1. **ุชูฺฉูโุณุงุฒ**: ุงุจุชุฏุง ูุชู ูุฑูุฏ ุจู ุดูุงุณูโูุง ุชูฺฉู ุชุจุฏู ูโุดูุฏ (ูุตู ฒ).
2. **ูพุดโุจู**: ุณูพุณ ุงู ุดูุงุณูโูุง ุจู ูุฏู ุฏุงุฏู ูโุดููุฏ ุชุง ุจุฑุฏุงุฑูุง ุจู ูุงู **logits** ุชููุฏ ุดูุฏุ ุงู ุจุฑุฏุงุฑูุง ุชูุฒุน ุงุญุชูุงู ูุฑ ุชูฺฉู ุฏุฑ ูุงฺฺฏุงู ุฑุง ูุดุงู ูโุฏููุฏ (ูุตู ด).
3. **ุจุงุฒุชูฺฉูโุณุงุฒ**: ููุงุชุงูุ logitsูุง ุจู ุชูฺฉูโูุง ุฎุฑูุฌ ุชุจุฏู ุดุฏู ู ุชูุณุท ุชูฺฉูุงุฒุฑ ุจู ูุชู ูุงุจูโุฎูุงูุฏู ุชุจุฏู ูโุดููุฏ.

---

#### **ูุณุช 5.1 โ ุชูุงุจุน ฺฉูฺฉ ุจุฑุง ุชุจุฏู ูุชู ุจู ุดูุงุณู ุชูฺฉู**

```python
import tiktoken
from chapter04 import generate_text_simple

def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)
    return encoded_tensor

def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)
    return tokenizer.decode(flat.tolist())

start_context = "Every effort moves you"
tokenizer = tiktoken.get_encoding("gpt2")

token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(start_context, tokenizer),
    max_new_tokens=10,
    context_size=GPT_CONFIG_124M["context_length"]
)

print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```

#### **ุฎุฑูุฌ ุงุญุชูุงู:**

```
Output text:
Every effort moves you rentingetic wasn? refres RexMeCHicular stren
```

---

#### **ุชุญูู ุฎุฑูุฌ**

ููุงูโุทูุฑ ฺฉู ูุดุงูุฏู ูโฺฉูุฏุ ูุฏู ูููุฒ ูุชู ููุทู ุชููุฏ ููโฺฉูุฏ. ุฏูู ุขู ุณุงุฏู ุงุณุช: ูุฏู ูููุฒ ุขููุฒุด ูุฏุฏู ุงุณุช. ุจุฑุง ุงูฺฉู ุจุชูุงูู ุชุนู ฺฉูู ฺฉ ูุชู ยซููุทูยป ุง ยซุจุง ฺฉูุชยป ุงุณุชุ ุจุงุฏ ุงุฒ ุฑูุด ุนุฏุฏ ุจุฑุง ุงุฑุฒุงุจ ุงุณุชูุงุฏู ฺฉูู. ุงู ฺฉุงุฑ ุจู ูุง ุงูฺฉุงู ูโุฏูุฏ ฺฉู ุนููฺฉุฑุฏ ูุฏู ุฑุง ุฏุฑ ุทูู ุขููุฒุด ุฑุตุฏ ฺฉุฑุฏู ู ุจูุจูุฏ ุฏูู.

ุฏุฑ ุงุฏุงููุ ฺฉ **ูุนุงุฑ ุฎุทุง (Loss Function)** ุฑุง ุจุฑุง ุฎุฑูุฌโูุง ุชููุฏุดุฏู ูุญุงุณุจู ุฎูุงูู ฺฉุฑุฏ. ุงู ูุนุงุฑ ุจู ุนููุงู ูุดุงูฺฏุฑ ูพุดุฑูุช ู ููููุช ุขููุฒุด ุนูู ูโฺฉูุฏ. ููฺูู ุฏุฑ ูุตูโูุง ุขูุฏูุ ููฺฏุงู ุชูุธู ุฏูู ูุฏูุ ุจุง ุฑูุดโูุง ุจุดุชุฑ ุจุฑุง ุงุฑุฒุงุจ ฺฉูุช ุขุดูุง ุฎูุงูู ุดุฏ.

---

ุงฺฏุฑ ุขูุงุฏู ูุณุชุฏุ ุงุฏุงูู ูุชู ุฑุง ุจูุฑุณุชุฏ ุชุง ุจุง ููู ุฑูฺฉุฑุฏ ุงุฏุงูู ุจุฏูู.

ุฏุฑ ุงุฏุงููุ ุชุฑุฌูู ู ุชูุถุญ ุนููุ ุฏูู ู ุฑูุงู ุจุฎุด 5.1.2 ุฑุง ูุทุงุจู ุจุง ุงุตูู ููุฑุฏ ูุธุฑ ุดูุง ุงุฑุงุฆู ูโุฏูู. ฺฉุฏูุง ุนูุงู ู ุจุฏูู ุชุบุฑ ุฏุฑ ูุงูุจ Markdown ุฏุฑุฌ ุดุฏูโุงูุฏ.

---

### **5.1.2 ูุญุงุณุจูโ ุฎุทุง ุชููุฏ ูุชู (Text Generation Loss)**

ุฏุฑ ุงู ุจุฎุดุ ุจุง ุฑูุดโูุง ุนุฏุฏ ุจุฑุง ุงุฑุฒุงุจ ฺฉูุช ูุชู ฺฉู ูุฏู ุฏุฑ ุญู ุขููุฒุด ุชููุฏ ูโฺฉูุฏ ุขุดูุง ูโุดูู. ุงู ฺฉุงุฑ ุฑุง ุจุง **ูุญุงุณุจูโ ฺฉ ูุนุงุฑ ุฎุทุง (loss)** ุงูุฌุงู ุฎูุงูู ุฏุงุฏ. ุจุฑุง ุฏุฑฺฉ ุจูุชุฑ ุงู ููุถูุนุ ุขู ุฑุง ูุฑุญููโุจูโูุฑุญูู ููุฑุงู ุจุง ฺฉ ูุซุงู ุนูู ุจุฑุฑุณ ูโฺฉููุ ุงุฒ ุจุงุฑฺฏุฐุงุฑ ุฏุงุฏูโูุง ฺฏุฑูุชู ุชุง ุชููุฏ ูุชู ุชูุณุท ุชุงุจุน `generate_text_simple`.

---

#### **ูุฑูุฑ ุจุฑ ูุฑุงูุฏ ุชููุฏ ูุชู**

**ุดฺฉู 5.4** ุฑููุฏ ฺฉู ุชููุฏ ูุชู ุฑุง ุฏุฑ ูพูุฌ ูุฑุญูู ูุดุงู ูโุฏูุฏ โ ุงุฒ ูุฑูุฏ ูุชู ุชุง ุฎุฑูุฌ ุชููุฏุดุฏู ุชูุณุท ูุฏู LLM. ุงู ุฑููุฏ ุฏุฑ ูุงูุน ฺฉุงุฑ ุงุณุช ฺฉู ุชุงุจุน `generate_text_simple` ุจูโุตูุฑุช ุฏุงุฎู ุงูุฌุงู ูโุฏูุฏ. ุจุฑุง ุงูฺฉู ุจุชูุงูู ููุฏุงุฑ ุฎุทุง ุฑุง ูุญุงุณุจู ฺฉููุ ุจุงุฏ ุฏููุงู ููู ูุฑุงุญู ุงุจุชุฏุง ุฑุง ุงุฌุฑุง ฺฉูู.

ูฺฉุชู: ุจุฑุง ุณุงุฏูโุณุงุฒุ ุฏุฑ **ุดฺฉู 5.4** ููุท ุงุฒ ฺฉ ูุงฺฺฏุงู ทโุชุง ุงุณุชูุงุฏู ุดุฏูุ ุงูุง ุฏุฑ ูุงูุนุชุ ูุฏู GPT ูุง ุจุง ูุงฺฺฏุงู ุจู ุงูุฏุงุฒูโ **ตฐูฌฒตท ุชูฺฉู** ฺฉุงุฑ ูโฺฉูุฏ. ุฏุฑ ูุชุฌูุ ุดูุงุณูโูุง ุชูฺฉู ุชููุฏ ุดุฏู ุฏุฑ ฺฉุฏูุง ุฒุฑ ุฏุฑ ุจุงุฒูโ ฐ ุชุง ตฐูฌฒตถ ูุฑุงุฑ ุฏุงุฑูุฏ.

---

#### **ููุฏุงุฑุฏู ุงููู ุฏุงุฏูโูุง ูุฑูุฏ ู ูุฏู**

ุฏุฑ ุงุฏุงููุ ุงุฒ ุฏู ูุซุงู ูุชู ุงุณุชูุงุฏู ูโฺฉูู:

- `"every effort moves"`
- `"I really like"`

ุดูุงุณูโูุง ุชูฺฉู ูุฑุจูุท ุจู ุงู ุฌููุงุช ุจูโุตูุฑุช ุฒุฑ ุชุนุฑู ูโุดููุฏ:

```python
inputs = torch.tensor([
    [16833, 3626, 6100],    # "every effort moves"
    [40, 1107, 588]         # "I really like"
])
```

ู ุดูุงุณูโูุง ุชูฺฉู ูุฏู (targets)ุ ฺฉู ฺฉ ูููุนุช ุฌููุชุฑ ุงุฒ ูุฑูุฏโูุง ูุณุชูุฏุ ุจูโุตูุฑุช ุฒุฑ ุชุนุฑู ุดุฏูโุงูุฏ:

```python
targets = torch.tensor([
    [3626, 6100, 345],      # "effort moves you"
    [1107, 588, 11311]      # "really like chocolate"
])
```

> โฌ๏ธ ุงู ุชฺฉูฺฉ **ุฌุงุจุฌุง ฺฉ ูููุนุช (shifted targets)** ุฏุฑ ูุตู ฒ ูุฒ ุชูุถุญ ุฏุงุฏู ุดุฏ ู ุจุฑุง ุขููุฒุด ูุฏูโูุง ุฒุจุงู ุถุฑูุฑ ุงุณุชุ ฺุฑุง ฺฉู ูุฏู ุฑุง ูุงุฏุงุฑ ูโฺฉูุฏ ยซุชูฺฉู ุจุนุฏยป ุฑุง ุงุฏ ุจฺฏุฑุฏ.

---

#### **ุชุจุฏู logits ุจู ุงุญุชูุงูุงุช (Softmax)**

ูุฑูุฏโูุง ุฑุง ุจู ูุฏู ูโุฏูู ู ุงุฒ ุฎุฑูุฌ `logits`ุ ููุงุฏุฑ ุงุญุชูุงู (probabilities) ุฑุง ุจุง ุชุงุจุน Softmax ูุญุงุณุจู ูโฺฉูู:

```python
with torch.no_grad():
    logits = model(inputs)
    probas = torch.softmax(logits, dim=-1)
    print(probas.shape)
```

ุฎุฑูุฌ:

```
torch.Size([2, 3, 50257])
```

> ๐ ุงู ุงุจุนุงุฏ ุจูโุชุฑุชุจ ุจู ููุงุฑุฏ ุฒุฑ ุงุดุงุฑู ุฏุงุฑูุฏ:
>
> - ฒ: ุชุนุฏุงุฏ ูููููโูุง (batch size)
> - ณ: ุชุนุฏุงุฏ ุชูฺฉูโูุง ุฏุฑ ูุฑ ููููู
> - ตฐูฌฒตท: ุงูุฏุงุฒูโ ูุงฺฺฏุงู (vocab size)

---

#### **ูุฑุญููโ ุจุนุฏ โ ุงูุชุฎุงุจ ุชูฺฉู ุฎุฑูุฌ ุจุง ุจุดุชุฑู ุงุญุชูุงู (Argmax)**

```python
token_ids = torch.argmax(probas, dim=-1, keepdim=True)
print("Token IDs:\n", token_ids)
```

ุฎุฑูุฌ:

```
Token IDs:
tensor([[[16657],
         [  339],
         [42826]],
        [[49906],
         [29669],
         [41751]]])
```

---

#### **ููุงุณู ุฎุฑูุฌ ูุฏู ุจุง ูุฏู (Target)**

```python
print(f"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}")
print(f"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}")
```

ุฎุฑูุฌ:

```
Targets batch 1: effort moves you
Outputs batch 1: Armed heNetflix
```

> โ ููุงูโุทูุฑ ฺฉู ูุดุฎุต ุงุณุชุ ุฎุฑูุฌ ูุฏู ฺฉุงููุงู ุชุตุงุฏู ุงุณุช ู ุงุฑุชุจุงุท ุจุง ุฌูููโ ูุฏู ูุฏุงุฑุฏุ ุฏูู ุงู ููุถูุนุ ุขููุฒุด ูุฏุฏู ุจูุฏู ูุฏู ุงุณุช.

---

#### **ูุญุงุณุจูโ ุนุฏุฏ ุฎุทุง (Loss) ุจุฑุง ุงุฑุฒุงุจ ุนููฺฉุฑุฏ**

ุฏุฑ ุงุฏุงููุ ูโุฎูุงูู **ููุฏุงุฑ ุฎุทุง ุฑุง ุจูโุตูุฑุช ุนุฏุฏ ูุญุงุณุจู ฺฉูู**. ุงู ููุฏุงุฑ ูู ุจู ูุง ุฏุฑ ุงุฑุฒุงุจ ฺฉูุช ุฎุฑูุฌ ฺฉูฺฉ ูโฺฉูุฏ ู ูู ุณูฺฏโุจูุง ุชุนุฑู ุชุงุจุน ุขููุฒุด ูุฏู ุงุณุช.

ูุฏู ููุง ุขููุฒุด ูุฏู ุงู ุงุณุช ฺฉู **ุงุญุชูุงู (Softmax probability)** ุงุฎุชุตุงุตโุงูุชู ุจู ุชูฺฉูโูุง ุตุญุญ (ููุงุจู ูุฏูโูุง) ุฑุง **ุงูุฒุงุด ุฏูุฏ**.

---

#### **ููุงุด ุงุญุชูุงู ุงููู ุชูฺฉู ูุฏู ุฏุฑ ุฎุฑูุฌ ูุฏู**

```python
text_idx = 0
target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]
print("Text 1:", target_probas_1)

text_idx = 1
target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]
print("Text 2:", target_probas_2)
```

ุฎุฑูุฌ:

```
Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])
Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])
```

> ๐ ุงู ููุงุฏุฑ ุจุณุงุฑ ฺฉูฺฺฉ ูุณุชูุฏุ ฺูู ูุฏู ูููุฒ ุชุตุงุฏู ุนูู ูโฺฉูุฏ. (ุฏุฑ ูุงฺฺฏุงู ตฐูฌฒตทโุชุงุ ุงุญุชูุงู ุงููู ูุฑ ุชูฺฉู ุชูุฑุจุงู 0.00002 ุงุณุช)

---

#### **Backpropagation ู ุชุงุจุน ุฎุทุง**

ุจุฑุง ุจูโุฑูุฒุฑุณุงู ูุฒูโูุง ูุฏู ุจูโุทูุฑโฺฉู ุงุญุชูุงู ุชูฺฉู ุตุญุญ ุงูุฒุงุด ุงุจุฏุ ุงุฒ **ุฑูุด ูพุณโุงูุชุดุงุฑ (Backpropagation)** ุงุณุชูุงุฏู ูโฺฉูู. ุงู ุฑูุด ูุงุฒ ุจู ฺฉ **ุชุงุจุน ุฎุทุง (Loss Function)** ุฏุงุฑุฏ ฺฉู ุชูุงูุช ุจู ุฎุฑูุฌ ูพุดโุจูโุดุฏู ู ุฎุฑูุฌ ูุงูุน ุฑุง ุงูุฏุงุฒูโฺฏุฑ ฺฉูุฏ.

---

#### **ูุญุงุณุจู log ุงุญุชูุงูุงุช ูุฏู**

```python
log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))
print(log_probas)
```

ุฎุฑูุฌ:

```
tensor([ -9.5042, -10.3796, -11.3677, -11.4798, -9.7764, -12.2561])
```

> โ๏ธ ฺฉุงุฑ ุจุง ูฺฏุงุฑุชู ุงุญุชูุงูโูุง ุฏุฑ ุจูููโุณุงุฒ ุนุฏุฏ ุณุงุฏูโุชุฑ ุงุฒ ฺฉุงุฑ ุจุง ุฎูุฏ ุงุญุชูุงูโูุงุณุช.

---

#### **ูุงูฺฏูโฺฏุฑ ุงุฒ log ุงุญุชูุงูุงุช**

```python
avg_log_probas = torch.mean(log_probas)
print(avg_log_probas)
```

ุฎุฑูุฌ:

```
tensor(-10.7940)
```

---

#### **ูุญุงุณุจู ููุฏุงุฑ ููุง ุฎุทุง (Cross-Entropy Loss)**

ุฏุฑ ุงุฏฺฏุฑ ุนููุ ูุนููู ุงุณุช ฺฉู **ูุงูฺฏู log ุงุญุชูุงูโูุง ุฑุง ููู ฺฉูู** ุชุง ููุฏุงุฑ loss ููุง ุจูโุฏุณุช ุขุฏ:

```python
neg_avg_log_probas = avg_log_probas * -1
print(neg_avg_log_probas)
```

ุฎุฑูุฌ:

```
tensor(10.7940)
```

> ๐บ ุงู ููุฏุงุฑ ููุงู **Cross Entropy Loss** ุงุณุชุ ูุฑฺู ุจู ุตูุฑ ูุฒุฏฺฉโุชุฑ ุจุงุดุฏุ ุฎุฑูุฌ ูุฏู ุจู ูุฏู ูุฒุฏฺฉโุชุฑ ุงุณุช.

๐ฆ ุฎูุดุจุฎุชุงููุ ุฏุฑ PyTorchุ ุชูุงู ุงู ูุฑุงุญู ุฏุฑ ฺฉ ุชุงุจุน ุขูุงุฏู ุจูโูุงู `torch.nn.functional.cross_entropy` ูพุงุฏูโุณุงุฒ ุดุฏูโุงูุฏ ู ูุงุฒ ุจู ุงูุฌุงู ุฏุณุช ูุฑุงุญู ุจุงูุง ุฏุฑ ุนูู ูุณุช.

---

#### โ ุฌูุนโุจูุฏ ุงู ุจุฎุด:

- ูุฏู ูุนูุงู ุนููฺฉุฑุฏ ุถุนู ุฏุงุฑุฏ ฺูู ุขููุฒุด ูุฏุฏู ุงุณุช.
- ุจุง ุงุณุชูุงุฏู ุงุฒ **ุงุญุชูุงู Softmax** ู **ูฺฏุงุฑุชู ุขูโูุง**ุ ูโุชูุงู ุนููฺฉุฑุฏ ูุฏู ุฑุง ุนุฏุฏ ุณูุฌุฏ.
- ุงู ููุฏุงุฑ **ูุจูุง ุขููุฒุด ูุฏู** ุฎูุงูุฏ ุจูุฏุ ุฒุฑุง ุจุง ฺฉููู ฺฉุฑุฏู ุงู lossุ ูุฒูโูุง ูุฏู ุทูุฑ ุจูโุฑูุฒุฑุณุงู ูโุดููุฏ ฺฉู ุฎุฑูุฌ ุชููุฏุดุฏู ุจู ูุฏู ูุฒุฏฺฉโุชุฑ ุดูุฏ.
- ุฏุฑ ุงุฏุงููุ ุงุฒ ููู ูฺฉุงูุฒู ุจุฑุง ููุดุชู ุชุงุจุน ุขููุฒุด ูุฏู ุงุณุชูุงุฏู ุฎูุงูู ฺฉุฑุฏ.

---

#### **Cross Entropy Loss (ุฎุทุง ุขูุชุฑููพ ูุชูุงุทุน)**

ุฎุทุง ุขูุชุฑููพ ูุชูุงุทุนุ ฺฉ ุงุฒ ูุนุงุฑูุง ุจุณุงุฑ ูุญุจูุจ ุฏุฑ ุงุฏฺฏุฑ ูุงุดู ู ุงุฏฺฏุฑ ุนูู ุงุณุช ฺฉู ุงุฎุชูุงู ุจู ุฏู ุชูุฒุน ุงุญุชูุงู ุฑุง ุงูุฏุงุฒู ูโฺฏุฑุฏ:

- ุชูุฒุน ูุงูุน (ูุงููุฏ ุชูฺฉูโูุง ูุฏู ุฏุฑ ุฏุงุฏูโูุง)
- ุชูุฒุน ูพุดโุจู ุดุฏู ุชูุณุท ูุฏู (ูุงููุฏ ุงุญุชูุงูโูุง ุชููุฏุดุฏู ุจุฑุง ุชูฺฉูโูุง ุชูุณุท ูุฏู ุฒุจุงู ุจุฒุฑฺฏ)

ุฏุฑ ฺุงุฑฺูุจโูุง ูุงููุฏ PyTorchุ ุชุงุจุน `cross_entropy` ุงู ุงุฎุชูุงู ุฑุง ุจุฑุง ุญุงูุชโูุง ฺฏุณุณุชู ูุญุงุณุจู ูโฺฉูุฏ. ุงู ููุฏุงุฑ ุจู ูุนูุง ููู ูุงูฺฏู ูฺฏุงุฑุชู ุงุญุชูุงู ุชูฺฉูโูุง ูุฏู ุงุณุช ฺฉู ูุฏู ูพุดโุจู ฺฉุฑุฏู ุงุณุช. ุจู ููู ุฏููุ ุงุตุทูุงุญุงุช **cross entropy** ู **negative average log probability** ูุนูููุงู ุจู ุฌุง ูู ุงุณุชูุงุฏู ูโุดููุฏ.

---

#### ุงุฏุขูุฑ ุงุจุนุงุฏ `logits` ู `targets`

```python
print("Logits shape:", logits.shape)
print("Targets shape:", targets.shape)
```

ุฎุฑูุฌ:

```
Logits shape: torch.Size([2, 3, 50257])
Targets shape: torch.Size([2, 3])
```

- `logits` ฺฉ ุชูุณูุฑ ุณูโุจุนุฏ ุงุณุช ฺฉู ุงุจุนุงุฏ ุขู ุจูโุชุฑุชุจ:
  (ุชุนุฏุงุฏ ูููููโูุงุ ุชุนุฏุงุฏ ุชูฺฉูโูุงุ ุงูุฏุงุฒู ูุงฺฺฏุงู)
- `targets` ฺฉ ุชูุณูุฑ ุฏูโุจุนุฏ ุงุณุช ุจุง ุงุจุนุงุฏ:
  (ุชุนุฏุงุฏ ูููููโูุงุ ุชุนุฏุงุฏ ุชูฺฉูโูุง)

---

#### ุชุจุฏู ุจู ุญุงูุช ูุณุทุญ (Flattening)

ุจุฑุง ุงุณุชูุงุฏู ุงุฒ ุชุงุจุน `cross_entropy` ุฏุฑ PyTorchุ ุจุงุฏ ุงู ุฏู ุชูุณูุฑ ุฑุง ุฑู ุงุจุนุงุฏ batch ู token ุจุง ูู ุชุฑฺฉุจ ฺฉูู (flatten ฺฉูู):

```python
logits_flat = logits.flatten(0, 1)
targets_flat = targets.flatten()
print("Flattened logits:", logits_flat.shape)
print("Flattened targets:", targets_flat.shape)
```

ุฎุฑูุฌ:

```
Flattened logits: torch.Size([6, 50257])
Flattened targets: torch.Size([6])
```

---

#### ูุญุงุณุจู loss ุจุง ุงุณุชูุงุฏู ุงุฒ `cross_entropy`

ุชุงุจุน `cross_entropy` ุฏุฑ PyTorch ุชูุงู ูุฑุงุญู ุฒุฑ ุฑุง ุจู ุตูุฑุช ุฎูุฏฺฉุงุฑ ุงูุฌุงู ูโุฏูุฏ:

- ุงุนูุงู ุชุงุจุน Softmax ุฑู `logits`
- ุงูุชุฎุงุจ ุงุญุชูุงูโูุง ูุฑุจูุท ุจู ุชูฺฉูโูุง ูุฏู
- ูุญุงุณุจู ูฺฏุงุฑุชู ุงุญุชูุงูุงุช
- ูุงูฺฏูโฺฏุฑ ู ููู ฺฉุฑุฏู ุจุฑุง ฺฏุฑูุชู Cross Entropy

ฺฉุฏ:

```python
loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)
print(loss)
```

ุฎุฑูุฌ:

```
tensor(10.7940)
```

> ุงู ููุฏุงุฑ ุฏููุงู ููุงู ููุฏุงุฑ ุงุณุช ฺฉู ูุจูุงู ุจู ุตูุฑุช ุฏุณุช ู ูุฑุญูู ุจู ูุฑุญูู ูุญุงุณุจู ฺฉุฑุฏู.

---

#### **Perplexity (ุชุฑุฏุฏ ูุฏู)**

ูพูุฑูพูููฺฉุณุช ูุนุงุฑ ุงุณุช ฺฉู ูุนูููุงู ููุฑุงู ุจุง cross entropy ุจุฑุง ุงุฑุฒุงุจ ูุฏูโูุง ุฒุจุงู ุงุณุชูุงุฏู ูโุดูุฏ ู ุชูุณุฑ ุจูุชุฑ ุงุฒ ุนุฏู ูุทุนุช ูุฏู ุฏุฑ ูพุดโุจู ุชูฺฉู ุจุนุฏ ูุฑุงูู ูโฺฉูุฏ.

- **Perplexity** ูุดุงู ูโุฏูุฏ ฺฉู ูุฏู ฺูุฏุฑ ุฏุฑ ุชุทุงุจู ุชูุฒุน ูพุดโุจู ุดุฏู ุจุง ุชูุฒุน ูุงูุน ุฏุงุฏูโูุง ูููู ุงุณุช.
- ูุฑฺู ููุฏุงุฑ perplexity ฺฉูุชุฑ ุจุงุดุฏุ ูุฏู ุนููฺฉุฑุฏ ุจูุชุฑ ุฏุงุฑุฏ ู ุงุญุชูุงู ุชูฺฉูโูุง ุตุญุญ ุฑุง ุจูุชุฑ ูพุดโุจู ูโฺฉูุฏ.

---

#### ูุญุงุณุจู perplexity ุงุฒ loss

ูุฑููู:

```python
perplexity = torch.exp(loss)
print(perplexity)
```

ุฎุฑูุฌ:

```
tensor(48725.8203)
```

---

#### ุชูุณุฑ ุนุฏุฏ perplexity

- ููุฏุงุฑ perplexity ูุนุงุฏู ุงูุฏุงุฒูโ ูุงฺฺฏุงู ููุซุฑ ุงุณุช ฺฉู ูุฏู ุฏุฑุจุงุฑู ุขูโูุง ุฏุฑ ูุฑ ฺฏุงู ูพุดโุจู ูุงูุทูุฆู ุงุณุช.
- ุฏุฑ ุงู ูุซุงูุ ููุฏุงุฑ ุชูุฑุจ 48,725 ุจู ุงู ูุนูุงุณุช ฺฉู ูุฏู ุฏุฑ ุงูุชุฎุงุจ ุชูฺฉู ุจุนุฏ ุฏุฑ ุจู ุญุฏูุฏ 48,725 ุชูฺฉู ููุฌูุฏ ุฏุฑ ูุงฺฺฏุงู ฺฉุงููุงู ูุงูุทูุฆู ุงุณุช.

---

#### **ุฎูุงุตู ู ฺฏุงู ุจุนุฏ**

- ูุง ุงฺฉููู loss ูุฑุจูุท ุจู ุฏู ููููู ฺฉูฺฺฉ ูุชู ุฑุง ูุญุงุณุจู ฺฉุฑุฏู.
- ุฏุฑ ุงุฏุงููุ ููู ูุญุงุณุจู loss ุฑุง ุจุฑุง ฺฉู ูุฌููุนูโูุง ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ ุจูโฺฉุงุฑ ุฎูุงูู ุจุฑุฏ.

---

### 5.1.3 ูุญุงุณุจู ุฎุทุง (loss) ูุฌููุนูโูุง ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ

ุงุจุชุฏุง ุจุงุฏ ูุฌููุนู ุฏุงุฏูโูุง ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ ุฑุง ุขูุงุฏู ฺฉูู ฺฉู ุจุฑุง ุขููุฒุด ูุฏู ุฒุจุงู ุจุฒุฑฺฏ (LLM) ุงุณุชูุงุฏู ุฎูุงูู ฺฉุฑุฏ. ุณูพุณุ ููุงูุทูุฑ ฺฉู ุฏุฑ ุดฺฉู 5.8 ูุดุงู ุฏุงุฏู ุดุฏูุ ูุฒุงู ุขูุชุฑููพ ูุชูุงุทุน (cross entropy) ุจุฑุง ูุฑ ุฏู ูุฌููุนู ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ ุฑุง ูุญุงุณุจู ูโฺฉูู ฺฉู ุฌุฒู ุจุฎุดโูุง ููู ูุฑุงูุฏ ุขููุฒุด ูุฏู ุงุณุช.

---

#### ุงูุชุฎุงุจ ูุฌููุนู ุฏุงุฏู ู ุนูุช ุงุณุชูุงุฏู

ุจุฑุง ูุญุงุณุจู loss ุฑู ูุฌููุนูโูุง ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌุ ุงุฒ ฺฉ ูุฌููุนู ุฏุงุฏู ูุชู ุจุณุงุฑ ฺฉูฺฺฉ ุงุณุชูุงุฏู ูโฺฉูู: ุฏุงุณุชุงู ฺฉูุชุงู ยซThe Verdictยป ุงุซุฑ Edith Wharton ฺฉู ูุจูุงู ุฏุฑ ูุตู ฒ ุจุง ุขู ฺฉุงุฑ ฺฉุฑุฏู.
ุงูุชุฎุงุจ ูุชู ุงุฒ **ุฏุงููู ุนููู (public domain)** ุจุงุนุซ ูโุดูุฏ ูฺฏุฑุงูโูุง ุญููู ุงุณุชูุงุฏู ุจุฑุทุฑู ุดูุฏ.
ููฺูู ุจู ุฏูู ฺฉูฺฺฉ ุจูุฏู ุงู ูุฌููุนู ุฏุงุฏูุ ุงุฌุฑุง ูุซุงูโูุง ุญุช ุฑู ููพโุชุงูพโูุง ูุนููู ู ุจุฏูู ูุงุฒ ุจู GPU ูพุดุฑูุชูุ ุฏุฑ ุนุฑุถ ฺูุฏ ุฏููู ููฺฉู ุงุณุชุ ฺฉู ุจุฑุง ุงูุฏุงู ุขููุฒุด ุจุณุงุฑ ููุงุณุจ ุงุณุช.

> **ูฺฉุชู:** ุฎูุงููุฏฺฏุงู ุนูุงููโููุฏ ูโุชูุงููุฏ ุงุฒ ฺฉุฏูุง ุชฺฉูู ฺฉุชุงุจ ุจุฑุง ุขูุงุฏูโุณุงุฒ ูุฌููุนู ุฏุงุฏูโูุง ุจุฒุฑฺฏโุชุฑ (ูุซูุงู ุดุงูู ุจุด ุงุฒ ถฐ ูุฒุงุฑ ฺฉุชุงุจ ุงุฒ ูพุฑูฺู ฺฏูุชูุจุฑฺฏ) ุงุณุชูุงุฏู ฺฉููุฏ ู LLM ุฑุง ุฑู ุขููุง ุขููุฒุด ุฏููุฏ (ุฌุฒุฆุงุช ุฏุฑ ูพูุณุช D).

---

#### ูุฒูู ุขููุฒุด ูุฏูโูุง LLM ุฏุฑ ููุงุณ ุจุฒุฑฺฏ

ุจุฑุง ุฏุฑฺฉ ุจุฒุฑฺฏ ูพุฑูฺูุ ูุฏู Llama 2 ุจุง 7 ููุงุฑุฏ ูพุงุฑุงูุชุฑ ุฑุง ุฏุฑ ูุธุฑ ุจฺฏุฑุฏ ฺฉู ูุงุฒููุฏ ฑธด,ณฒฐ ุณุงุนุช GPU ุฑู ฺฉุงุฑุชโูุง A100 ฺฏุฑุงูโููุช ุจูุฏู ุงุณุช ู ุฏุฑ ูุฌููุน ฒ ุชุฑููู ุชูฺฉู ูพุฑุฏุงุฒุด ฺฉุฑุฏู ุงุณุช.
ุจุง ุชูุฌู ุจู ููุช ุญุฏูุฏ ณฐ ุฏูุงุฑ ุฏุฑ ุณุงุนุช ุจุฑุง ธ ฺฉุงุฑุช A100 ุฏุฑ ุณุฑูุณ ุงุจุฑ AWSุ ูุฒูู ุชูุฑุจ ุขููุฒุด ฺูู ูุฏู ุญุฏูุฏ **ถนฐ,ฐฐฐ ุฏูุงุฑ** ุงุณุช.

---

#### ุจุงุฑฺฏุฐุงุฑ ูุฌููุนู ุฏุงุฏู ยซThe Verdictยป

```python
file_path = "the-verdict.txt"
with open(file_path, "r", encoding="utf-8") as file:
    text_data = file.read()
```

---

#### ุดูุงุฑุด ุชุนุฏุงุฏ ฺฉุงุฑุงฺฉุชุฑูุง ู ุชูฺฉูโูุง

```python
total_characters = len(text_data)
total_tokens = len(tokenizer.encode(text_data))
print("Characters:", total_characters)
print("Tokens:", total_tokens)
```

ุฎุฑูุฌ:

```
Characters: 20479
Tokens: 5145
```

---

ุจุง ตฑดต ุชูฺฉูุ ุงู ูุฌููุนู ุฏุงุฏู ููฺฉู ุงุณุช ุจุฑุง ุขููุฒุด ฺฉุงูู ฺฉ LLM ุจุฒุฑฺฏ ุจุณุงุฑ ฺฉูฺฺฉ ุจู ูุธุฑ ุจุฑุณุฏุ ุงูุง ูุฏู ุขููุฒุด ุงุณุช ุชุง ฺฉุฏ ุฏุฑ ฺูุฏ ุฏููู ุงุฌุฑุง ุดูุฏ ูู ฺูุฏ ููุชู.
ููฺูู ุฏุฑ ุงุฏุงูู ุงุฒ ูุฒูโูุง ูพุดโุขููุฒุดโุฏุฏู (pretrained) ูุฏู OpenAI ุฏุฑ ฺฉุฏ GPTModel ุงุณุชูุงุฏู ุฎูุงูู ฺฉุฑุฏ.

---

#### ุชูุณู ุฏุงุฏู ุจู ูุฌููุนู ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ

ูุฌููุนู ุฏุงุฏู ุฑุง ุจู ุฏู ุจุฎุด ุชูุณู ูโฺฉูู: นฐูช ุจุฑุง ุขููุฒุด ู ฑฐูช ุจุฑุง ุงุนุชุจุงุฑุณูุฌ.

```python
train_ratio = 0.90
split_idx = int(train_ratio * len(text_data))
train_data = text_data[:split_idx]
val_data = text_data[split_idx:]
```

---

#### ุงุฌุงุฏ DataLoader ุจุฑุง ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ

ุงุฒ ุชุงุจุน `create_dataloader_v1` ฺฉู ุฏุฑ ูุตู ฒ ุชุนุฑู ุดุฏู ุงุณุชูุงุฏู ูโฺฉูู:

```python
from chapter02 import create_dataloader_v1

torch.manual_seed(123)

train_loader = create_dataloader_v1(
    train_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],  # ูุนูููุงู 256
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=True,
    shuffle=True,
    num_workers=0
)

val_loader = create_dataloader_v1(
    val_data,
    batch_size=2,
    max_length=GPT_CONFIG_124M["context_length"],
    stride=GPT_CONFIG_124M["context_length"],
    drop_last=False,
    shuffle=False,
    num_workers=0
)
```

---

#### ุจุฑุฑุณ ุงุจุนุงุฏ ุฏุงุฏูโูุง

ุจุฑุง ุงุทููุงู ุงุฒ ุฏุฑุณุช ุณุงุฎุช DataLoaderูุงุ ูโุชูุงูู ุงุจุนุงุฏ ุฏุณุชูโูุง ุฏุงุฏู ุฑุง ฺุงูพ ฺฉูู:

```python
print("Train loader:")
for x, y in train_loader:
    print(x.shape, y.shape)

print("\nValidation loader:")
for x, y in val_loader:
    print(x.shape, y.shape)
```

ุฎุฑูุฌ ููููู:

```
Train loader:
torch.Size([2, 256]) torch.Size([2, 256])
... (ุชฺฉุฑุงุฑ ุจุฑุง น ุฏุณุชู)
Validation loader:
torch.Size([2, 256]) torch.Size([2, 256])
```

> ุฏุฑ ูุฑ ุฏุณุชู (batch)ุ ุฏู ููููู ุจุง ุทูู ฒตถ ุชูฺฉู ุฏุงุฑู. ุฏุณุชูโูุง ุงุนุชุจุงุฑุณูุฌ ูุฒ ูุดุงุจู ูุณุชูุฏ ุงูุง ููุท ฺฉ ุฏุณุชู ุฏุงุฑู ฺูู ุฏุงุฏู ฺฉูุชุฑ ุงุณุช.

---

#### ูุญุงุณุจู loss ุจุฑุง ฺฉ ุฏุณุชู ุฏุงุฏู

```python
def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch = input_batch.to(device)
    target_batch = target_batch.to(device)
    logits = model(input_batch)
    loss = torch.nn.functional.cross_entropy(
        logits.flatten(0, 1), target_batch.flatten()
    )
    return loss
```

---

#### ูุญุงุณุจู ูุชูุณุท loss ุจุฑุง ููู ุฏุณุชูโูุง

```python
def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if len(data_loader) == 0:
        return float("nan")
    elif num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))

    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches
```

- ุงู ุชุงุจุน ุจู ุทูุฑ ูพุดโูุฑุถ ุฑู ููู ุฏุณุชูโูุง ุฏุฑ DataLoader ูพูุงุด ฺฉุฑุฏูุ loss ูุฑ ุฏุณุชู ุฑุง ุฌูุน ูโฺฉูุฏ ู ูุงูฺฏู ูโฺฏุฑุฏ.
- ูโุชูุงู ุชุนุฏุงุฏ ุฏุณุชูโูุง ุฑุง ุจุง `num_batches` ูุญุฏูุฏ ฺฉุฑุฏ ุชุง ุงุฑุฒุงุจ ุณุฑุนโุชุฑ ุงูุฌุงู ุดูุฏ.

---

#### ูุญุงุณุจู ู ููุงุด loss ูุฌููุนูโูุง ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

with torch.no_grad():
    train_loss = calc_loss_loader(train_loader, model, device)
    val_loss = calc_loss_loader(val_loader, model, device)

print("Training loss:", train_loss)
print("Validation loss:", val_loss)
```

ููููู ุฎุฑูุฌ:

```
Training loss: 10.98758347829183
Validation loss: 10.98110580444336
```

---

#### ุชูุณุฑ ูุชุงุฌ

- ููุฏุงุฑ loss ูููุฒ ูุณุจุชุงู ุจุงูุง ุงุณุชุ ฺูู ูุฏู ูููุฒ ุขููุฒุด ูุฏุฏู ู ุชุตุงุฏู ุนูู ูโฺฉูุฏ.
- ูุฑฺู ูุฏู ุจูุชุฑ ุดูุฏ ู ุชูฺฉูโูุง ุฑุง ุฏููโุชุฑ ูพุดโุจู ฺฉูุฏุ ููุฏุงุฑ loss ุจู ุตูุฑ ูุฒุฏฺฉ ุฎูุงูุฏ ุดุฏ.

---

#### ุฌูุนโุจูุฏ

ูุง ูุฑุงุญู ุฒุฑ ุฑุง ุท ฺฉุฑุฏู:

1. ุจุงุฑฺฏุฐุงุฑ ุฏุงุฏูโูุง ูุชู
2. ุชูุณู ุฏุงุฏู ุจู ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ
3. ุงุฌุงุฏ DataLoader ุจุง ุฏุณุชูโูุง ุชูฺฉูุฒู ุดุฏู
4. ุชุนุฑู ุชูุงุจุน ูุญุงุณุจู loss ุจุฑุง ูุฑ ุฏุณุชู ู ฺฉู ุฏุงุฏู
5. ุงุฑุฒุงุจ ุงููู loss ุฑู ุฏุงุฏูโูุง ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ

ุฏุฑ ูุฑุงุญู ุจุนุฏุ ูุฏู ุฑุง ุขููุฒุด ุฏุงุฏู ู ุณุน ูโฺฉูู ุงู loss ุฑุง ฺฉุงูุด ุฏูู ุชุง ูุฏู ุฏุฑ ุชููุฏ ูุชู ุจูุชุฑ ุนูู ฺฉูุฏ.

---

## ุจุฎุด ต.ฒ ุขููุฒุด ฺฉ ูุฏู ุฒุจุงู ุจุฒุฑฺฏ (LLM)

ุงฺฉููู ุฒูุงู ุขู ุฑุณุฏู ุงุณุช ฺฉู ฺฉุฏ ูพุดโุขููุฒุด ูุฏู ุฒุจุงู ุจุฒุฑฺฏ ุฎูุฏุ ุนู GPTModel ุฑุง ูพุงุฏูโุณุงุฒ ฺฉูู. ุฏุฑ ุงู ุจุฎุดุ ุฑู ฺฉ ุญููู ุขููุฒุด ุณุงุฏู ุชูุฑฺฉุฒ ูโฺฉูู ุชุง ฺฉุฏ ูุฎุชุตุฑุ ูุงุจู ููู ู ุฎูุงูุง ุจุงุดุฏ.

> **ูฺฉุชู:** ุนูุงููโููุฏุงู ูโุชูุงููุฏ ุชฺฉูฺฉโูุง ูพุดุฑูุชูโุชุฑ ูุงููุฏ ฺฏุฑูโฺฉุฑุฏู ูุฑุฎ ุงุฏฺฏุฑ (learning rate warmup)ุ ฺฉุงูุด ูุฑุฎ ุงุฏฺฏุฑ ุจู ุตูุฑุช ฺฉุณููุณ (cosine annealing) ู ุจุฑุด ฺฏุฑุงุฏุงู (gradient clipping) ุฑุง ุฏุฑ ุถููู D ูุทุงูุนู ฺฉููุฏ.

---

**ุดฺฉู ต.ฑฑ**:
ฺฉ ุญููู ุขููุฒุด ูุนููู ุฏุฑ PyTorch ุจุฑุง ุดุจฺฉูโูุง ุนุตุจ ุนูู ุดุงูู ูุฑุงุญู ูุชุนุฏุฏ ุงุณุช ฺฉู ุฏุฑ ูุฑ ุฏูุฑ (epoch) ุฑู ุฏุณุชูโูุง ุฏุงุฏู (batch) ุขููุฒุด ุชฺฉุฑุงุฑ ูโุดูุฏ. ุฏุฑ ูุฑ ุชฺฉุฑุงุฑุ ููุฏุงุฑ ุฎุทุง (loss) ูุฑ ุฏุณุชู ูุญุงุณุจู ุดุฏู ู ุงุฒ ุขู ุจุฑุง ุจูโุฑูุฒุฑุณุงู ูุฒูโูุง ูุฏู ุงุณุชูุงุฏู ูโุดูุฏ ุชุง ุฎุทุง ูุฌููุนู ุขููุฒุด ุจู ุญุฏุงูู ุจุฑุณุฏ.

---

**ุฑููุฏ ฺฉู ุขููุฒุด ุฏุฑ PyTorch** (ุดฺฉู ต.ฑฑ) ุดุงูู ธ ูุฑุญูู ุงุณุช:

- ุชฺฉุฑุงุฑ ุฑู ูุฑ epoch
- ูพุฑุฏุงุฒุด ุฏุณุชูโูุง ุฏุงุฏู
- ุตูุฑ ฺฉุฑุฏู ฺฏุฑุงุฏุงูโูุง
- ูุญุงุณุจู ุฎุทุง ู ฺฏุฑุงุฏุงูโูุง
- ุจูโุฑูุฒุฑุณุงู ูุฒูโูุง
- ููุงุด ูุฒุงู ุฎุทุง ู ููููู ูุชู ุชููุฏ ุดุฏู

> **ูฺฉุชู:** ุงฺฏุฑ ุชุงุฒูโฺฉุงุฑ ูุณุชุฏ ู ุจุฑุฎ ูุฑุงุญู ุจุฑุงุชุงู ูุงููููู ุงุณุชุ ูพุดููุงุฏ ูโฺฉูู ุจุฎุดโูุง A.5 ุชุง A.8 ุฏุฑ ุถููู A ุฑุง ูุทุงูุนู ฺฉูุฏ.

---

#### ูพุงุฏูโุณุงุฒ ุญููู ุขููุฒุด ุณุงุฏู

ุชุงุจุน `train_model_simple` ุงู ุฑููุฏ ุฑุง ุจู ุตูุฑุช ฺฉุฏ ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ:

```python
def train_model_simple(model, train_loader, val_loader,
                       optimizer, device, num_epochs,
                       eval_freq, eval_iter, start_context, tokenizer):
    train_losses, val_losses, track_tokens_seen = [], [], []    #1
    tokens_seen, global_step = 0, -1

    for epoch in range(num_epochs):    #2
        model.train()
        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()   #3
            loss = calc_loss_batch(
                input_batch, target_batch, model, device
            )
            loss.backward()                     #4
            optimizer.step()                    #5
            tokens_seen += input_batch.numel()
            global_step += 1

            if global_step % eval_freq == 0:    #6
                train_loss, val_loss = evaluate_model(
                    model, train_loader, val_loader, device, eval_iter)
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                track_tokens_seen.append(tokens_seen)
                print(f"Ep {epoch+1} (Step {global_step:06d}): "
                      f"Train loss {train_loss:.3f}, "
                      f"Val loss {val_loss:.3f}"
                )

        generate_and_print_sample(                      #7
            model, tokenizer, device, start_context
        )
    return train_losses, val_losses, track_tokens_seen
```

- #1 ููุฏุงุฑุฏู ุงููู ูุณุชโูุง ุจุฑุง ุซุจุช ุฎุทุง ู ุชุนุฏุงุฏ ุชูฺฉูโูุง ุฏุฏู ุดุฏู
- #2 ุดุฑูุน ุญููู ุงุตู ุขููุฒุด
- #3 ุตูุฑ ฺฉุฑุฏู ฺฏุฑุงุฏุงูโูุง ุงุฒ ุชฺฉุฑุงุฑ ูุจู
- #4 ูุญุงุณุจู ฺฏุฑุงุฏุงูโูุง
- #5 ุจูโุฑูุฒุฑุณุงู ูุฒูโูุง ูุฏู
- #6 ุงุฑุฒุงุจ ูุฏู ุจู ุตูุฑุช ุฏูุฑูโุง
- #7 ุชููุฏ ู ฺุงูพ ููููู ูุชู ูพุณ ุงุฒ ูุฑ epoch

> ุชูุฌู: ุฏู ุชุงุจุน `evaluate_model` ู `generate_and_print_sample` ฺฉู ุฏุฑ ุงู ุชุงุจุน ุงุณุชูุงุฏู ุดุฏูโุงูุฏ ูููุฒ ุชุนุฑู ูุดุฏูโุงูุฏ.

---

#### ุชุนุฑู ุชุงุจุน ุงุฑุฒุงุจ ูุฏู

ุชุงุจุน `evaluate_model` ูุทุงุจู ุจุง ูุฑุญูู ท ุดฺฉู ต.ฑฑุ ุฎุทุง ุฑู ุฏุงุฏูโูุง ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ ุฑุง ูุญุงุณุจู ูโฺฉูุฏ ู ููฺฏุงู ูุญุงุณุจูุ ูุฏู ุฑุง ุฏุฑ ุญุงูุช ุงุฑุฒุงุจ ูุฑุงุฑ ูโุฏูุฏ ุชุง ฺฏุฑุงุฏุงูโฺฏุฑ ู dropout ุบุฑูุนุงู ุดูุฏ:

```python
def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()  #1
    with torch.no_grad():                              #2
        train_loss = calc_loss_loader(
            train_loader, model, device, num_batches=eval_iter
        )
        val_loss = calc_loss_loader(
            val_loader, model, device, num_batches=eval_iter
        )
    model.train()
    return train_loss, val_loss
```

- #1 ุญุงูุช ุงุฑุฒุงุจ ูุฏู ุจุฑุง ุบุฑูุนุงูโฺฉุฑุฏู dropout
- #2 ุบุฑ ูุนุงูโฺฉุฑุฏู ูพฺฏุฑ ฺฏุฑุงุฏุงู ุจุฑุง ฺฉุงูุด ูุฒูู ูุญุงุณุจุงุช

---

#### ุชุนุฑู ุชุงุจุน ุชููุฏ ู ููุงุด ููููู ูุชู

ุงู ุชุงุจุน ุจุฑุง ุจุฑุฑุณ ฺฉู ุจูุจูุฏ ูุฏู ุฏุฑ ุทูู ุขููุฒุด ุงุณุชูุงุฏู ูโุดูุฏ. ูุชู ุดุฑูุน (start_context) ุฑุง ุจู ุชูฺฉู ุชุจุฏู ฺฉุฑุฏูุ ุณูพุณ ุจุง ุงุณุชูุงุฏู ุงุฒ ูุฏูุ ูุชู ุฌุฏุฏ ุชููุฏ ูโฺฉูุฏ ู ฺุงูพ ูโฺฉูุฏ:

```python
def generate_and_print_sample(model, tokenizer, device, start_context):
    model.eval()
    context_size = model.pos_emb.weight.shape[0]
    encoded = text_to_token_ids(start_context, tokenizer).to(device)
    with torch.no_grad():
        token_ids = generate_text_simple(
            model=model, idx=encoded,
            max_new_tokens=50, context_size=context_size
        )
    decoded_text = token_ids_to_text(token_ids, tokenizer)
    print(decoded_text.replace("\n", " "))      #1
    model.train()
```

- #1 ฺุงูพ ูุชู ุชููุฏ ุดุฏู ุฏุฑ ฺฉ ุฎุท ุจุฑุง ูุถูุญ ุจูุชุฑ

---

#### ุงุณุชูุงุฏู ุงุฒ ุจูููโุณุงุฒ AdamW

ุจูููโุณุงุฒ AdamW ฺฉ ูุณุฎู ุจูุจูุฏ ุงูุชู ุงุฒ Adam ุงุณุช ฺฉู ุฑูุด ูุฒูโฺฉุงูุด (weight decay) ุฑุง ุจูุจูุฏ ูโุจุฎุดุฏ. ุงู ฺฉุงุฑ ุจุงุนุซ ฺฉุงูุด ุจุดโุจุฑุงุฒุด (overfitting) ู ุจูุจูุฏ ุชุนููโูพุฐุฑ ูุฏู ูโุดูุฏ ู ุจู ููู ุฏูู ุฏุฑ ุขููุฒุด ูุฏูโูุง ุฒุจุงู ุจุฒุฑฺฏ ุจู ุทูุฑ ฺฏุณุชุฑุฏู ุงุณุชูุงุฏู ูโุดูุฏ.

---

#### ุงุฌุฑุง ุขููุฒุด ูุฏู

ูููููโุง ุงุฒ ุงุฌุฑุง ุขููุฒุด ฑฐ epoch ุจุง ุจูููโุณุงุฒ AdamW ู ุชุงุจุน `train_model_simple` ุจู ุดฺฉู ุฒุฑ ุงุณุช:

```python
torch.manual_seed(123)
model = GPTModel(GPT_CONFIG_124M)
model.to(device)
optimizer = torch.optim.AdamW(
     model.parameters(),           #1
    lr=0.0004, weight_decay=0.1
)
num_epochs = 10
train_losses, val_losses, tokens_seen = train_model_simple(
    model, train_loader, val_loader, optimizer, device,
    num_epochs=num_epochs, eval_freq=5, eval_iter=5,
    start_context="Every effort moves you", tokenizer=tokenizer
)
```

- #1 ูุชุฏ `.parameters()` ุชูุงู ูุฒูโูุง ูุงุจู ุขููุฒุด ูุฏู ุฑุง ุจุฑูโฺฏุฑุฏุงูุฏ.

---

#### ูุชุงุฌ ุงุฌุฑุง ุขููุฒุด

ุฎุฑูุฌ ููููู ุงุฌุฑุง ุจู ุดุฑุญ ุฒุฑ ุงุณุช:

```
Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933
Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339
Every effort moves you,,,,,,,,,,,,.
Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048
Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616
Every effort moves you, and, and, and, and, and, and, and, and, and, and,
 and, and, and, and, and, and, and, and, and, and, and, and,, and, and,
[...]                                                   #1
Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393
Every effort moves you?"  "Yes--quite insensible to the irony. She wanted
him vindicated--and by me!"  He laughed again, and threw back the
window-curtains, I had the donkey. "There were days when I
Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452
Every effort moves you know," was one of the axioms he laid down across the
Sevres and silver of an exquisitely appointed luncheon-table, when, on a
later day, I had again run over from Monte Carlo; and Mrs. Gis
```

- #1 ุจุฎุดโูุง ุงุฒ ุฎุฑูุฌ ุจุฑุง ุตุฑููโุฌู ุฏุฑ ูุถุง ุญุฐู ุดุฏูโุงูุฏ.

ููุงูุทูุฑ ฺฉู ูุดุงูุฏู ูโฺฉููุ ุฎุทุง ุขููุฒุด ุงุฒ น.ทธฑ ุจู ฐ.ณนฑ ฺฉุงูุด ฺุดูฺฏุฑ ุฏุงุฑุฏ ู ูุฏู ุชูุงูุง ุชููุฏ ุฌููุงุช ุตุญุญ ุฏุณุชูุฑ ุฑุง ูพุฏุง ฺฉุฑุฏู ุงุณุช.
ุฎุทุง ุงุนุชุจุงุฑุณูุฌ ูุฒ ฺฉุงูุด ูโุงุจุฏ ุงูุง ูุฑฺฏุฒ ุจู ุงูุฏุงุฒู ุฎุทุง ุขููุฒุด ูพุงู ููโุขุฏ ฺฉู ูุดุงูโุฏููุฏู ุงุญุชูุงู ุจุดโุจุฑุงุฒุด ูุฏู ุงุณุช.

---

#### ุฑุณู ูููุฏุงุฑ ุฎุทุงูุง ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ

ุจุฑุง ูุดุงูุฏู ุจูุชุฑ ุฑููุฏ ุขููุฒุดุ ูโุชูุงูู ุฎุทุงูุง ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ ุฑุง ุฏุฑ ฺฉ ูููุฏุงุฑ ุฑุณู ฺฉูู:

```python
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
    fig, ax1 = plt.subplots(figsize=(5, 3))
    ax1.plot(epochs_seen, train_losses, label="Training loss")
    ax1.plot(
        epochs_seen, val_losses, linestyle="-.", label="Validation loss"
    )
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")
    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))
    ax2 = ax1.twiny()                   #1
    ax2.plot(tokens_seen, train_losses, alpha=0)     #2
    ax2.set_xlabel("Tokens seen")
    fig.tight_layout()
    plt.show()

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)
```

- #1 ุงุฌุงุฏ ูุญูุฑ ุฏูู ุจุฑุง ูุญูุฑ x ฺฉู ุจุง ูุญูุฑ y ูุดุชุฑฺฉ ุงุณุช
- #2 ุฑุณู ูููุฏุงุฑ ูุงูุฑุฆ ุจุฑุง ููุงููฺฏโุณุงุฒ ุชฺฉโูุง

---

#### ุชุญูู ูููุฏุงุฑ

ููุงูุทูุฑ ฺฉู ุฏุฑ ุดฺฉู ต.ฑฒ ูุดุงูุฏู ูโุดูุฏุ ุฎุทุงูุง ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ ุฏุฑ ุงุจุชุฏุง ูุฑ ุฏู ฺฉุงูุด ูโุงุจูุฏุ ุงูุง ูพุณ ุงุฒ epoch ุฏูู ุงุฒ ูู ูุงุตูู ูโฺฏุฑูุฏ. ุงู ููุถูุน ููุฑุงู ุจุง ุจุฒุฑฺฏโุชุฑ ุจูุฏู ุฎุทุง ุงุนุชุจุงุฑุณูุฌ ูุณุจุช ุจู ุฎุทุง ุขููุฒุดุ ูุดุงูโุฏููุฏู ุจุดโุจุฑุงุฒุด ูุฏู ุงุณุช. ูุชู ุชููุฏ ุดุฏู ูุฒ ฺฏูุงู ุจุฑ ุญูุธ ุฏูู ุฏุงุฏูโูุง ุขููุฒุด ุชูุณุท ูุฏู ุงุณุช.

---

#### ุขูุงุฏูโุณุงุฒ ูุฏู ุจุฑุง ุงุณุชูุชุงุฌ

ูพุณ ุงุฒ ุงุชูุงู ุขููุฒุดุ ูุฏู ุฑุง ุงุฒ GPU ุจู CPU ููุชูู ฺฉุฑุฏู ู ุฏุฑ ุญุงูุช ุงุฑุฒุงุจ ูุฑุงุฑ ูโุฏูู ุชุง ุจุฎุดโูุง ุชุตุงุฏู ูุงููุฏ dropout ุบุฑูุนุงู ุดููุฏ:

```python
model.to("cpu")
model.eval()
```

---

#### ุชููุฏ ูุชู ููููู ุจุง ูุฏู ุขููุฒุดโุฏุฏู

ุงุฒ ุชุงุจุน `generate_text_simple` ุจุฑุง ุชููุฏ ูุชู ุจู ุตูุฑุช ูุฑุญูู ุจู ูุฑุญูู ุงุณุชูุงุฏู ูโฺฉูู:

```python
tokenizer = tiktoken.get_encoding("gpt2")
token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids("Every effort moves you", tokenizer),
    max_new_tokens=25,
    context_size=GPT_CONFIG_124M["context_length"]
)
print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```

ุฎุฑูุฌ ููููู:

```
Output text:
Every effort moves you know," was one of the axioms he laid down across the
Sevres and silver of an exquisitely appointed lun
```

ููุงูุทูุฑ ฺฉู ูุจูุงู ุชูุถุญ ุฏุงุฏู ุดุฏุ ุฏุฑ ูุฑ ูุฑุญูู ุชููุฏุ ุชูฺฉู ุจุง ุจุดุชุฑู ุงุญุชูุงู ุงูุชุฎุงุจ ูโุดูุฏุ ุจูุงุจุฑุงู ูุฑ ุจุงุฑ ุงุฌุฑุง ุชุงุจุน ุฎุฑูุฌ ฺฉุณุงู ุชููุฏ ูโฺฉูุฏ.

#### ต.ณ.ฑ ููุงุณโุจูุฏ ุฏูุง (Temperature Scaling)

ุงฺฉููู ุจู ููุงุณโุจูุฏ ุฏูุง ูโูพุฑุฏุงุฒูุ ุชฺฉูฺฉ ฺฉู ูุฑุงูุฏ ุชููุฏ ุชูฺฉู ุจุนุฏ ุฑุง ุจู ุตูุฑุช ุงุญุชูุงูุงุช ุฏุฑูโุขูุฑุฏ. ูพุดโุชุฑุ ุฏุฑ ุชุงุจุน `generate_text_simple`ุ ููุดู ุชูฺฉู ฺฉู ุจุงูุงุชุฑู ุงุญุชูุงู ุฑุง ุฏุงุดุช ุจู ุนููุงู ุชูฺฉู ุจุนุฏ ุจุง ุงุณุชูุงุฏู ุงุฒ ุชุงุจุน `torch.argmax` ุงูุชุฎุงุจ ูโฺฉุฑุฏู ฺฉู ุจู ุขู **ุฑูุฒฺฏุดุง ุทูุงุนุงูู (greedy decoding)** ฺฏูุชู ูโุดูุฏ. ุจุฑุง ุชููุฏ ูุชู ุจุง ุชููุน ุจุดุชุฑุ ูโุชูุงูู ุจู ุฌุง `argmax` ุงุฒ ุชุงุจุน ุงุณุชูุงุฏู ฺฉูู ฺฉู ูููููโฺฏุฑ ุฑุง ุจุฑ ุงุณุงุณ ุชูุฒุน ุงุญุชูุงูุงุช ุงูุฌุงู ูโุฏูุฏ (ุงูุฌุง ููุธูุฑ ุชูุฒุน ุงุญุชูุงูุงุช ุงุณุช ฺฉู ูุฏู ุฒุจุงู ุจุฒุฑฺฏ ุจุฑุง ูุฑ ฺฉููู ุฏุฑ ูุฑ ูุฑุญูู ุชููุฏ ุชูฺฉู ุงุฑุงุฆู ูโุฏูุฏ).

ุจุฑุง ุชูุถุญ ุงู ูููููโฺฏุฑ ุงุญุชูุงูุงุชุ ฺฉ ูุซุงู ุณุงุฏู ุจุง ฺฉ ุฏฺฉุดูุฑ ูุงฺฺฏุงู ฺฉูฺฺฉ ูโุขูุฑู:

```python
vocab = {
    "closer": 0,
    "every": 1,
    "effort": 2,
    "forward": 3,
    "inches": 4,
    "moves": 5,
    "pizza": 6,
    "toward": 7,
    "you": 8,
}
inverse_vocab = {v: k for k, v in vocab.items()}
```

ูุฑุถ ฺฉูุฏ ูุฏู ุฒุจุงู ุฌููู ุดุฑูุน ยซevery effort moves youยป ุฑุง ฺฏุฑูุชู ู ูุงุฌุชโูุง ุชูฺฉู ุจุนุฏ ุฑุง ุจู ุดฺฉู ุฒุฑ ุชููุฏ ฺฉุฑุฏู ุงุณุช:

```python
next_token_logits = torch.tensor(
    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]
)
```

ููุงูโุทูุฑ ฺฉู ุฏุฑ ูุตู ด ุชูุถุญ ุฏุงุฏู ุดุฏุ ุฏุฑ ุชุงุจุน `generate_text_simple` ุงู ูุงุฌุชโูุง ุฑุง ุจู ุงุญุชูุงู ุจุง ุงุณุชูุงุฏู ุงุฒ ุชุงุจุน softmax ุชุจุฏู ูโฺฉูู ู ุณูพุณ ุจุง `argmax`ุ ุดูุงุณู ุชูฺฉู ุจุง ุจุดุชุฑู ุงุญุชูุงู ุฑุง ุงูุชุฎุงุจ ู ุจุง `inverse_vocab` ุจู ูุชู ุชุจุฏู ูโฺฉูู:

```python
probas = torch.softmax(next_token_logits, dim=0)
next_token_id = torch.argmax(probas).item()
print(inverse_vocab[next_token_id])
```

ุงุฒ ุขูุฌุง ฺฉู ุจุฒุฑฺฏโุชุฑู ููุฏุงุฑ ูุงุฌุช (ู ุจู ุชุจุน ุขู ุจุดุชุฑู ุงุญุชูุงู softmax) ุฏุฑ ุฌุงฺฏุงู ฺูุงุฑู (ุงูุฏุณ ณ ุฏุฑ ูพุงุชูู) ูุฑุงุฑ ุฏุงุฑุฏุ ุชูฺฉู ุชููุฏ ุดุฏู "forward" ุงุณุช.

---

ุจุฑุง ูููููโฺฏุฑ ุงุญุชูุงูุงุชุ ูโุชูุงูู `argmax` ุฑุง ุจุง ุชุงุจุน `multinomial` ุฏุฑ PyTorch ุฌุงฺฏุฒู ฺฉูู:

```python
torch.manual_seed(123)
next_token_id = torch.multinomial(probas, num_samples=1).item()
print(inverse_vocab[next_token_id])
```

ุฎุฑูุฌ ููุงู "forward" ุงุณุช. ุฏููุด ฺุณุชุ ุชุงุจุน `multinomial` ุชูฺฉู ุจุนุฏ ุฑุง ูุชูุงุณุจ ุจุง ุงุญุชูุงู ุขู ูููููโฺฏุฑ ูโฺฉูุฏุ ุนู "forward" ูููุฒ ูพุฑ ุงุญุชูุงูโุชุฑู ุชูฺฉู ุงุณุช ู ุงุบูุจ ุงูุชุฎุงุจ ูโุดูุฏุ ุงูุง ููุดู ูู. ุจุฑุง ููุงุด ุจูุชุฑุ ุชุงุจุน ูโููุณู ฺฉู ุงู ูููููโฺฏุฑ ุฑุง ฑฐฐฐ ุจุงุฑ ุชฺฉุฑุงุฑ ฺฉูุฏ:

```python
def print_sampled_tokens(probas):
    torch.manual_seed(123)
    sample = [torch.multinomial(probas, num_samples=1).item()
             for i in range(1_000)]
    sampled_ids = torch.bincount(torch.tensor(sample))
    for i, freq in enumerate(sampled_ids):
        print(f"{freq} x {inverse_vocab[i]}")

print_sampled_tokens(probas)
```

ุฎุฑูุฌ ูููููโฺฏุฑ ุจู ุดฺฉู ุฒุฑ ุงุณุช:

```
73 x closer
0 x every
0 x effort
582 x forward
2 x inches
0 x moves
0 x pizza
343 x toward
```

ููุงูโุทูุฑ ฺฉู ูโุจููุ "forward" ุงุบูุจ ุงูุชุฎุงุจ ุดุฏู ุงูุง ุชูฺฉูโูุง ูุงููุฏ "closer"ุ "inches" ู "toward" ูุฒ ฺฏุงู ุงูุชุฎุงุจ ุดุฏูโุงูุฏ. ุงู ุนู ุงฺฏุฑ ุฏุฑ ุชุงุจุน `generate_and_print_sample` ุจู ุฌุง `argmax` ุงุฒ `multinomial` ุงุณุชูุงุฏู ฺฉููุ ูุชูโูุง ูุซู ยซevery effort moves you towardยป ุง ยซevery effort moves you inchesยป ูู ููฺฉู ุงุณุช ุชููุฏ ุดููุฏ.

---

#### ููุงุณโุจูุฏ ุฏูุง (Temperature Scaling)

ุจุง ุงุณุชูุงุฏู ุงุฒ **ููุงุณโุจูุฏ ุฏูุง**ุ ูโุชูุงูู ุชูุฒุน ุงุญุชูุงู ุชูฺฉูโูุง ุฑุง ฺฉูุชุฑู ฺฉูู. ุงู ฺฉุงุฑ ุจุง ุชูุณู ูุงุฌุชโูุง ุจุฑ ฺฉ ุนุฏุฏ ูุซุจุช ุงูุฌุงู ูโุดูุฏ:

```python
def softmax_with_temperature(logits, temperature):
    scaled_logits = logits / temperature
    return torch.softmax(scaled_logits, dim=0)
```

- ุฏูุงูุง ุจุงูุงุชุฑ ุงุฒ ฑ ุชูุฒุน ุงุญุชูุงู ุฑุง ฺฉููุงุฎุชโุชุฑ ูโฺฉููุฏ (ุงุญุชูุงูโูุง ุจู ูู ูุฒุฏฺฉโุชุฑ ูโุดููุฏ).
- ุฏูุงูุง ฺฉูุชุฑ ุงุฒ ฑ ุชูุฒุน ุฑุง ุชุฒุชุฑ ู ูุทุนโุชุฑ ูโฺฉููุฏ.

ุจุฑุง ุฏุฑฺฉ ุจูุชุฑุ ุชูุฒุน ุงุญุชูุงู ุงุตู ู ุชูุฒุนโูุง ููุงุณโุงูุชู ุจุง ุฏูุงูุง ูุฎุชูู ุฑุง ุฑุณู ูโฺฉูู:

```python
temperatures = [1, 0.1, 5]
scaled_probas = [softmax_with_temperature(next_token_logits, T)
                for T in temperatures]
x = torch.arange(len(vocab))
bar_width = 0.15
fig, ax = plt.subplots(figsize=(5, 3))
for i, T in enumerate(temperatures):
    rects = ax.bar(x + i * bar_width, scaled_probas[i],
                   bar_width, label=f'Temperature = {T}')
ax.set_ylabel('Probability')
ax.set_xticks(x)
ax.set_xticklabels(vocab.keys(), rotation=90)
ax.legend()
plt.tight_layout()
plt.show()
```

**ุชูุถุญ ุดฺฉู (ุดฺฉู ต.ฑด):**

- ุฏูุง ฑ ูุนุงุฏู ุนุฏู ุชุบุฑ ุฏุฑ ุชูุฒุน ุงุญุชูุงู ุงุณุช.
- ฺฉุงูุด ุฏูุง ุจู ฐ.ฑ ุจุงุนุซ ุชุฒ ุดุฏู ุชูุฒุน ู ุงูุฒุงุด ุงุญุชูุงู "forward" ูโุดูุฏ.
- ุงูุฒุงุด ุฏูุง ุจู ต ุชูุฒุน ุฑุง ฺฉููุงุฎุชโุชุฑ ฺฉุฑุฏู ู ุงุญุชูุงู ุณุงุฑ ุชูฺฉูโูุง ุฑุง ุจุดุชุฑ ูโฺฉูุฏ.

ุจุง ุฏูุง ฑุ ุชูฺฉู "forward" ุญุฏูุฏ ถฐูช ุงูุชุฎุงุจ ูโุดูุฏ. ุฏูุง ูพุงู ูุงููุฏ ฐ.ฑ ุชูุฑุจุงู ุฑูุชุงุฑ `argmax` ุฑุง ุดุจูโุณุงุฒ ูโฺฉูุฏ (ุงูุชุฎุงุจ ูุทุน ุชูฺฉู ูพุฑ ุงุญุชูุงู). ุฏูุง ุจุงูุง ูุซู ต ุจุงุนุซ ุงูุชุฎุงุจโูุง ูุชููุนโุชุฑุ ุงูุง ฺฏุงูุงู ูุชูโูุง ุจโูุนู ูโุดูุฏุ ูุซูุงู ูุชูโูุง ุจุง ุชูฺฉู "pizza".

---

#### ุชูุฑู ต.ฑ

ุจุง ุงุณุชูุงุฏู ุงุฒ ุชุงุจุน `print_sampled_tokens`ุ ุชูุฒุน ูููููโฺฏุฑ ุชูฺฉูโูุง ุฑุง ุจุฑุง ุฏูุงูุง ูุฎุชูู ุดฺฉู ต.ฑด ฺุงูพ ฺฉูุฏ. ุฏุฑ ูุฑ ุญุงูุชุ ฺูุฏ ุจุงุฑ ุชูฺฉู "pizza" ุงูุชุฎุงุจ ุดุฏู ุงุณุชุ ุขุง ูโุชูุงูุฏ ุฑูุด ุณุฑุนโุชุฑ ู ุฏููโุชุฑ ุจุฑุง ุชุนู ูุฑุงูุงู ุงูุชุฎุงุจ "pizza" ูพุดููุงุฏ ุฏูุฏุ

---

#### ต.ณ.ฒ ูููููโฺฏุฑ Top-k

ุชุง ุงูุฌุงุ ูููููโฺฏุฑ ุงุญุชูุงูุงุช ุจู ููุฑุงู ููุงุณโุจูุฏ ุฏูุง ุฑุง ูพุงุฏูโุณุงุฒ ฺฉุฑุฏู ุชุง ุชููุน ูุชุงุฌ ุงูุฒุงุด ุงุจุฏ. ุฏูุงูุง ุจุงูุงุชุฑ ุจุงุนุซ ุชูุฒุน ฺฉููุงุฎุชโุชุฑ ู ุฎุฑูุฌโูุง ูุชููุนโุชุฑ ูโุดููุฏ ุงูุง ฺฏุงู ููุฌุฑ ุจู ุฌููุงุช ุจโูุนู ูุงููุฏ ยซevery effort moves you pizzaยป ูโุดููุฏ.

ุจุฑุง ุจูุจูุฏ ฺฉูุชุ ูโุชูุงู ุงุฒ **ูููููโฺฏุฑ Top-k** ุงุณุชูุงุฏู ฺฉุฑุฏ. ุฏุฑ ุงู ุฑูุดุ ููุท ุงุฒ ูุงู **k** ุชูฺฉู ุจุง ุจุดุชุฑู ุงุญุชูุงู ูููููโฺฏุฑ ูโฺฉูู ู ุจูู ุชูฺฉูโูุง ุฑุง ุงุฒ ุงูุชุฎุงุจ ุญุฐู ูโฺฉูู (ุจุง ุตูุฑ ฺฉุฑุฏู ุงุญุชูุงู ุขูโูุง ุง ุฌุงฺฏุฒู ูุงุฌุชโุดุงู ุจุง ููู ุจโููุงุช).

---

**ุดุฑุญ ุชุตูุฑ (ุดฺฉู ต.ฑต):**

ุจุง ุชูุธู ( k=3 )ุ ููุท ุณู ุชูฺฉู ุจุง ุจุฒุฑฺฏโุชุฑู ูุงุฌุชโูุง ูฺฏู ุฏุงุดุชู ูโุดููุฏ ู ุจูู ูุงุฌุชโูุง ุจู (-\infty) ุชุบุฑ ูโฺฉููุฏ. ุณูพุณ ุจุง ุงุนูุงู softmax ุงุญุชูุงู ุจูู ุชูฺฉูโูุง ุตูุฑ ูโุดูุฏ.

---

ฺฉุฏ ูููููโฺฏุฑ Top-k ุจู ุงู ุตูุฑุช ุงุณุช:

```python
top_k = 3
top_logits, top_pos = torch.topk(next_token_logits, top_k)
print("Top logits:", top_logits)
print("Top positions:", top_pos)
```

ุฎุฑูุฌ ูุงุฌุชโูุง ู ุงูุฏุณโูุง ุณู ุชูฺฉู ุจุฑุชุฑ:

```
Top logits: tensor([6.7500, 6.2800, 4.5100])
Top positions: tensor([3, 7, 0])
```

ุณูพุณ ุจุง ุงุณุชูุงุฏู ุงุฒ `torch.where` ูุงุฌุชโูุง ฺฉูุชุฑ ุงุฒ ฺฉูุชุฑู ูุงุฌุช ุฏุฑ ุงู ุณู ุฑุง ุจู ููู ุจโููุงุช ุชุบุฑ ูโุฏูู:

```python
new_logits = torch.where(
    condition=next_token_logits < top_logits[-1],    #1
    input=torch.tensor(float('-inf')),               #2
    other=next_token_logits                          #3
)
print(new_logits)
#1 ุดุฑุท ุดูุงุณุง ูุงุฌุชโูุง ฺฉูุชุฑ ุงุฒ ฺฉูุชุฑู ูุงุฌุช ุฏุฑ top-k
#2 ุงุฎุชุตุงุต ููุฏุงุฑ โinf ุจู ุขูโูุง
#3 ุญูุธ ูุงุฌุช ุงุตู ุณุงุฑ ุชูฺฉูโูุง
```

ูุงุฌุชโูุง ุฌุฏุฏ:

```
tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800, -inf])
```

ุจุง ุงุนูุงู softmax ุชูุฒุน ุงุญุชูุงู ููุง:

```python
topk_probas = torch.softmax(new_logits, dim=0)
print(topk_probas)
```

ฺฉู ูุชุฌู ูโุดูุฏ:

```
tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])
```

ุฏุฑ ููุงุชุ ูโุชูุงูู ุจุง ุชุฑฺฉุจ ุฏูุง ู ูููููโฺฏุฑ ฺูุฏุฌูููโุง (multinomial) ุงุฒ ูุงู ุงู ุณู ุชูฺฉู ุงูุชุฎุงุจ ฺฉูู ู ุชูฺฉู ุจุนุฏ ุฑุง ุชููุฏ ููุงู. ุงู ฺฉุงุฑ ุฑุง ุฏุฑ ุงุฏุงูู ุจุง ุชุบุฑ ุชุงุจุน ุชููุฏ ูุชู ุงูุฌุงู ุฎูุงูู ุฏุงุฏ.

### ต.ณ.ณ ุชุบุฑ ุชุงุจุน ุชููุฏ ูุชู

ุญุงูุ ุชุฑฺฉุจ ุงุฒ ูููููโฺฏุฑ ุจุง ุฏูุง ู ูููููโฺฏุฑ Top-k ุฑุง ุฏุฑ ุชุงุจุน `generate_text_simple` ฺฉู ูุจูุงู ุจุฑุง ุชููุฏ ูุชู ุจุง ูุฏู ุฒุจุงู ุจุฒุฑฺฏ (LLM) ุงุณุชูุงุฏู ูโฺฉุฑุฏูุ ูพุงุฏูโุณุงุฒ ูโฺฉูู ู ฺฉ ุชุงุจุน ุฌุฏุฏ ุจู ูุงู `generate` ูโุณุงุฒู.

---

**ูุณุช ต.ด: ุชุงุจุน ุชููุฏ ูุชู ุงุตูุงุญ ุดุฏู ุจุง ุชููุน ุจุดุชุฑ**

```python
def generate(model, idx, max_new_tokens, context_size,
             temperature=0.0, top_k=None, eos_id=None):
    for _ in range(max_new_tokens):            #1
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :]
        if top_k is not None:                #2
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(
                logits < min_val,
                torch.tensor(float('-inf')).to(logits.device),
                logits
            )
        if temperature > 0.0:                  #3
            logits = logits / temperature
            probs = torch.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
        else:    #4
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)
        if idx_next == eos_id:              #5
            break
        idx = torch.cat((idx, idx_next), dim=1)
    return idx
```

**ุชูุถุญ ุฎุทูุท ฺฉุฏ:**

1. ุญููู `for` ูุดุงุจู ูุจู ุงุณุชุ ูุงุฌุชโูุง ุฑุง ุฏุฑุงูุช ูโฺฉูุฏ ู ููุท ุจู ุขุฎุฑู ฺฏุงู ุฒูุงู ุชูุฌู ูโฺฉูุฏ.
2. ุงุนูุงู ูููููโฺฏุฑ Top-k ุจุง ููุชุฑ ฺฉุฑุฏู ูุงุฌุชโูุง ูพุงูโุชุฑ ุงุฒ ุขุณุชุงูู.
3. ุงุนูุงู ููุงุณโุจูุฏ ุฏูุง ู ูููููโฺฏุฑ ุงุญุชูุงูุงุช.
4. ุงฺฏุฑ ุฏูุง ุตูุฑ ุง ฺฉูุชุฑ ุจุงุดุฏุ ุงูุชุฎุงุจ ุจู ุตูุฑุช ุทูุงุนุงูู (greedy) ุงูุฌุงู ูโุดูุฏ.
5. ุงฺฏุฑ ุชูฺฉู ูพุงุงู ุฏูุจุงูู (eos_id) ุชููุฏ ุดูุฏุ ุชููุฏ ุฒูุฏุชุฑ ูุชููู ูโุดูุฏ.

---

### ุงุฌุฑุง ุชุงุจุน generate ุจุง ุชูุธูุงุช ููููู

```python
torch.manual_seed(123)
token_ids = generate(
    model=model,
    idx=text_to_token_ids("Every effort moves you", tokenizer),
    max_new_tokens=15,
    context_size=GPT_CONFIG_124M["context_length"],
    top_k=25,
    temperature=1.4
)
print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```

ุฎุฑูุฌ ุชููุฏ ุดุฏู:

```
Output text:
 Every effort moves you stand to work on surprise, a one of us had gone
 with random-
```

ููุงูุทูุฑ ฺฉู ูุดุงูุฏู ูโฺฉููุ ูุชู ุชููุฏ ุจุณุงุฑ ูุชูุงูุช ุงุฒ ูุชู ุชููุฏ ุดุฏู ุจุง ุชุงุจุน ุณุงุฏู `generate_simple` ุฏุฑ ุจุฎุด ต.ณ ุงุณุช (ฺฉู ุฌูููโุง ุงุฒ ุฏุงุฏูโูุง ุขููุฒุด ุจูุฏ).

---

### ุชูุฑู ต.ฒ

ุจุง ุชูุธูุงุช ูุฎุชูู ุฏูุง ู Top-k ุจุงุฒ ฺฉูุฏ. ุจุฑ ุงุณุงุณ ูุดุงูุฏุงุช ุฎูุฏุ ุฏุฑ ฺู ฺฉุงุฑุจุฑุฏูุง ุชูุธูุงุช ุฏูุง ู Top-k ูพุงูโุชุฑ ูุทููุจ ุงุณุชุ ู ุฏุฑ ฺู ฺฉุงุฑุจุฑุฏูุง ุชูุธูุงุช ุจุงูุงุชุฑ ุชุฑุฌุญ ุฏุงุฏู ูโุดูุฏุ
(ูพุดููุงุฏ ูโุดูุฏ ุงู ุชูุฑู ุฑุง ูพุณ ุงุฒ ุจุงุฑฺฏุฐุงุฑ ูุฒูโูุง ูพุดโุขููุฒุดโุฏุฏู ุงุฒ OpenAI ุฏุฑ ุงูุชูุง ูุตู ูุฌุฏุฏุงู ุจุฑุฑุณ ฺฉูุฏ.)

---

### ุชูุฑู ต.ณ

ฺู ุชุฑฺฉุจโูุง ูุฎุชูู ุงุฒ ูพุงุฑุงูุชุฑูุง ุชุงุจุน `generate` ูุฌูุฏ ุฏุงุฑุฏ ฺฉู ุฑูุชุงุฑ **ูุทุน** (deterministic) ุฑุง ุชุถูู ฺฉูุฏุ ุนู ูููููโฺฏุฑ ุชุตุงุฏู ุบุฑูุนุงู ุดูุฏ ู ุฎุฑูุฌ ููุดู ูุดุงุจู ุฎุฑูุฌ ุชุงุจุน `generate_simple` ุจุงุดุฏุ

---

### ุจุฎุด ต.ด ุจุงุฑฺฏุฐุงุฑ ู ุฐุฎุฑู ูุฒูโูุง ูุฏู ุฏุฑ PyTorch

ุชุง ุงูุฌุงุ ูุญูู ุงุฑุฒุงุจ ุนุฏุฏ ูพุดุฑูุช ุขููุฒุด ู ูพุดโุขููุฒุด ฺฉ ูุฏู ุฒุจุงู ุจุฒุฑฺฏ (LLM) ุงุฒ ุงุจุชุฏุง ุฑุง ุจุฑุฑุณ ฺฉุฑุฏู. ูุฑฺูุฏ ูุฏู ู ุฏุงุฏูโูุง ูุณุจุชุงู ฺฉูฺฺฉ ุจูุฏูุฏุ ุงู ุชูุฑู ูุดุงู ุฏุงุฏ ฺฉู ูพุดโุขููุฒุด LLMโูุง ูุงุฒููุฏ ูุฒูู ูุญุงุณุจุงุช ุจุงูุง ุงุณุช. ุจูุงุจุฑุงูุ ููู ุงุณุช ฺฉู ุจุชูุงูู ูุฏู ุฑุง ุฐุฎุฑู ฺฉูู ุชุง ูุฌุจูุฑ ูุจุงุดู ูุฑ ุจุงุฑ ุจุฑุง ุงุณุชูุงุฏู ุฏุฑ ุฌูุณู ุฌุฏุฏุ ุฏูุจุงุฑู ุขู ุฑุง ุขููุฒุด ุฏูู.

ุฏุฑ ุงู ุจุฎุดุ ูุญูู ุฐุฎุฑู ู ุจุงุฑฺฏุฐุงุฑ ูุฏู ูพุดโุขููุฒุดโุฏุฏู ุฑุง ฺฉู ุฏุฑ ุดฺฉู ต.ฑถ ูุดุงู ุฏุงุฏู ุดุฏูุ ุชูุถุญ ูโุฏูู. ุณูพุณ ฺฉ ูุฏู GPT ูพุดโุขููุฒุดโุฏุฏู ููโุชุฑ ุงุฒ OpenAI ุฑุง ุฏุฑ ููููู GPTModel ุฎูุฏ ุจุงุฑฺฏุฐุงุฑ ูโฺฉูู.

---

**ุดฺฉู ต.ฑถ:** ูพุณ ุงุฒ ุขููุฒุด ู ุจุฑุฑุณ ูุฏูุ ูุนูููุงู ููุฏ ุงุณุช ฺฉู ูุฏู ุฑุง ุฐุฎุฑู ฺฉูู ุชุง ุจุนุฏุงู ุจุชูุงูู ุงุฒ ุขู ุงุณุชูุงุฏู ฺฉูู ุง ุขููุฒุด ุขู ุฑุง ุงุฏุงูู ุฏูู (ูุฑุญูู ถ).

ุฎูุดุจุฎุชุงููุ ุฐุฎุฑู ูุฏู ุฏุฑ PyTorch ูุณุจุชุงู ุณุงุฏู ุงุณุช. ุฑูุด ูพุดููุงุฏ ุฐุฎุฑู `state_dict` ูุฏู ุงุณุชุ ุฏฺฉุดูุฑโุง ฺฉู ูุฑ ูุงู ุฑุง ุจู ูพุงุฑุงูุชุฑูุง ุขู ูฺฏุงุดุช ูโฺฉูุฏ. ุจุฑุง ุงู ฺฉุงุฑ ุงุฒ ุชุงุจุน `torch.save` ุงุณุชูุงุฏู ูโฺฉูู:

```python
torch.save(model.state_dict(), "model.pth")
```

`model.pth` ูุงู ูุงู ุงุณุช ฺฉู `state_dict` ุฏุฑ ุขู ุฐุฎุฑู ูโุดูุฏ. ูพุณููุฏ `.pth` ุฏุฑ PyTorch ูุชุฏุงูู ุงุณุช ุงูุง ูโุชูุงู ุงุฒ ูุฑ ูพุณููุฏ ุฏฺฏุฑ ูุฒ ุงุณุชูุงุฏู ฺฉุฑุฏ.

ุจุนุฏ ุงุฒ ุฐุฎุฑู ูุฒูโูุงุ ูโุชูุงูู ุขูโูุง ุฑุง ุฏุฑ ฺฉ ููููู ุฌุฏุฏ ุงุฒ ูุฏู ุจุงุฑฺฏุฐุงุฑ ฺฉูู:

```python
model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(torch.load("model.pth", map_location=device))
model.eval()
```

ููุงูุทูุฑ ฺฉู ุฏุฑ ูุตู ด ุจุญุซ ุดุฏุ ุฏุฑ ุญู ุขููุฒุดุ ุฏุฑุงูพโุงูุช ุงุฒ ุจุดโุจุฑุงุฒุด ูุฏู ุฌููฺฏุฑ ูโฺฉูุฏุ ุงูุง ููฺฏุงู ุงุณุชูุชุงุฌ (Inference) ููโุฎูุงูู ุงุทูุงุนุงุช ูุฏู ุจู ุตูุฑุช ุชุตุงุฏู ุญุฐู ุดููุฏ. ุฏุณุชูุฑ `model.eval()` ูุฏู ุฑุง ุจู ุญุงูุช ุงุฑุฒุงุจ ูโุจุฑุฏ ู ุฏุฑุงูพโุงูุช ุฑุง ุบุฑูุนุงู ูโฺฉูุฏ. ุงฺฏุฑ ูุตุฏ ุงุฏุงูู ูพุดโุขููุฒุด ูุฏู ุฑุง ุฏุงุดุชู ุจุงุดูุ ุฐุฎุฑู ูุถุนุช ุจูููโุณุงุฒ ูุฒ ุชูุตู ูโุดูุฏ.

ุจูููโุณุงุฒูุง ุชุทุจู ูุงููุฏ AdamW ูพุงุฑุงูุชุฑูุง ุงุถุงูโุง ุจุฑุง ูุฑ ูุฒู ูุฏู ุฐุฎุฑู ูโฺฉููุฏ ฺฉู ูุฑุฎ ุงุฏฺฏุฑ ูุฑ ูพุงุฑุงูุชุฑ ุฑุง ูพูุง ุชูุธู ูโฺฉูุฏ. ุจุฏูู ุงู ุงุทูุงุนุงุชุ ุจูููโุณุงุฒ ุฑุณุช ูโุดูุฏ ู ูุฏู ููฺฉู ุงุณุช ุจููู ุขููุฒุด ูุจูุฏ ุง ููฺฏุฑุง ูุดูุฏ ฺฉู ุจุงุนุซ ุงุฒ ุฏุณุช ุฑูุชู ูุงุจูุช ุชููุฏ ูุชู ููโุจุณุชู ุฎูุงูุฏ ุดุฏ. ุจุง `torch.save` ูโุชูุงูู ูุถุนุช ูุฏู ู ุจูููโุณุงุฒ ุฑุง ููุฒูุงู ุฐุฎุฑู ฺฉูู:

```python
torch.save({
    "model_state_dict": model.state_dict(),
    "optimizer_state_dict": optimizer.state_dict(),
    },
    "model_and_optimizer.pth"
)
```

ุณูพุณ ุจุฑุง ุจุงุฒฺฏุฑุฏุงูุฏู ูุถุนุชุ ุงุจุชุฏุง ุฏุงุฏูโูุง ุฑุง ุจุงุฑฺฏุฐุงุฑ ฺฉุฑุฏู ู ุณูพุณ ุจุง `load_state_dict` ูุถุนุชโูุง ุฑุง ุจุงุฒูุดุงู ูโฺฉูู:

```python
checkpoint = torch.load("model_and_optimizer.pth", map_location=device)
model = GPTModel(GPT_CONFIG_124M)
model.load_state_dict(checkpoint["model_state_dict"])
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)
optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
model.train();
```

---

**ุชูุฑู ต.ด:**
ูพุณ ุงุฒ ุฐุฎุฑู ูุฒูโูุงุ ูุฏู ู ุจูููโุณุงุฒ ุฑุง ุฏุฑ ฺฉ ุฌูุณู ุฌุฏุฏ ูพุงุชูู ุง Jupyter ุจุงุฑฺฏุฐุงุฑ ฺฉูุฏ ู ุจุง ุชุงุจุน `train_model_simple` ฺฉ ุฏูุฑู ุขููุฒุด ุฏฺฏุฑ ุงุฏุงูู ุฏูุฏ.

---

## ุจุฎุด ต.ต ุจุงุฑฺฏุฐุงุฑ ูุฒูโูุง ูพุดโุขููุฒุดโุฏุฏู ุงุฒ OpenAI

ูุจูุงู ฺฉ ูุฏู ฺฉูฺฺฉ GPT-2 ุฑุง ุฑู ูุฌููุนู ุฏุงุฏูโุง ูุญุฏูุฏ (ฺฉุชุงุจ ุฏุงุณุชุงู ฺฉูุชุงู) ุขููุฒุด ุฏุงุฏู ุชุง ุงุตูู ูพุงู ุฑุง ุจุฏูู ุตุฑู ุฒูุงู ู ููุงุจุน ุฒุงุฏ ุจุงููุฒู.

ุฎูุดุจุฎุชุงูู OpenAI ูุฒูโูุง ูุฏูโูุง GPT-2 ุฎูุฏ ุฑุง ุจู ุตูุฑุช ูุชูโุจุงุฒ ููุชุดุฑ ฺฉุฑุฏู ุงุณุชุ ูพุณ ูุงุฒ ุจู ุตุฑู ูุฒููโูุง ุฒุงุฏ ุจุฑุง ุขููุฒุด ูุฌุฏุฏ ูุฏู ุฑู ุฏุงุฏูโูุง ุจุฒุฑฺฏ ูุณุช. ุญุงูุง ุงู ูุฒูโูุง ุฑุง ุฏุฑ ฺฉูุงุณ `GPTModel` ุจุงุฑฺฏุฐุงุฑ ูโฺฉูู ู ูุฏู ุฑุง ุจุฑุง ุชููุฏ ูุชู ุจู ฺฉุงุฑ ูโุจุฑู. ูุฒูโูุง ูพุงุฑุงูุชุฑูุง ุฐุฎุฑูโุดุฏู ุฏุฑ ูฺฺฏ `.weight` ูุงูโูุง PyTorch ูุงููุฏ Linear ู Embedding ูุณุชูุฏ.

ุฏุฑ ูุตู ถุ ุงุฒ ุงู ูุฒูโูุง ุจุฑุง ุชูุธู ุฏูู (Fine-tuning) ูุฏู ุฏุฑ ฺฉ ฺฉุงุฑ ุฏุณุชูโุจูุฏ ูุชู ู ูพุฑู ุงุฒ ุฏุณุชูุฑุงุช ูุงููุฏ ChatGPT ุงุณุชูุงุฏู ุฎูุงูู ฺฉุฑุฏ.

---

**ูฺฉุชู:**
OpenAI ูุฒูโูุง GPT-2 ุฑุง ุฏุฑ ุงุจุชุฏุง ุจุง TensorFlow ุฐุฎุฑู ฺฉุฑุฏู ุจูุฏ ฺฉู ุจุงุฏ ุขู ุฑุง ูุตุจ ฺฉูู. ููฺูู ุจุฑุง ููุงุด ูพุดุฑูุช ุฏุงูููุฏ ุงุฒ ฺฉุชุงุจุฎุงูู `tqdm` ุงุณุชูุงุฏู ูโุดูุฏ:

```bash
pip install tensorflow>=2.15.0 tqdm>=4.66
```

ฺฉุฏ ุฏุงูููุฏ ูุณุจุชุง ุทููุงู ู ฺฉููุงุฎุช ุงุณุชุ ุจูุงุจุฑุงู ูุง ูุงู `gpt_download.py` ุฑุง ูุณุชููุงู ุงุฒ ูุฎุฒู ุขููุงู ุงู ูุตู ุฏุฑุงูุช ูโฺฉูู:

```python
import urllib.request
url = (
    "https://raw.githubusercontent.com/rasbt/"
    "LLMs-from-scratch/main/ch05/"
    "01_main-chapter-code/gpt_download.py"
)
filename = url.split('/')[-1]
urllib.request.urlretrieve(url, filename)
```

ุจุนุฏ ุงุฒ ุฏุงูููุฏุ ุจูุชุฑ ุงุณุช ูุญุชูุงุช ูุงู ุฑุง ุจุฑุฑุณ ฺฉูุฏ ุชุง ุงุฒ ุตุญุช ุขู ูุทูุฆู ุดูุฏ.

ุญุงูุง ุชุงุจุน `download_and_load_gpt2` ุฑุง ูุงุฑุฏ ูโฺฉูู ฺฉู ุชูุธูุงุช ูุนูุงุฑ (`settings`) ู ูพุงุฑุงูุชุฑูุง ูุฒู (`params`) ุฑุง ุจุงุฑฺฏุฐุงุฑ ูโฺฉูุฏ:

```python
from gpt_download import download_and_load_gpt2
settings, params = download_and_load_gpt2(
    model_size="124M", models_dir="gpt2"
)
```

ุงู ฺฉุฏ ููุช ูุงู ูุฑุชุจุท ุจุง ูุฏู GPT-2 ุจุง ฑฒด ูููู ูพุงุฑุงูุชุฑ ุฑุง ุฏุงูููุฏ ูโฺฉูุฏ.

---

**ฺุงูพ ุชูุธูุงุช ู ฺฉูุฏูุง ุฏฺฉุดูุฑ ูุฒูโูุง:**

```python
print("Settings:", settings)
print("Parameter dictionary keys:", params.keys())
```

ูุญุชูุงุช:

```
Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}
Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])
```

- `settings`: ุดุงูู ุชูุธูุงุช ูุนูุงุฑ ูุฏู ูุดุงุจู `GPT_CONFIG_124M` ุงุณุช.
- `params`: ุฏฺฉุดูุฑ ูุฒูโูุง ูุงูุน ูุฏู ุงุณุช.

ูโุชูุงูู ูุฒูโูุง ูุงู ุฌุงุณุงุฒ ุชูฺฉู ุฑุง ุงูโฺฏููู ุจุฑุฑุณ ฺฉูู:

```python
print(params["wte"])
print("Token embedding weight tensor dimensions:", params["wte"].shape)
```

ุงุจุนุงุฏ ูุฒู ูุงู ุฌุงุณุงุฒ: `(50257, 768)`

---

OpenAI ููฺูู ูุฒู ูุฏูโูุง ุจุฒุฑฺฏุชุฑ ุจุง ูพุงุฑุงูุชุฑูุง ณตตMุ ททดM ู ฑตตธM ุฑุง ููุชุดุฑ ฺฉุฑุฏู ุงุณุช ฺฉู ุณุงุฎุชุงุฑ ฺฉู ูุดุงุจู ุฏุงุฑูุฏ ุงูุง ุงูุฏุงุฒู ุฌุงุณุงุฒ ู ุชุนุฏุงุฏ ูุงูโูุง ู ุณุฑูุง ุชูุฌู ูุชูุงูุช ุงุณุช (ุดฺฉู ต.ฑท).

---

### ุขูุงุฏูโุณุงุฒ ูพฺฉุฑุจูุฏ ูุฏู ุจุฑุง ุจุงุฑฺฏุฐุงุฑ ูุฒูโูุง

ฺฉ ุฏฺฉุดูุฑ ุจุฑุง ูุดุฎุตุงุช ูุฏูโูุง ูุฎุชูู GPT-2 ุชุนุฑู ูโฺฉูู:

```python
model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}
```

ุจุฑุง ุจุงุฑฺฏุฐุงุฑ ูุฏู ฺฉูฺฺฉ ฑฒดM:

```python
model_name = "gpt2-small (124M)"
NEW_CONFIG = GPT_CONFIG_124M.copy()
NEW_CONFIG.update(model_configs[model_name])
```

ุชูุฌู ุฏุงุดุชู ุจุงุดุฏ ฺฉู ูุฏูโูุง ุงุตู GPT-2 ุจุง ุทูู ุชูฺฉู ฑฐฒด ุขููุฒุด ุฏุฏูโุงูุฏุ ูพุณ ุจุงุฏ ุงู ููุฏุงุฑ ุฑุง ุจูโุฑูุฒุฑุณุงู ฺฉูู:

```python
NEW_CONFIG.update({"context_length": 1024})
```

ููฺูู OpenAI ุงุฒ ุจุฑุฏุงุฑูุง ุจุงุงุณ ุฏุฑ ูุงูโูุง ุฎุท ุชูุฌู ฺูุฏุณุฑ ุงุณุชูุงุฏู ฺฉุฑุฏู ุงุณุช ฺฉู ูุง ูุฒ ุจุฑุง ุณุงุฒฺฏุงุฑ ุขู ุฑุง ูุนุงู ูโฺฉูู:

```python
NEW_CONFIG.update({"qkv_bias": True})
```

ุญุงูุง ูุฏู ุฑุง ุจุง ุงู ูพฺฉุฑุจูุฏ ุฌุฏุฏ ููุฏุงุฑุฏู ุงููู ูโฺฉูู:

```python
gpt = GPTModel(NEW_CONFIG)
gpt.eval()
```

---

### ุจุงุฑฺฏุฐุงุฑ ูุฒูโูุง ุงุฒ ุฏฺฉุดูุฑ `params` ุจู ูุฏู

ฺฉ ุชุงุจุน ฺฉูฺฉ `assign` ุชุนุฑู ูโฺฉูู ฺฉู ุดฺฉู ุฏู ุขุฑุงู ุฑุง ุจุฑุฑุณ ฺฉุฑุฏู ู ูุฒูโูุง ุฑุง ุจู ุตูุฑุช ูพุงุฑุงูุชุฑูุง ูุงุจู ุขููุฒุด PyTorch ุชุจุฏู ูโฺฉูุฏ:

```python
def assign(left, right):
    if left.shape != right.shape:
        raise ValueError(f"Shape mismatch. Left: {left.shape}, Right: {right.shape}")
    return torch.nn.Parameter(torch.tensor(right))
```

ุณูพุณ ุชุงุจุน `load_weights_into_gpt` ุฑุง ุชุนุฑู ูโฺฉูู ฺฉู ูุฒูโูุง ุฑุง ุจู ูุฏู ููุชูู ูโฺฉูุฏ:

```python
import numpy as np

def load_weights_into_gpt(gpt, params):
    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])
    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])

    for b in range(len(params["blocks"])):
        q_w, k_w, v_w = np.split(params["blocks"][b]["attn"]["c_attn"]["w"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.weight = assign(gpt.trf_blocks[b].att.W_query.weight, q_w.T)
        gpt.trf_blocks[b].att.W_key.weight = assign(gpt.trf_blocks[b].att.W_key.weight, k_w.T)
        gpt.trf_blocks[b].att.W_value.weight = assign(gpt.trf_blocks[b].att.W_value.weight, v_w.T)

        q_b, k_b, v_b = np.split(params["blocks"][b]["attn"]["c_attn"]["b"], 3, axis=-1)
        gpt.trf_blocks[b].att.W_query.bias = assign(gpt.trf_blocks[b].att.W_query.bias, q_b)
        gpt.trf_blocks[b].att.W_key.bias = assign(gpt.trf_blocks[b].att.W_key.bias, k_b)
        gpt.trf_blocks[b].att.W_value.bias = assign(gpt.trf_blocks[b].att.W_value.bias, v_b)

        gpt.trf_blocks[b].att.out_proj.weight = assign(gpt.trf_blocks[b].att.out_proj.weight, params["blocks"][b]["attn"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].att.out_proj.bias = assign(gpt.trf_blocks[b].att.out_proj.bias, params["blocks"][b]["attn"]["c_proj"]["b"])

        gpt.trf_blocks[b].ff.layers[0].weight = assign(gpt.trf_blocks[b].ff.layers[0].weight, params["blocks"][b]["mlp"]["c_fc"]["w"].T)
        gpt.trf_blocks[b].ff.layers[0].bias = assign(gpt.trf_blocks[b].ff.layers[0].bias, params["blocks"][b]["mlp"]["c_fc"]["b"])
        gpt.trf_blocks[b].ff.layers[2].weight = assign(gpt.trf_blocks[b].ff.layers[2].weight, params["blocks"][b]["mlp"]["c_proj"]["w"].T)
        gpt.trf_blocks[b].ff.layers[2].bias = assign(gpt.trf_blocks[b].ff.layers[2].bias, params["blocks"][b]["mlp"]["c_proj"]["b"])

        gpt.trf_blocks[b].norm1.scale = assign(gpt.trf_blocks[b].norm1.scale, params["blocks"][b]["ln_1"]["g"])
        gpt.trf_blocks[b].norm1.shift = assign(gpt.trf_blocks[b].norm1.shift, params["blocks"][b]["ln_1"]["b"])
        gpt.trf_blocks[b].norm2.scale = assign(gpt.trf_blocks[b].norm2.scale, params["blocks"][b]["ln_2"]["g"])
        gpt.trf_blocks[b].norm2.shift = assign(gpt.trf_blocks[b].norm2.shift, params["blocks"][b]["ln_2"]["b"])

    gpt.final_norm.scale = assign(gpt.final_norm.scale, params["g"])
    gpt.final_norm.shift = assign(gpt.final_norm.shift, params["b"])
    gpt.out_head.weight = assign(gpt.out_head.weight, params["wte"])
```

- ูฺฉุชู ููู: OpenAI ูุฒูโูุง ูุงู ุฎุฑูุฌ ุฑุง ููุงู ูุฒูโูุง ุฌุงุณุงุฒ ุชูฺฉูโูุง (weight tying) ุงุณุชูุงุฏู ฺฉุฑุฏู ฺฉู ุจุงุนุซ ฺฉุงูุด ุชุนุฏุงุฏ ฺฉู ูพุงุฑุงูุชุฑูุง ูโุดูุฏ.

---

### ุงุณุชูุงุฏู ุงุฒ ุชุงุจุน ุจุงุฑฺฏุฐุงุฑ ูุฒูโูุง ู ุชููุฏ ูุชู

```python
load_weights_into_gpt(gpt, params)
gpt.to(device)

torch.manual_seed(123)
token_ids = generate(
    model=gpt,
    idx=text_to_token_ids("Every effort moves you", tokenizer).to(device),
    max_new_tokens=25,
    context_size=NEW_CONFIG["context_length"],
    top_k=50,
    temperature=1.5
)
print("Output text:\n", token_ids_to_text(token_ids, tokenizer))
```

ุฎุฑูุฌ ููููู:

```
Output text:
 Every effort moves you toward finding an ideal new way to practice something!
What makes us want to be on top of that?
```

ุงู ุฎุฑูุฌ ูุดุงู ูโุฏูุฏ ฺฉู ูุฒูโูุง ุจู ุฏุฑุณุช ุจุงุฑฺฏุฐุงุฑ ุดุฏูโุงูุฏุ ุฒุฑุง ูุฏู ูุชู ููโุจุณุชู ู ูุนูุงุฏุงุฑ ุชููุฏ ูโฺฉูุฏ.

---

### ุชูุฑู ต.ต

**ูุญุงุณุจูโ ููุงุฏุฑ ุฎุทุง ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ (training ู validation losses) ุจุฑุง ูุฏู GPT ุจุง ุงุณุชูุงุฏู ุงุฒ ูุฒูโูุง ูพุดโุขููุฒุด ุฏุฏู ุดุฏู ุงุฒ OpenAI ุฑู ุฏุงุฏูโูุง ูุฌููุนู "The Verdict".**

- ูุฏู: ุจุง ุงุณุชูุงุฏู ุงุฒ ูุฒูโูุง ูพุดโุขููุฒุด ูุฏู GPT ฺฉู ุงุฒ OpenAI ุจุงุฑฺฏุฐุงุฑ ุดุฏูโุงูุฏุ ููุงุฏุฑ ุฎุทุง ุฑุง ุฑู ูุฌููุนู ุฏุงุฏู ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ ูุญุงุณุจู ฺฉูุฏ.
- ุงู ฺฉุงุฑ ุจู ุดูุง ุงูฺฉุงู ูโุฏูุฏ ฺฉูุช ุนููฺฉุฑุฏ ูุฏู ุฑุง ุฑู ุฏุงุฏูโูุง ูุงูุน ุจุฑุฑุณ ฺฉูุฏ ู ูุฒุงู ุชุทุงุจู ูุฏู ุจุง ุฏุงุฏูโูุง ุฏุฏู ุดุฏู ู ูุฏุฏู ุฑุง ุจุณูุฌุฏ.

---

### ุชูุฑู ต.ถ

**ุขุฒูุงุด ูุฏูโูุง GPT-2 ุจุง ุงูุฏุงุฒูโูุง ูุฎุชููุ ุจูโุนููุงู ูุซุงู ูุฏู ุจุฒุฑฺฏโุชุฑู ุจุง 1,558 ูููู ูพุงุฑุงูุชุฑุ ู ููุงุณูโ ูุชู ุชููุฏุดุฏู ุจุง ูุฏู 124 ูููู ูพุงุฑุงูุชุฑ.**

- ูุฏู: ูุฏูโูุง GPT-2 ุจุง ุงูุฏุงุฒูโูุง ูุชูุงูุช ุฑุง ุจุงุฑฺฏุฐุงุฑ ู ุงุฌุฑุง ฺฉูุฏ.
- ุณูพุณ ูุชู ุชููุฏุดุฏู ุชูุณุท ุงู ูุฏูโูุง ุฑุง ุจุง ฺฉุฏฺฏุฑ ููุงุณู ฺฉูุฏ ุชุง ุชุฃุซุฑ ุงูุฏุงุฒู ูุฏู ุจุฑ ฺฉูุช ู ุชููุน ูุชู ุฎุฑูุฌ ุฑุง ุจุฑุฑุณ ฺฉูุฏ.
- ุงู ุชูุฑู ุจู ุดูุง ุฏุฑฺฉ ุจูุชุฑ ุงุฒ ุฑุงุจุทู ุจู ุงูุฏุงุฒู ูุฏู ู ุชูุงูุง ุชููุฏ ูุชู ุฏูู ู ููุณุฌู ูโุฏูุฏ.

## ุฎูุงุตู

- ููฺฏุงู ุชููุฏ ูุชู ุชูุณุท ูุฏูโูุง ุฒุจุงู ุจุฒุฑฺฏ (LLMs)ุ ุฎุฑูุฌ ุจู ุตูุฑุช ฺฏุงู ุจู ฺฏุงู ู ฺฉ ุชูฺฉู ุฏุฑ ูุฑ ุจุงุฑ ุชููุฏ ูโุดูุฏ.
- ุจู ุทูุฑ ูพุดโูุฑุถุ ุชูฺฉู ุจุนุฏ ุจุง ุชุจุฏู ุฎุฑูุฌ ูุฏู ุจู ุงูุชุงุฒูุง ุงุญุชูุงู ู ุงูุชุฎุงุจ ุชูฺฉู ฺฉู ุจุดุชุฑู ุงุญุชูุงู ุฑุง ุฏุงุฑุฏ ุชููุฏ ูโุดูุฏ ฺฉู ุจู ุขู ยซุฑูุฒฺฏุดุง ุญุฑุตุงููยป (greedy decoding) ฺฏูุชู ูโุดูุฏ.
- ุจุง ุงุณุชูุงุฏู ุงุฒ ูููููโฺฏุฑ ุงุญุชูุงูุงุช ู ุชูุธู ุฏูุง (temperature scaling)ุ ูโุชูุงู ุชููุน ู ุงูุณุฌุงู ูุชู ุชููุฏุดุฏู ุฑุง ฺฉูุชุฑู ฺฉุฑุฏ.
- ููุงุฏุฑ ุฎุทุง (loss) ุฏุฑ ูุฌููุนูโูุง ุขููุฒุด ู ุงุนุชุจุงุฑุณูุฌ ูโุชูุงููุฏ ุจุฑุง ุงุฑุฒุงุจ ฺฉูุช ูุชู ุชููุฏุดุฏู ุชูุณุท ูุฏู ุฏุฑ ุทูู ุขููุฒุด ุจู ฺฉุงุฑ ุฑููุฏ.
- ูพุดโุขููุฒุด ฺฉ ูุฏู ุฒุจุงู ุจุฒุฑฺฏ ุดุงูู ุจูโุฑูุฒุฑุณุงู ูุฒูโูุง ุจู ููุธูุฑ ฺฉููู ฺฉุฑุฏู ุฎุทุง ุขููุฒุด ุงุณุช.
- ุญููู ุขููุฒุด ูุฏูโูุง ุฒุจุงู ุจุฒุฑฺฏุ ุฎูุฏ ฺฉ ุฑููุฏ ุงุณุชุงูุฏุงุฑุฏ ุฏุฑ ุงุฏฺฏุฑ ุนูู ุงุณุช ฺฉู ุงุฒ ุชุงุจุน ุฎุทุง ูุชูุงุทุน (cross entropy) ู ุจูููโุณุงุฒ AdamW ุงุณุชูุงุฏู ูโฺฉูุฏ.
- ูพุดโุขููุฒุด ูุฏู ุฑู ูุฌููุนู ุฏุงุฏู ุจุฒุฑฺฏุ ุฒูุงูโุจุฑ ู ูพุฑูุฒูู ุงุณุชุ ุจูุงุจุฑุงู ูโุชูุงู ุจู ุฌุง ูพุดโุขููุฒุด ุงุฒ ูุฒูโูุง ูพุดโุขููุฒุด ุฏุฏู ุดุฏู ู ุฏุฑ ุฏุณุชุฑุณ ุนููู ุงุณุชูุงุฏู ฺฉุฑุฏ.


> [ 
        4.ูพุงุฏูโุณุงุฒ ูุฏู GPT ุงุฒ ุงุจุชุฏุง ุจุฑุง ุชููุฏ ูุชู
     (ูุจู) ](
        <04.Implementing a GPT model from scratch to generate text.md>
        ) <
    5.ูพุดโุขููุฒุด ุฑู ุฏุงุฏูโูุง ุจุฏูู ุจุฑฺุณุจ>[
    6.ุชูุธู ุฏูู  ุจุฑุง ุฏุณุชูโุจูุฏ 
(ุจุนุฏ)
](<06.Fine-tuning for classification.md>)