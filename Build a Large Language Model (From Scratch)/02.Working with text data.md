<!-- language: rtl -->

# ۲.کار با داده‌های متنی

تا اینجا با ساختار کلی مدل‌های زبانی بزرگ (LLM) آشنا شدیم و دانستیم که این مدل‌ها با حجم بسیار زیادی از متن از پیش آموزش داده می‌شوند. تمرکز ما به‌ویژه بر مدل‌های **فقط دیکودر (decoder-only)** مبتنی بر معماری **ترنسفورمر** بود؛ معماری‌ای که در مدل‌هایی مانند **ChatGPT** و سایر مدل‌های مشابه GPT استفاده می‌شود.

در مرحله پیش‌آموزش (pretraining)، مدل‌های زبانی، متن را **کلمه به کلمه** پردازش می‌کنند. آموزش این مدل‌ها با میلیون‌ها یا حتی میلیاردها پارامتر، با استفاده از وظیفه‌ای به نام **پیش‌بینی کلمه بعدی** (next-word prediction)، منجر به تولید مدل‌هایی با قابلیت‌های چشمگیر می‌شود. پس از آن، این مدل‌ها می‌توانند برای انجام وظایف خاص یا پیروی از دستورالعمل‌های عمومی **تنظیم دقیق (fine-tuning)** شوند.

اما پیش از آنکه بتوانیم یک مدل زبانی بزرگ را پیاده‌سازی و آموزش دهیم، باید **داده‌های آموزشی را آماده کنیم**؛ روندی که در **شکل ۲.۱** به تصویر کشیده شده است. این فصل بر **مرحله اول از گام اول** یعنی **پیاده‌سازی پایپ‌لاین نمونه‌برداری از داده‌ها** تمرکز دارد.

در این فصل یاد می‌گیرید که چگونه ورودی‌های متنی را برای آموزش مدل‌های زبانی آماده کنید. این فرآیند شامل **تقسیم متن به توکن‌های کلمه‌ای یا زیرکلمه‌ای** و سپس **کدگذاری آن‌ها به بردارهای عددی** است که مدل می‌تواند از آن‌ها استفاده کند. همچنین با **روش‌های پیشرفته توکن‌سازی** مانند **رمزگذاری جفت‌بایتی (Byte Pair Encoding)** آشنا می‌شوید که در مدل‌هایی مانند GPT کاربرد دارند. در نهایت، روش‌هایی برای **نمونه‌برداری از داده‌ها و بارگذاری آن‌ها** جهت تولید جفت‌های ورودی–خروجی برای آموزش مدل ارائه خواهد شد.

---

## ۲.۱ درک بردارهای تعبیه‌شده‌ی واژه (Word Embeddings)

مدل‌های عصبی عمیق، از جمله مدل‌های زبانی بزرگ، **قادر به پردازش مستقیم متن خام نیستند**. دلیل این امر آن است که متن، **داده‌ای دسته‌ای (categorical)** است و با عملیات ریاضی‌ای که برای آموزش شبکه‌های عصبی لازم است، سازگار نیست. بنابراین، نیاز داریم که کلمات را به شکل **بردارهایی با مقادیر پیوسته** نمایش دهیم.

> **نکته:** اگر با مفاهیم بردار و تنسور در زمینه محاسباتی آشنا نیستید، می‌توانید به **ضمیمه A، بخش A.2.2** مراجعه کنید.

فرآیند تبدیل داده‌های غیرعددی به بردارهای عددی، معمولاً با عنوان **تعبیه (Embedding)** شناخته می‌شود. با استفاده از یک لایه خاص در شبکه عصبی یا یک مدل از پیش آموزش‌دیده، می‌توان داده‌هایی از انواع مختلف مانند **ویدئو، صدا یا متن** را به فضای برداری منتقل کرد. البته باید توجه داشت که هر نوع داده نیاز به مدل تعبیه مخصوص به خود دارد؛ برای مثال، مدلی که برای تعبیه متن طراحی شده باشد، برای تعبیه ویدئو یا صدا مناسب نیست.

**شکل ۲.۲** این موضوع را به‌خوبی نشان می‌دهد؛ اینکه داده‌های خام مانند متن، ویدئو یا صدا مستقیماً برای مدل قابل استفاده نیستند و باید ابتدا به بردارهای فشرده و عددی تبدیل شوند تا مدل بتواند آن‌ها را پردازش کند.

در اصل، تعبیه، نوعی نگاشت (mapping) از اشیای گسسته – مانند کلمات، تصاویر یا حتی کل اسناد – به **نقاطی در یک فضای برداری پیوسته** است. هدف اصلی از تعبیه این است که داده‌های غیرعددی را به فرمتی قابل استفاده برای شبکه‌های عصبی تبدیل کند.

گرچه **تعبیه واژه‌ای (word embedding)** رایج‌ترین شکل تعبیه برای متن است، اما نسخه‌های پیشرفته‌تری نیز وجود دارد، مانند تعبیه‌های جمله‌ای یا پاراگرافی. این نوع تعبیه‌ها در تکنیکی به نام **تولید مبتنی بر بازیابی (Retrieval-Augmented Generation)** بسیار کاربرد دارند. در این روش، مدل هم عمل تولید متن و هم بازیابی اطلاعات از منابع خارجی را انجام می‌دهد. البته این مبحث، خارج از حیطه این کتاب است.

از آنجا که هدف ما **آموزش مدل‌هایی مانند GPT** است که متن را **کلمه به کلمه تولید می‌کنند**، تمرکز ما بر تعبیه واژه‌ای خواهد بود.

---

چندین الگوریتم و چارچوب برای تولید تعبیه‌های واژه‌ای توسعه یافته‌اند. یکی از روش‌های ابتدایی و محبوب، **Word2Vec** است. این روش از یک معماری شبکه عصبی برای تولید بردارهای واژه‌ای استفاده می‌کند، به‌طوری‌که یا **کلمه هدف را بر اساس کلمات زمینه پیش‌بینی می‌کند** یا بالعکس. ایده اصلی Word2Vec این است که **کلماتی که در زمینه‌های مشابه ظاهر می‌شوند، معانی مشابهی دارند**. به همین دلیل، زمانی که این بردارها را به دو بعد کاهش می‌دهیم تا به‌صورت گرافیکی نمایش داده شوند، واژه‌های مشابه در کنار یکدیگر قرار می‌گیرند. **شکل ۲.۳** این پدیده را نمایش می‌دهد.

تعبیه‌های واژه‌ای می‌توانند ابعاد مختلفی داشته باشند؛ از یک بعد گرفته تا هزاران بعد. افزایش تعداد ابعاد می‌تواند روابط ظریف‌تری بین کلمات را ضبط کند، اما **هزینه محاسباتی** بیشتری نیز به همراه دارد.

> در **شکل ۲.۳**، بردارهای دوبعدی واژه‌ها در یک نمودار پراکندگی دوبعدی رسم شده‌اند. این نمودار نشان می‌دهد که واژه‌های دارای مفاهیم مشابه، در فضای برداری به هم نزدیک‌ترند؛ برای مثال، اسامی پرندگان در نزدیکی یکدیگر قرار دارند، در حالی‌که نام کشورها و شهرها از آن‌ها فاصله دارند.

در حالی که می‌توان از مدل‌هایی مانند Word2Vec برای تولید بردارهای تعبیه واژه استفاده کرد، مدل‌های زبانی بزرگ معمولاً **بردارهای تعبیه خود را تولید می‌کنند**. این بردارها بخشی از **لایه ورودی مدل** هستند و در طول آموزش مدل **به‌روزرسانی می‌شوند**. مزیت این روش آن است که تعبیه‌ها به‌طور خاص برای **وظیفه و داده مورد نظر بهینه می‌شوند**، در حالی‌که Word2Vec تعبیه‌هایی عمومی تولید می‌کند.

در ادامه این فصل، لایه‌های تعبیه را که در مدل‌های زبانی استفاده می‌شوند پیاده‌سازی خواهیم کرد. همچنین در فصل سوم، به **بردارهای تعبیه خروجی زمینه‌محور (contextualized embeddings)** نیز خواهیم پرداخت.

یکی از چالش‌های مهم در مورد تعبیه‌های با ابعاد بالا، **محدودیت درک بصری انسان** و ابزارهای گرافیکی برای نمایش آن‌هاست؛ چراکه ما معمولاً نمی‌توانیم بیش از سه بعد را به‌صورت تصویری درک کنیم. به همین دلیل، در شکل ۲.۳ فقط ابعاد دوگانه نمایش داده شده است. با این حال، در عمل، مدل‌های LLM معمولاً از تعبیه‌هایی با ابعاد بسیار بالا استفاده می‌کنند.

برای نمونه، مدل‌های کوچک‌تر GPT-2 (با ۱۱۷ یا ۱۲۵ میلیون پارامتر) از بردارهایی با **۷۶۸ بُعد** استفاده می‌کنند. در مقابل، بزرگ‌ترین نسخه GPT-3 با **۱۷۵ میلیارد پارامتر**، از بردارهایی با **۱۲٬۲۸۸ بُعد** بهره می‌برد. این انتخاب همواره یک **موازنه بین کارایی و بهره‌وری محاسباتی** است.

در بخش بعدی، مراحل لازم برای آماده‌سازی تعبیه‌های مورد استفاده در مدل‌های زبانی را بررسی خواهیم کرد. این مراحل شامل **تقسیم متن به کلمات، تبدیل کلمات به توکن و سپس تبدیل توکن‌ها به بردارهای تعبیه** است.

---

## ۲.۲ توکنیزه‌کردن متن

در این بخش، به بررسی نحوه تقسیم متن ورودی به **توکن‌های جداگانه** می‌پردازیم؛ این کار یکی از مراحل ضروری پیش‌پردازش است که برای ساخت بردارهای جاسازی (embedding) در مدل‌های زبانی بزرگ (LLM) انجام می‌شود. این توکن‌ها می‌توانند **واژه‌های مستقل یا نویسه‌های خاص** مانند علائم نگارشی باشند، همان‌طور که در شکل ۲.۴ نشان داده شده است.

> **شکل ۲.۴**: نمایی از مراحل پردازش متن در زمینه مدل‌های زبانی. در اینجا، متن ورودی به توکن‌هایی مجزا شامل واژه‌ها یا نویسه‌هایی مانند علائم نگارشی تقسیم می‌شود.

متنی که در اینجا برای آموزش مدل LLM توکنیزه می‌کنیم، داستان کوتاه **"The Verdict"** اثر **ادیت وارتون** است که به دلیل قرار گرفتن در مالکیت عمومی (public domain)، استفاده از آن در پروژه‌های آموزشی مجاز است. این متن در ویکی‌سورس در آدرس زیر موجود است:

[https://en.wikisource.org/wiki/The_Verdict](https://en.wikisource.org/wiki/The_Verdict)

می‌توانید این متن را کپی کرده و در یک فایل متنی به نام `the-verdict.txt` ذخیره کنید.

همچنین، این فایل در مخزن گیت‌هاب مربوط به کتاب نیز در دسترس است:

[https://mng.bz/Adng](https://mng.bz/Adng)

برای دانلود مستقیم این فایل با استفاده از پایتون، می‌توان از کد زیر استفاده کرد:

```
import urllib.request
url = ("https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt")
file_path = "the-verdict.txt"
urllib.request.urlretrieve(url, file_path)
```

سپس می‌توان این فایل را با استفاده از ابزارهای استاندارد خواندن فایل در پایتون بارگذاری کرد:

> **لیست ۲.۱: بارگذاری متن داستان به‌عنوان نمونه ورودی در پایتون**

```
with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()
print("Total number of character:", len(raw_text))
print(raw_text[:99])
```

خروجی این کد شامل تعداد کل نویسه‌ها و ۱۰۰ نویسه اول فایل است:

```
Total number of character: 20479
I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no
```

هدف ما، **توکنیزه‌کردن** این داستان ۲۰۴۷۹ نویسه‌ای به واژه‌ها و نویسه‌های خاص جداگانه است، تا بتوانیم آن‌ها را به embedding برای آموزش مدل LLM تبدیل کنیم.

> 🔍 **نکته:** در کاربردهای واقعی مدل‌های زبانی، معمولاً با میلیون‌ها مقاله و صدها هزار کتاب (حجمی در حد چندین گیگابایت متن) کار می‌شود. اما برای اهداف آموزشی، کار با نمونه‌های کوچک‌تر (مانند یک داستان کوتاه) کافی است و اجرای آن نیز روی سخت‌افزارهای خانگی امکان‌پذیر می‌باشد.

---

### استفاده از Regular Expression برای توکنیزه‌کردن

چگونه می‌توان این متن را به بهترین شکل به فهرستی از توکن‌ها تقسیم کرد؟ برای بررسی این موضوع، از کتابخانه `re` در پایتون (مخصوص کار با عبارات باقاعده) استفاده می‌کنیم. نیازی به حفظ دستور زبان عبارات باقاعده نیست، زیرا در ادامه از توکنیزرهای آماده استفاده خواهیم کرد.

در ابتدا، از متنی ساده برای آزمایش استفاده می‌کنیم و از تابع `re.split` برای تقسیم متن بر اساس فضاهای خالی استفاده می‌کنیم:

```
import re
text = "Hello, world. This, is a test."
result = re.split(r'(\s)', text)
print(result)
```

خروجی:

```
['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']
```

در این روش، متن به‌خوبی به واژه‌ها جدا می‌شود، اما برخی از واژه‌ها هنوز به علائم نگارشی متصل هستند. ما همچنین از تبدیل حروف به حالت lowercase صرف‌نظر می‌کنیم، زیرا **حساسیت به حروف بزرگ و کوچک** به مدل کمک می‌کند تا تفاوت میان اسم خاص و اسم عام را تشخیص دهد و ساختار جمله را بهتر درک کند.

برای بهبود، عبارت باقاعده را طوری تنظیم می‌کنیم که بر اساس فاصله، کاما و نقطه تقسیم کند:

```
result = re.split(r'([,.]|\s)', text)
print(result)
```

خروجی:

```
['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']
```

در این نسخه، واژه‌ها و علائم نگارشی جدا شده‌اند، اما همچنان کاراکترهای خالی (whitespace) باقی مانده‌اند. برای حذف آن‌ها، می‌توان از دستور زیر استفاده کرد:

```
result = [item for item in result if item.strip()]
print(result)
```

خروجی نهایی بدون فضاهای خالی:

```
['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']
```

> 🔍 **نکته:** اینکه آیا باید فاصله‌های خالی را به‌عنوان توکن حفظ کنیم یا خیر، بستگی به کاربرد دارد. حذف آن‌ها می‌تواند مصرف حافظه و زمان پردازش را کاهش دهد. با این حال، نگه‌داشتن فاصله‌ها برای متون حساس به ساختار (مانند کد پایتون) ضروری است. در این مثال، برای ساده‌سازی، فاصله‌ها حذف می‌شوند.

---

### پشتیبانی از علائم نگارشی بیشتر

توکنیزر ما تا اینجا خوب عمل کرده، اما بیایید آن را طوری به‌روز کنیم که علائم بیشتری مثل علامت سؤال، نقل‌قول و dashهای دوتایی (`--`) را نیز پشتیبانی کند:

```
text = "Hello, world. Is this-- a test?"
result = re.split(r'([,.:;?_!"()\']|--|\s)', text)
result = [item.strip() for item in result if item.strip()]
print(result)
```

خروجی:

```
['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']
```

> **شکل ۲.۵**: توکنیزر ما اکنون می‌تواند متن را به توکن‌هایی شامل واژه‌ها و نویسه‌های خاص تقسیم کند. در این نمونه، متن به ۱۰ توکن مجزا تقسیم شده است.

---

### اعمال توکنیزر به متن کامل داستان

اکنون، توکنیزر توسعه‌یافته را روی کل داستان ادیت وارتون اعمال می‌کنیم:

```
preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', raw_text)
preprocessed = [item.strip() for item in preprocessed if item.strip()]
print(len(preprocessed))
```

خروجی:

```
4690
```

یعنی متن ما به ۴۶۹۰ توکن تقسیم شده است (بدون درنظر گرفتن فاصله‌ها). حال ۳۰ توکن اول را برای بررسی چاپ می‌کنیم:

```
print(preprocessed[:30])
```

خروجی:

```
['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']
```

این خروجی نشان می‌دهد که توکنیزر به‌درستی واژه‌ها و علائم نگارشی را از هم جدا کرده است.

---

## ۲.۳ تبدیل توکن‌ها به شناسه‌های عددی (Token IDs)

در این مرحله، می‌خواهیم توکن‌هایی را که قبلاً تولید کرده‌ایم، از رشته‌های متنی پایتون به نمایش عددی تبدیل کنیم؛ این نمایش عددی همان **شناسه توکن‌ها** (Token IDs) است. این مرحله، یک گام میانی است پیش از آن‌که توکن‌ها به بردارهای جاسازی‌شده (embedding vectors) تبدیل شوند.

برای نگاشت (map) توکن‌های به‌دست‌آمده به شناسه‌های عددی، ابتدا باید **واژگان (vocabulary)** ایجاد کنیم. این واژگان مشخص می‌کند که چگونه هر واژه یا نماد خاص، به یک عدد یکتا اختصاص داده می‌شود. در شکل ۲.۶ این فرآیند به‌تصویر کشیده شده است.

> **شکل ۲.۶**: ما واژگان را با توکنیزه کردن کل متن یک مجموعه‌داده آموزشی ایجاد می‌کنیم. توکن‌ها به‌صورت الفبایی مرتب می‌شوند و نمونه‌های تکراری حذف می‌گردند. توکن‌های منحصربه‌فرد سپس در قالب یک واژگان گردآوری می‌شوند که در آن، هر توکن به یک عدد یکتا نگاشت می‌شود. (برای ساده‌سازی، در این واژگان نشانه‌گذاری‌ها یا نمادهای خاص لحاظ نشده‌اند.)

اکنون که داستان کوتاه "The Verdict" از ادیت وارتون را توکنیزه کرده و آن را در متغیری به نام `preprocessed` ذخیره کرده‌ایم، قصد داریم فهرستی از تمام توکن‌های منحصربه‌فرد ایجاد کنیم، آن‌ها را به ترتیب الفبایی مرتب کنیم و اندازه واژگان را تعیین کنیم:

```
all_words = sorted(set(preprocessed))
vocab_size = len(all_words)
print(vocab_size)
```

پس از اجرای این کد، درمی‌یابیم که اندازه واژگان برابر با ۱۱۳۰ است. حال، واژگان را می‌سازیم و برای نمونه، ۵۱ مورد اول آن را چاپ می‌کنیم.

> **لیست ۲.۲: ساخت واژگان**

```
vocab = {token:integer for integer,token in enumerate(all_words)}
for i, item in enumerate(vocab.items()):
    print(item)
    if i >= 50:
        break
('!', 0)
('"', 1)
("'", 2)
...
('Her', 49)
('Hermia', 50)
```

همان‌طور که می‌بینیم، این واژگان به‌صورت یک دیکشنری پیاده‌سازی شده که در آن، هر توکن با یک شناسه عددی یکتا مرتبط شده است. گام بعدی ما، استفاده از این واژگان برای تبدیل متن جدید به شناسه‌های توکن است (شکل ۲.۷).

> **شکل ۲.۷**: با استفاده از یک نمونه متن جدید، ابتدا آن را توکنیزه می‌کنیم و سپس با استفاده از واژگان، توکن‌ها را به شناسه‌های عددی تبدیل می‌کنیم. واژگان از کل مجموعه آموزشی ساخته شده و می‌تواند هم برای داده‌های آموزشی و هم برای متون جدید به‌کار رود. (در این واژگان نیز نشانه‌گذاری لحاظ نشده است.)

برای بازگرداندن خروجی مدل‌های زبانی بزرگ (LLM) از شکل عددی به متن، نیاز داریم شناسه‌های توکن را دوباره به توکن‌های متنی تبدیل کنیم. برای این کار، نسخه معکوس واژگان را ایجاد می‌کنیم که در آن، هر شناسه عددی به توکن متناظر خود نگاشت می‌شود.

اکنون قصد داریم یک **کلاس ساده برای توکنیزه‌سازی متن** در پایتون پیاده‌سازی کنیم. این کلاس شامل یک متد `encode` خواهد بود که متن را به توکن‌ها تقسیم کرده و آن‌ها را به شناسه عددی تبدیل می‌کند، و همچنین متد `decode` که فرآیند معکوس را انجام داده و شناسه‌های عددی را به متن بازمی‌گرداند.

> **لیست ۲.۳: پیاده‌سازی یک توکنیزر ساده در پایتون**

```
class SimpleTokenizerV1:
    def __init__(self, vocab):
        self.str_to_int = vocab
        self.int_to_str = {i: s for s, i in vocab.items()}

    def encode(self, text):
        preprocessed = re.split(r'([,.?_!"()\']|--|\s)', text)
        preprocessed = [
            item.strip() for item in preprocessed if item.strip()
        ]
        ids = [self.str_to_int[s] for s in preprocessed]
        return ids

    def decode(self, ids):
        text = " ".join([self.int_to_str[i] for i in ids])
        text = re.sub(r'\s+([,.?!"()\'])', r'\1', text)
        return text
```

با استفاده از کلاس `SimpleTokenizerV1`، اکنون می‌توانیم یک شی جدید از این کلاس بسازیم و با کمک واژگان موجود، متن را توکنیزه و سپس بازسازی کنیم. بیایید این کار را با یک قطعه از داستان ادیت وارتون انجام دهیم:

```
tokenizer = SimpleTokenizerV1(vocab)
text = """"It's the last he painted, you know,"
Mrs. Gisburn said with pardonable pride."""
ids = tokenizer.encode(text)
print(ids)
```

خروجی کد بالا چنین خواهد بود:

```
[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108,754, 793, 7]
```

در گام بعد، بررسی می‌کنیم که آیا می‌توانیم این شناسه‌ها را دوباره به متن تبدیل کنیم:

```
print(tokenizer.decode(ids))
```

> **شکل ۲.۸**: توکنیزرها معمولاً دو متد اصلی دارند: `encode` برای تبدیل متن به شناسه توکن، و `decode` برای تبدیل شناسه توکن به متن. ابتدا متن به توکن‌های جداگانه تقسیم می‌شود، سپس به شناسه تبدیل می‌گردد. در مرحله معکوس، شناسه‌ها به توکن تبدیل شده و به‌صورت رشته‌ای قابل خواندن در می‌آیند.

خروجی:

```
'" It\' s the last he painted, you know," Mrs. Gisburn said with pardonable pride.'
```

این خروجی نشان می‌دهد که متد `decode` با موفقیت توانسته شناسه‌های عددی را به متن اصلی بازگرداند.

تا اینجا، ما یک توکنیزر طراحی کرده‌ایم که می‌تواند متنی از مجموعه آموزشی را توکنیزه و سپس آن را بازسازی کند. حال بیایید آن را روی یک نمونه متن جدید امتحان کنیم:

```
text = "Hello, do you like tea?"
print(tokenizer.encode(text))
```

اما اجرای این کد منجر به خطای زیر خواهد شد:

```
KeyError: 'Hello'
```

دلیل خطا آن است که واژه "Hello" در داستان "The Verdict" وجود نداشته و در نتیجه در واژگان نیز نیامده است. این موضوع نشان می‌دهد که برای کار با مدل‌های زبانی بزرگ (LLM)، ضروری است که مجموعه‌داده آموزشی **گسترده و متنوع** باشد تا واژگان حاصل بتواند کلمات جدید را نیز پوشش دهد.

در ادامه، توکنیزر را با متونی آزمایش می‌کنیم که حاوی واژگان ناشناخته هستند و سپس به بررسی **توکن‌های خاص** (special tokens) می‌پردازیم که در آموزش مدل‌های زبانی به آن‌ها زمینه و ساختار بیشتری می‌دهند.

---

## ۲.۴ افزودن توکن‌های خاص زمینه‌ای

در این بخش، قصد داریم توکنیزر را به‌گونه‌ای اصلاح کنیم که بتواند واژه‌های ناشناخته را مدیریت کرده و همچنین توکن‌هایی خاص را برای فراهم‌کردن **زمینه (context)** بهتر به مدل زبانی اضافه کند. این توکن‌های خاص می‌توانند شامل نشانگرهایی برای واژه‌های ناشناخته و **مرز بین اسناد** مختلف باشند.

به‌طور مشخص، قرار است واژگان و توکنیزر را به نسخه‌ای جدید به نام `SimpleTokenizerV2` ارتقا دهیم تا از دو توکن جدید پشتیبانی کند:

- توکن `<|unk|>`: نماینده‌ی واژه‌های ناشناخته‌ای است که در داده‌ی آموزشی مدل وجود نداشته‌اند.
- توکن `<|endoftext|>`: نشان‌دهنده‌ی پایان یک متن یا سند مستقل است.

> **شکل ۲.۹**: با افزودن توکن‌های خاص به واژگان، می‌توان شرایط خاصی را مدیریت کرد. برای مثال، `<|unk|>` نماینده‌ی واژه‌هایی است که در داده‌ی آموزشی مدل وجود نداشته‌اند. همچنین توکن `<|endoftext|>` می‌تواند برای جدا کردن دو متن مستقل استفاده شود.

هنگامی که یک توکنیزر با واژه‌ای مواجه می‌شود که در واژگان (vocabulary) وجود ندارد، می‌توان آن را با توکن `<|unk|>` جایگزین کرد. همچنین، در زمانی که از چند سند مستقل برای آموزش استفاده می‌شود، رایج است که بین هر دو متن، توکن `<|endoftext|>` قرار داده شود.

> **شکل ۲.۱۰**: در مواقعی که از چند منبع متنی مستقل استفاده می‌شود، افزودن توکن `<|endoftext|>` بین آن‌ها باعث می‌شود مدل LLM بتواند تشخیص دهد که این متون به هم مرتبط نیستند. این کار به مدل کمک می‌کند تا ساختار متن را بهتر درک کند.

---

### افزودن توکن‌های خاص به واژگان

برای اضافه کردن این توکن‌ها به واژگان موجود، ابتدا فهرستی از همه‌ی توکن‌های منحصربه‌فرد موجود در متن تهیه می‌کنیم و سپس این دو توکن خاص را به آن اضافه می‌نماییم:

```
all_tokens = sorted(list(set(preprocessed)))
all_tokens.extend(["<|endoftext|>", "<|unk|>"])
vocab = {token: integer for integer, token in enumerate(all_tokens)}
print(len(vocab.items()))
```

خروجی نشان می‌دهد که اندازه واژگان جدید **۱۱۳۲** است (در حالی که پیش از این ۱۱۳۰ بود).

برای بررسی، پنج ورودی آخر واژگان را چاپ می‌کنیم:

```
for i, item in enumerate(list(vocab.items())[-5:]):
    print(item)
```

خروجی:

```
('younger', 1127)
('your', 1128)
('yourself', 1129)
('<|endoftext|>', 1130)
('<|unk|>', 1131)
```

این خروجی تأیید می‌کند که دو توکن ویژه با موفقیت به واژگان اضافه شده‌اند.

---

### پیاده‌سازی توکنیزر با پشتیبانی از واژه‌های ناشناخته

در نسخه جدید توکنیزر، یعنی `SimpleTokenizerV2`، واژه‌هایی که در واژگان نیستند با توکن `<|unk|>` جایگزین می‌شوند. پیاده‌سازی آن به صورت زیر است:

> **لیست ۲.۴: توکنیزر ساده که واژه‌های ناشناخته را مدیریت می‌کند**

```
class SimpleTokenizerV2:
    def __init__(self, vocab):
        self.str_to_int = vocab
        self.int_to_str = {i: s for s, i in vocab.items()}

    def encode(self, text):
        preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', text)
        preprocessed = [
            item.strip() for item in preprocessed if item.strip()
        ]
        preprocessed = [
            item if item in self.str_to_int else "<|unk|>" for item in preprocessed
        ]
        ids = [self.str_to_int[s] for s in preprocessed]
        return ids

    def decode(self, ids):
        text = " ".join([self.int_to_str[i] for i in ids])
        text = re.sub(r'\s+([,.:;?!"()\'])', r'\1', text)
        return text
```

تفاوت این نسخه با نسخه‌ی قبلی (`SimpleTokenizerV1`) در این است که اکنون **واژه‌های خارج از واژگان** با `<|unk|>` جایگزین می‌شوند.

---

### آزمایش توکنیزر جدید در عمل

در ادامه، دو جمله‌ی مستقل را با توکن `<|endoftext|>` از هم جدا کرده و به توکنیزر می‌دهیم:

```
text1 = "Hello, do you like tea?"
text2 = "In the sunlit terraces of the palace."
text = " <|endoftext|> ".join((text1, text2))
print(text)
```

خروجی:

```
Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.
```

اکنون متن ترکیبی را با `SimpleTokenizerV2` توکنیزه می‌کنیم:

```
tokenizer = SimpleTokenizerV2(vocab)
print(tokenizer.encode(text))
```

خروجی توکن آیدی‌ها:

```
[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]
```

در این خروجی:

- **عدد 1130** مربوط به توکن `<|endoftext|>` است.
- **عدد 1131** دو بار تکرار شده و نشان‌دهنده‌ی واژه‌های ناشناخته (یعنی “Hello” و “palace”) است.

اکنون متن توکنیزه‌شده را به متن قابل خواندن تبدیل می‌کنیم:

```
print(tokenizer.decode(tokenizer.encode(text)))
```

خروجی:

```
<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.
```

همان‌طور که دیده می‌شود، دو واژه‌ی "Hello" و "palace" چون در داستان اصلی وجود نداشتند، به توکن `<|unk|>` تبدیل شده‌اند.

---

### توکن‌های خاص بیشتر (اختیاری)

بسته به نوع مدل زبانی، گاهی از توکن‌های زمینه‌ای بیشتری نیز استفاده می‌شود:

- توکن `[BOS]` (شروع توالی): نشان‌دهنده‌ی آغاز یک متن است.
- توکن `[EOS]` (پایان توالی): نشان‌دهنده‌ی پایان یک متن است و برای تفکیک متون مستقل به‌ویژه در آموزش مدل‌های چندمتنی استفاده می‌شود. عملکردی مشابه `<|endoftext|>` دارد.
- توکن `[PAD]` (پدینگ): هنگام آموزش با **بچ‌های چندمتنی با طول متفاوت**، برای یکسان‌سازی طول، متون کوتاه‌تر با این توکن تا طول بلندترین متن پُر می‌شوند.

با این حال، توکنیزر مدل‌های GPT از هیچ‌کدام از این توکن‌ها استفاده نمی‌کند. در GPT تنها از توکن `<|endoftext|>` استفاده می‌شود که عملکردی مشابه `[EOS]` و حتی `[PAD]` دارد. هنگام استفاده از batching، توکن‌های padding با **ماسک کردن (masking)** نادیده گرفته می‌شوند، بنابراین نوع دقیق توکن padding چندان اهمیت ندارد.

همچنین، در GPT برخلاف SimpleTokenizer، از توکن `<|unk|>` نیز استفاده نمی‌شود. به‌جای آن، GPT از **توکنیزر مبتنی بر رمزگذاری زوج‌نویسه‌ها (Byte Pair Encoding - BPE)** استفاده می‌کند که واژه‌ها را به واحدهای کوچک‌تر (زیرواژه‌ها یا subword units) تقسیم می‌کند. این موضوع در فصل‌های بعدی بررسی خواهد شد.

---


## ۲.۵ رمزگذاری زوج‌نویسه‌ها (Byte Pair Encoding)

در این بخش، با یک روش پیشرفته‌تر برای توکنیزه‌کردن متن آشنا می‌شویم که بر پایه الگویی به نام **رمزگذاری زوج‌نویسه‌ها (BPE)** طراحی شده است. این روش برای آموزش مدل‌های زبانی بزرگی مانند **GPT-2، GPT-3** و نسخه اصلی **ChatGPT** مورد استفاده قرار گرفته است.

از آن‌جا که پیاده‌سازی BPE از ابتدا پیچیدگی نسبتاً بالایی دارد، در اینجا از یک **کتابخانه‌ی متن‌باز پایتون به نام `tiktoken`** استفاده می‌کنیم که این الگوریتم را به شکلی بسیار بهینه (با استفاده از زبان Rust) پیاده‌سازی کرده است.

---

### نصب کتابخانه `tiktoken`

برای نصب این کتابخانه، از دستور زیر در ترمینال استفاده می‌کنیم:

```
pip install tiktoken
```

برای بررسی نسخه‌ی نصب‌شده:

```
from importlib.metadata import version
import tiktoken
print("tiktoken version:", version("tiktoken"))
```

---

### استفاده از توکنیزر BPE

پس از نصب، می‌توانیم توکنیزر مبتنی بر BPE را مانند زیر مقداردهی کنیم:

```
tokenizer = tiktoken.get_encoding("gpt2")
```

ساختار استفاده از این توکنیزر بسیار شبیه به `SimpleTokenizerV2` است. مثلاً برای تبدیل متن به توکن‌ها از متد `encode` استفاده می‌کنیم:

```
text = (
    "Hello, do you like tea? <|endoftext|> In the sunlit terraces"
    "of someunknownPlace."
)
integers = tokenizer.encode(text, allowed_special={"<|endoftext|>"})
print(integers)
```

خروجی این کد به شکل زیر است:

```
[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250,
8812, 2114, 286, 617, 34680, 27271, 13]
```

---

### تبدیل دوباره به متن

برای تبدیل توکن‌ها به متن قابل خواندن، از متد `decode` استفاده می‌کنیم:

```
strings = tokenizer.decode(integers)
print(strings)
```

خروجی:

```
Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.
```

---

### دو نکته مهم درباره‌ی توکن‌ها

۱. **شناسه‌ی توکن <|endoftext|>** عدد 50256 است. این توکن معمولاً بزرگ‌ترین شناسه را دارد و نشان می‌دهد که **اندازه‌ی واژگان توکنیزر BPE در GPT-2 و GPT-3 برابر با 50,257 توکن** است.

۲. **مدیریت واژه‌های ناشناخته (مثلاً someunknownPlace)** به‌درستی انجام می‌شود. برخلاف توکنیزرهای قبلی که از `<|unk|>` برای واژه‌های ناشناخته استفاده می‌کردند، توکنیزر BPE **بدون نیاز به هیچ توکن ویژه‌ای، واژه‌های ناشناخته را به اجزای کوچک‌تر (subword یا حتی کاراکتر) تجزیه می‌کند**.

> **شکل ۲.۱۱** این فرآیند را نشان می‌دهد که چگونه واژه‌های ناشناخته به زیرواحدهایی تجزیه می‌شوند و به این ترتیب، توکنیزر می‌تواند با هر واژه‌ای (حتی واژه‌های دیده‌نشده) کار کند.

---

### چگونه BPE واژه‌های ناشناخته را مدیریت می‌کند؟

الگوریتم BPE، واژگان از پیش تعریف‌شده‌اش را با استفاده از ترکیب‌های پرتکرار کاراکترها و زیرواژه‌ها می‌سازد. بنابراین، هنگام مواجهه با واژه‌ای که قبلاً در واژگان نبوده، می‌تواند آن را به توکن‌های کوچک‌تر شکسته و پردازش کند.

برای مثال:

- BPE ابتدا همه‌ی حروف الفبا را به عنوان توکن‌های اولیه در واژگان قرار می‌دهد (مانند "a"، "b"، ...).
- سپس، ترکیب‌هایی از کاراکترهایی که **بیشتر با هم تکرار می‌شوند** را به زیرواژه‌های جدید تبدیل می‌کند.

مثال: ترکیب "d" و "e" ممکن است به "de" تبدیل شود، چون در واژگانی مثل "define"، "depend"، "made" و "hidden" پرتکرار است.

الگوریتم این ترکیب‌ها را بر اساس **آستانه‌ی فراوانی (frequency cutoff)** انتخاب می‌کند.

---

### تمرین ۲.۱: رمزگذاری BPE واژه‌های ناشناخته

> واژه‌های "Akwirw ier" را با توکنیزر BPE از کتابخانه `tiktoken` رمزگذاری کنید و آیدی‌های توکن‌ها را چاپ کنید. سپس، برای هر آیدی توکن، متد `decode` را فراخوانی کنید تا نگاشت دقیق آن‌ها را به متن ببینید. در نهایت، بررسی کنید که آیا با متد `decode` می‌توان متن اصلی "Akwirw ier" را بازسازی کرد یا خیر.

این تمرین به شما کمک می‌کند درک عمیق‌تری از نحوه‌ی شکستن واژه‌های ناشناخته در BPE به دست آورید.

---


## ۲.۶ نمونه‌برداری داده با پنجره‌ی لغزان (Sliding Window)

گام بعدی در ساخت تعبیه‌ها (embeddings) برای مدل زبانی بزرگ (LLM)، ایجاد **جفت‌های ورودی-هدف** است که برای آموزش مدل لازم است. این جفت‌ها چگونه ساخته می‌شوند؟ همانطور که پیش‌تر آموختیم، LLMها به شکل **پیش‌بینی کلمه‌ی بعدی** آموزش داده می‌شوند (شکل ۲.۱۲).

---

### نمونه‌گیری جفت‌های ورودی-هدف با پنجره لغزان

یک روش ساده و شهودی برای ساخت این جفت‌ها، استفاده از دو متغیر ( x ) و ( y ) است که:

- متغیر ( x ) شامل توکن‌های ورودی است،
- متغیر ( y ) شامل همان توکن‌ها اما به اندازه یک موقعیت به جلو شیفت داده شده است (یعنی کلمات بعدی که باید پیش‌بینی شوند).

---

### مثال عملی با BPE توکنیزر و متن

فرض کنید کل متن داستان کوتاه «The Verdict» را با توکنیزر BPE رمزگذاری کرده‌ایم:

```python
with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()
enc_text = tokenizer.encode(raw_text)
print(len(enc_text))  # خروجی: 5145 (تعداد توکن‌ها)
```

برای ساده‌سازی، ۵۰ توکن اول را حذف می‌کنیم:

```python
enc_sample = enc_text[50:]
```

فرض کنید اندازه پنجره (context size) برابر ۴ است:

```python
context_size = 4
x = enc_sample[:context_size]          # توکن‌های ورودی
y = enc_sample[1:context_size+1]       # توکن‌های هدف (شیفت یافته)
print(f"x: {x}")
print(f"y: {y}")
```

خروجی:

```
x: [290, 4920, 2241, 287]
y: [4920, 2241, 287, 257]
```

---

### جفت‌های ورودی و هدف گام به گام

با افزایش طول ورودی به ترتیب ۱ تا ( context_size )، جفت‌های زیر ساخته می‌شود:

```python
for i in range(1, context_size + 1):
    context = enc_sample[:i]
    desired = enc_sample[i]
    print(context, "---->", desired)
```

خروجی:

```
[290] ----> 4920
[290, 4920] ----> 2241
[290, 4920, 2241] ----> 287
[290, 4920, 2241, 287] ----> 257
```

برای دیدن متن به جای آیدی‌ها:

```python
for i in range(1, context_size + 1):
    context = enc_sample[:i]
    desired = enc_sample[i]
    print(tokenizer.decode(context), "---->", tokenizer.decode([desired]))
```

خروجی:

```
and ----> established
and established ----> himself
and established himself ----> in
and established himself in ----> a
```

---

### ساخت داده‌های آموزشی به صورت دسته‌ای (Batch)

برای آموزش بهینه مدل، نیاز به ساخت **داده‌های دسته‌ای** داریم. از PyTorch و کلاس‌های Dataset و DataLoader استفاده می‌کنیم تا داده‌ها را به صورت ورودی و هدف آماده کنیم.

---

### کلاس Dataset: `GPTDatasetV1`

```python
import torch
from torch.utils.data import Dataset

class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []
        token_ids = tokenizer.encode(txt)

        # ساخت داده با پنجره لغزان
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]
```

---

### ساخت DataLoader

```python
from torch.utils.data import DataLoader
import tiktoken

def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128,
                         shuffle=True, drop_last=True, num_workers=0):
    tokenizer = tiktoken.get_encoding("gpt2")
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        drop_last=drop_last,
        num_workers=num_workers
    )
    return dataloader
```

---

### تست با batch size=1 و context_size=4

```python
with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()

dataloader = create_dataloader_v1(
    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)

data_iter = iter(dataloader)
first_batch = next(data_iter)
print(first_batch)
```

خروجی (توکن‌های ورودی و هدف):

```
[tensor([[ 40, 367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]
```

- توجه کنید که هر دسته شامل توکن‌های ورودی و همان توکن‌ها ولی شیفت یافته برای هدف است.

---

### حرکت پنجره با stride=1 (لغزش به اندازه ۱)

```python
second_batch = next(data_iter)
print(second_batch)
```

خروجی:

```
[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]
```

با مقایسه دسته اول و دوم می‌بینیم که دسته دوم به اندازه یک توکن نسبت به دسته اول به جلو لغزیده است. این همان ایده پنجره لغزان است (شکل ۲.۱۴).

---

### دسته‌بندی با batch size بزرگ‌تر و stride بزرگ‌تر

مثلاً:

```python
dataloader = create_dataloader_v1(
    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)

data_iter = iter(dataloader)
inputs, targets = next(data_iter)

print("Inputs:\n", inputs)
print("\nTargets:\n", targets)
```

خروجی (نمایش بخشی از ورودی و هدف):

```
Inputs:
tensor([[ 40, 367, 2885, 1464],
        [1807, 3619, 402, 271],
        ... ])

Targets:
tensor([[ 367, 2885, 1464, 1807],
        [3619, 402, 271, 10899],
        ... ])
```

وقتی stride برابر با اندازه پنجره باشد، دسته‌ها روی هم نمی‌افتند و از همه داده‌ها بدون تکرار استفاده می‌شود.

---

### نکات تکمیلی:

- متغیر‌های **batch size** و **stride** از هایپرپارامترهای مهم هستند که روی کیفیت و کارایی آموزش تاثیر می‌گذارند.
- انتخاب stride کوچکتر از max_length باعث همپوشانی دسته‌ها و مدل را در معرض توکن‌های مشابه قرار می‌دهد (ممکن است باعث overfitting شود).
- انتخاب stride مساوی max_length باعث عدم همپوشانی دسته‌ها می‌شود.

---

### تمرین ۲.۲

برای درک بهتر، دیتالودر را با تنظیمات مختلف امتحان کنید، مثل:

- `max_length=2, stride=2`
- `max_length=8, stride=2`

و تاثیر آن روی نمونه‌های ورودی و هدف را بررسی کنید.

---

## ۲.۷ ایجاد بردارهای جاسازی (توکن ایمبدینگ‌ها)

آخرین مرحله در آماده‌سازی متن ورودی برای آموزش مدل‌های زبان بزرگ (LLM) تبدیل شناسه‌های توکن به بردارهای جاسازی یا **embedding vectors** است، همان‌طور که در شکل ۲.۱۵ نشان داده شده است. در ابتدا باید وزن‌های این بردارهای جاسازی را با مقادیر تصادفی مقداردهی اولیه کنیم. این مقداردهی اولیه نقطه شروع فرایند یادگیری مدل زبان بزرگ است. در فصل ۵، وزن‌های این بردارهای جاسازی در طول آموزش مدل بهینه خواهند شد.

شکل ۲.۱۵ نشان می‌دهد که آماده‌سازی شامل سه مرحله اصلی است: توکن‌سازی متن، تبدیل توکن‌ها به شناسه‌های توکن و در نهایت تبدیل شناسه‌های توکن به بردارهای جاسازی. در اینجا فرض می‌کنیم شناسه‌های توکن از قبل ایجاد شده‌اند و حال می‌خواهیم بردارهای جاسازی متناظر با آن‌ها را بسازیم.

یک نمایش برداری پیوسته یا همان جاسازی، ضروری است زیرا مدل‌های مشابه GPT شبکه‌های عصبی عمیقی هستند که با الگوریتم پس‌انتشار خطا (backpropagation) آموزش داده می‌شوند.
**توجه:** اگر با نحوه آموزش شبکه‌های عصبی با الگوریتم پس‌انتشار آشنایی ندارید، بخش B.4 در ضمیمه A را مطالعه کنید.

حالا با یک مثال عملی نحوه تبدیل شناسه توکن به بردار جاسازی را ببینیم. فرض کنیم چهار توکن ورودی با شناسه‌های ۲، ۳، ۵ و ۱ داریم:

```python
input_ids = torch.tensor([2, 3, 5, 1])
```

برای سادگی فرض کنیم دیکشنری ما فقط ۶ کلمه دارد (در مقابل ۵۰,۲۵۷ کلمه در دیکشنری BPE توکنایزر واقعی) و می‌خواهیم بردارهای جاسازی سه‌بعدی بسازیم (در GPT-3 اندازه بردار جاسازی ۱۲۲۸۸ بعد است):

```python
vocab_size = 6
output_dim = 3
```

با استفاده از اندازه دیکشنری و بعد خروجی، لایه جاسازی در PyTorch را ایجاد می‌کنیم و برای تکرارپذیری، مقدار seed را روی ۱۲۳ تنظیم می‌کنیم:

```python
torch.manual_seed(123)
embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
print(embedding_layer.weight)
```

خروجی این دستور، ماتریس وزن‌های زیرساختی لایه جاسازی را نشان می‌دهد:

```
Parameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
        [ 0.9178, 1.5810, 1.3010],
        [ 1.2753, -0.2010, -0.1606],
        [-0.4015, 0.9666, -1.1481],
        [-1.1589, 0.3255, -0.6315],
        [-2.8400, -0.7849, -1.4096]], requires_grad=True)
```

ماتریس وزن لایه جاسازی شامل مقادیر کوچک و تصادفی است که در طول آموزش مدل بهینه می‌شوند. این ماتریس ۶ سطر و ۳ ستون دارد؛ هر سطر متناظر با یکی از ۶ توکن موجود در دیکشنری و هر ستون متناظر با یکی از ۳ بعد بردار جاسازی است.

حالا بیایید این لایه را برای یک شناسه توکن به کار ببریم تا بردار جاسازی آن را به دست آوریم:

```python
print(embedding_layer(torch.tensor([3])))
```

بردار جاسازی بازگردانده شده به صورت زیر است:

```
tensor([[-0.4015, 0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)
```

اگر این بردار را با ردیف چهارم ماتریس وزن مقایسه کنیم (چون اندیس‌ها از صفر شروع می‌شوند، ردیف با اندیس ۳)، مشاهده می‌کنیم که دقیقاً همان است. به عبارت دیگر، لایه جاسازی در واقع یک عملیات جستجو (lookup) است که ردیف‌های ماتریس وزن را بر اساس شناسه توکن استخراج می‌کند.

**توجه:** برای کسانی که با رمزگذاری one-hot آشنا هستند، روش لایه جاسازی اینجا صرفاً یک پیاده‌سازی بهینه‌تر از رمزگذاری one-hot به همراه ضرب ماتریسی در لایه کاملاً متصل است. این موضوع در کدهای مکمل موجود در گیت‌هاب در آدرس [https://mng.bz/ZEB5](https://mng.bz/ZEB5) توضیح داده شده است. به همین دلیل لایه جاسازی به عنوان یک لایه شبکه عصبی محسوب می‌شود که می‌توان آن را با پس‌انتشار بهینه کرد.

تا اینجا دیدیم چگونه یک شناسه توکن را به بردار جاسازی سه‌بعدی تبدیل کنیم. حالا این کار را برای هر چهار شناسه ورودی اعمال می‌کنیم:

```python
print(embedding_layer(input_ids))
```

خروجی یک ماتریس ۴ در ۳ خواهد بود:

```
tensor([[ 1.2753, -0.2010, -0.1606],
        [-0.4015, 0.9666, -1.1481],
        [-2.8400, -0.7849, -1.4096],
        [ 0.9178, 1.5810, 1.3010]], grad_fn=<EmbeddingBackward0>)
```

هر ردیف این ماتریس، از طریق عملیات جستجو از ماتریس وزن لایه جاسازی استخراج شده است، همانطور که در شکل ۲.۱۶ نشان داده شده است.

اکنون که بردارهای جاسازی متناظر با شناسه‌های توکن را ایجاد کرده‌ایم، در ادامه کمی این بردارها را تغییر می‌دهیم تا اطلاعات موقعیتی هر توکن در متن را نیز در آنها بگنجانیم.

## ۲.۸ رمزگذاری موقعیت کلمات

در اصل، بردارهای جاسازی توکن‌ها (token embeddings) ورودی مناسبی برای مدل‌های زبان بزرگ (LLM) هستند. با این حال، یک نکته کوچک وجود دارد: مکانیزم توجه خودی (self-attention) در این مدل‌ها، که در فصل ۳ توضیح داده شده، موقعیت یا ترتیب توکن‌ها را درون توالی در نظر نمی‌گیرد. به همین دلیل، همان‌طور که در شکل ۲.۱۷ مشاهده می‌کنید، لایه جاسازی، هر شناسه توکن (token ID) را به یک بردار ثابت تبدیل می‌کند، بدون توجه به جایگاه آن توکن در توالی ورودی.

در شکل ۲.۱۶ نیز دیده می‌شود که لایه جاسازی یک عملیات جستجو است که بردار جاسازی متناظر با شناسه توکن را از ماتریس وزن لایه بازیابی می‌کند. برای مثال، بردار جاسازی شناسه توکن ۵، ردیف ششم ماتریس وزن است (چون اندیس‌شماری از صفر شروع می‌شود).

بردارهای جاسازی مستقل از موقعیت، از نظر قابلیت تکرار (reproducibility) مفید هستند. اما از آنجا که مکانیزم توجه خودی نیز به موقعیت حساس نیست، اضافه کردن اطلاعات موقعیتی به مدل بسیار کمک‌کننده است.

برای این منظور دو نوع اصلی جاسازی موقعیت وجود دارد:

- **جاسازی موقعیت مطلق (Absolute Positional Embeddings):** که به هر موقعیت مشخص در توالی یک بردار موقعیتی یکتا اختصاص می‌دهد و این بردار به بردار جاسازی توکن افزوده می‌شود. مثلا توکن اول یک بردار موقعیت مخصوص خود را دارد، توکن دوم بردار دیگری و ...، همان‌طور که در شکل ۲.۱۸ نشان داده شده است.
- **جاسازی موقعیت نسبی (Relative Positional Embeddings):** در این نوع، تمرکز روی فاصله یا موقعیت نسبی بین توکن‌هاست، نه موقعیت مطلق آن‌ها. این کار باعث می‌شود مدل بهتر بتواند توالی‌هایی با طول‌های متفاوت را تعمیم دهد حتی اگر در طول آموزش چنین توالی‌هایی را ندیده باشد.

هدف هر دو نوع جاسازی موقعیت، افزایش توانایی مدل در درک ترتیب و روابط بین توکن‌ها است تا پیش‌بینی‌های دقیق‌تر و متنی‌تر ارائه شود. انتخاب بین این دو معمولاً به کاربرد و نوع داده‌ها بستگی دارد.

مدل‌های GPT شرکت OpenAI از جاسازی موقعیت مطلق استفاده می‌کنند که در طول آموزش مدل بهینه می‌شود و ثابت یا از پیش تعریف‌شده نیست، برخلاف رمزگذاری موقعیت در مدل ترنسفورمر اصلی.

---

حالا می‌خواهیم جاسازی موقعیت اولیه را برای آماده‌سازی ورودی‌های LLM بسازیم.

قبلاً برای سادگی، اندازه بردارهای جاسازی را خیلی کوچک در نظر گرفتیم. حالا اندازه‌های واقعی‌تر و کاربردی‌تر را مد نظر قرار می‌دهیم و توکن‌ها را به بردارهای ۲۵۶ بعدی تبدیل می‌کنیم. این اندازه کمتر از مقدار استفاده شده در GPT-3 (۱۲۲۸۸ بعد) است، اما برای آزمایش مناسب است. همچنین فرض می‌کنیم شناسه‌های توکن از توکنایزر BPE با دیکشنری ۵۰,۲۵۷ کلمه‌ای که قبلاً پیاده‌سازی کردیم، به دست آمده‌اند:

```python
vocab_size = 50257
output_dim = 256
token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
```

با استفاده از این لایه جاسازی، اگر داده‌ای را از داده‌خوان (data loader) نمونه‌برداری کنیم، هر توکن در هر دسته (batch) به بردار ۲۵۶ بعدی تبدیل می‌شود. اگر اندازه دسته ۸ و تعداد توکن‌ها در هر نمونه ۴ باشد، خروجی یک تنسور ۸ × ۴ × ۲۵۶ خواهد بود.

ابتدا داده‌خوان را ایجاد می‌کنیم (بخش ۲.۶):

```python
max_length = 4
dataloader = create_dataloader_v1(
    raw_text, batch_size=8, max_length=max_length,
    stride=max_length, shuffle=False
)
data_iter = iter(dataloader)
inputs, targets = next(data_iter)
print("Token IDs:\n", inputs)
print("\nInputs shape:\n", inputs.shape)
```

خروجی به صورت زیر خواهد بود:

```
Token IDs:
tensor([[ 40, 367, 2885, 1464],
        [1807, 3619, 402, 271],
        [10899, 2138, 257, 7026],
        [15632, 438, 2016, 257],
        [922, 5891, 1576, 438],
        [568, 340, 373, 645],
        [1049, 5975, 284, 502],
        [284, 3285, 326, 11]])
Inputs shape:
torch.Size([8, 4])
```

همانطور که می‌بینیم، تنسور شناسه‌های توکن ۸ × ۴ است، یعنی هر دسته شامل هشت نمونه متنی با چهار توکن هر کدام است.

حالا شناسه‌های توکن را با لایه جاسازی به بردارهای ۲۵۶ بعدی تبدیل می‌کنیم:

```python
token_embeddings = token_embedding_layer(inputs)
print(token_embeddings.shape)
```

خروجی:

```
torch.Size([8, 4, 256])
```

این تنسور نشان می‌دهد که هر شناسه توکن به بردار ۲۵۶ بعدی تبدیل شده است.

---

برای روش جاسازی موقعیت مطلق در GPT، باید یک لایه جاسازی موقعیت دیگر با همان ابعاد جاسازی توکن‌ها ایجاد کنیم:

```python
context_length = max_length
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)
pos_embeddings = pos_embedding_layer(torch.arange(context_length))
print(pos_embeddings.shape)
```

ورودی این لایه معمولاً یک بردار ترتیب‌شماری است:

```python
torch.arange(context_length)
```

که شامل اعداد ۰، ۱، ... تا طول بیشینه ورودی منهای یک است. متغیر context_length نمایانگر بیشینه طول ورودی پشتیبانی شده توسط LLM است. در اینجا آن را برابر با بیشینه طول ورودی متن قرار داده‌ایم. در عمل، اگر متن ورودی طولانی‌تر باشد، باید آن را کوتاه کنیم (truncate).

خروجی:

```
torch.Size([4, 256])
```

پس بردارهای جاسازی موقعیت، شامل چهار بردار ۲۵۶ بعدی هستند.

حالا این بردارهای موقعیتی را مستقیماً به بردارهای جاسازی توکن اضافه می‌کنیم. در PyTorch، این عمل برای هر ۸ دسته به صورت همزمان انجام می‌شود:

```python
input_embeddings = token_embeddings + pos_embeddings
print(input_embeddings.shape)
```

خروجی:

```
torch.Size([8, 4, 256])
```

بردارهای **input_embeddings** که به صورت خلاصه در شکل ۲.۱۹ نمایش داده شده‌اند، ورودی‌های جاسازی شده‌ای هستند که اکنون آماده پردازش توسط ماژول‌های اصلی مدل زبان بزرگ هستند، که در فصل بعد پیاده‌سازی خواهیم کرد.

---

**خلاصه فرآیند در شکل ۲.۱۹:**
متن ورودی ابتدا به توکن‌های جداگانه تقسیم می‌شود، سپس این توکن‌ها به شناسه‌های عددی تبدیل می‌شوند، شناسه‌ها به بردارهای جاسازی شده (embedding vectors) تبدیل می‌شوند و در نهایت بردارهای موقعیتی با ابعاد مشابه به بردارهای توکن افزوده می‌شوند تا ورودی نهایی مدل ساخته شود.

## خلاصه

- مدل‌های زبان بزرگ (LLM) برای پردازش متن خام نیاز دارند که داده‌های متنی به بردارهای عددی تبدیل شوند که به آن‌ها «جاسازی» (embedding) گفته می‌شود. این بردارها داده‌های گسسته مانند کلمات یا تصاویر را به فضای برداری پیوسته تبدیل می‌کنند تا بتوان آن‌ها را با عملیات شبکه‌های عصبی سازگار کرد.

- در گام اول، متن خام به توکن‌ها تقسیم می‌شود که می‌توانند کلمات یا حروف باشند. سپس این توکن‌ها به اعداد صحیح تبدیل می‌شوند که به آن‌ها شناسه توکن (token ID) گفته می‌شود.

- توکن‌های ویژه‌ای مانند `<|unk|>` (برای کلمات ناشناخته) و `<|endoftext|>` (برای علامت‌گذاری پایان متن) می‌توانند به مدل اضافه شوند تا درک آن بهتر شده و بتواند موقعیت‌های مختلف مانند کلمات ناشناخته یا مرزهای بین متون نامرتبط را مدیریت کند.

- شکل ۲.۱۹ روند کلی پردازش ورودی را نشان می‌دهد: ابتدا متن ورودی به توکن‌های جداگانه تقسیم می‌شود، سپس این توکن‌ها به شناسه‌های عددی تبدیل می‌شوند، بعد شناسه‌ها به بردارهای جاسازی تبدیل می‌شوند و در نهایت بردارهای موقعیتی با ابعاد مشابه به آن‌ها افزوده می‌شوند تا ورودی نهایی مدل ساخته شود.

- توکنایزر بایت‌زوج (BPE) که در مدل‌های GPT-2 و GPT-3 استفاده می‌شود، به طور مؤثری کلمات ناشناخته را با شکستن آن‌ها به زیرواحدهای کلمه یا حروف جداگانه مدیریت می‌کند.

برای آموزش LLMها، از روش پنجره لغزان روی داده‌های توکنیزه شده استفاده می‌کنیم تا جفت‌های ورودی-هدف تولید شود.

- لایه‌های جاسازی در PyTorch به شکل یک عملیات جستجو عمل می‌کنند و بردارهای مربوط به شناسه‌های توکن را بازیابی می‌کنند. این بردارهای جاسازی نمایندگی‌های پیوسته‌ای از توکن‌ها ارائه می‌دهند که برای آموزش مدل‌های عمیق مانند LLMها بسیار حیاتی است.

- اگرچه بردارهای جاسازی توکن نمایندگی‌های ثابتی برای هر توکن فراهم می‌کنند، اما اطلاعاتی درباره موقعیت توکن در توالی ندارند. برای رفع این مشکل، دو نوع اصلی بردار موقعیتی وجود دارد: موقعیت مطلق و موقعیت نسبی. مدل‌های GPT شرکت OpenAI از جاسازی موقعیت مطلق استفاده می‌کنند که به بردارهای جاسازی توکن اضافه شده و در طول آموزش مدل بهینه می‌شوند.
---

> [ 1.درک مدل‌های زبانی بزرگ
     (قبلی) ](<01.Understanding large language models.md>) < 2.کار با داده‌های متنی > 
[
    3.کدنویسی مکانیزم‌های توجه
(بعدی)
](<03.Coding attention mechanisms.md>)
